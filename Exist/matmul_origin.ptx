//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-37061995
// Cuda compilation tools, release 13.1, V13.1.115
// Based on NVVM 21.0.0
//

.version 9.1
.target sm_120a
.address_size 64

	// .globl	matmul_kernel
// global_smem has been demoted

.visible .entry matmul_kernel(
	.param .u64 .ptr .global .align 1 matmul_kernel_param_0,
	.param .u32 matmul_kernel_param_1,
	.param .u32 matmul_kernel_param_2,
	.param .u32 matmul_kernel_param_3,
	.param .u32 matmul_kernel_param_4,
	.param .u64 .ptr .global .align 1 matmul_kernel_param_5,
	.param .u32 matmul_kernel_param_6,
	.param .u32 matmul_kernel_param_7,
	.param .u32 matmul_kernel_param_8,
	.param .u32 matmul_kernel_param_9,
	.param .u64 .ptr .global .align 1 matmul_kernel_param_10,
	.param .u32 matmul_kernel_param_11,
	.param .u32 matmul_kernel_param_12,
	.param .u32 matmul_kernel_param_13,
	.param .u32 matmul_kernel_param_14,
	.param .u32 matmul_kernel_param_15,
	.param .u32 matmul_kernel_param_16,
	.param .u32 matmul_kernel_param_17,
	.param .u64 .ptr .global .align 1 matmul_kernel_param_18,
	.param .u32 matmul_kernel_param_19,
	.param .u32 matmul_kernel_param_20,
	.param .u64 .ptr .global .align 1 matmul_kernel_param_21,
	.param .u32 matmul_kernel_param_22,
	.param .u32 matmul_kernel_param_23,
	.hidden	.param .align 64 .b8 matmul_kernel_param_24[128],
	.hidden	.param .align 64 .b8 matmul_kernel_param_25[128],
	.hidden	.param .align 64 .b8 matmul_kernel_param_26[128]
)
.reqntid 256
.minnctapersm 1
.maxnreg 128
{
	.reg .pred 	%p<83>;
	.reg .b32 	%r<2128>;
	.reg .b64 	%rd<12>;
	.loc	1 10 0
	// demoted variable
	.shared .align 128 .b8 global_smem[49272];
	mov.b64 	%rd4, matmul_kernel_param_24;
	mov.b64 	%rd5, matmul_kernel_param_25;
	mov.b64 	%rd6, matmul_kernel_param_26;
	.loc	1 47 12
	mov.u32 	%r1, %tid.x;
	setp.ne.s32 	%p1, %r1, 0;
	shr.u32 	%r1097, %r1, 5;
	shfl.sync.idx.b32 	%r2, %r1097, 0, 31, -1;
	elect.sync 	%r1098|%p2, -1;
	@%p1 bra 	$L__BB0_2;
	mov.b32 	%r1099, 1;
	mbarrier.init.shared.b64 	[global_smem+49152], %r1099;
	mov.b32 	%r1100, 128;
	mbarrier.init.shared.b64 	[global_smem+49176], %r1100;
	mbarrier.init.shared.b64 	[global_smem+49160], %r1099;
	mbarrier.init.shared.b64 	[global_smem+49184], %r1100;
	mbarrier.init.shared.b64 	[global_smem+49168], %r1099;
	mbarrier.init.shared.b64 	[global_smem+49192], %r1100;
	.loc	1 48 12
	mbarrier.init.shared.b64 	[global_smem+49200], %r1099;
	mbarrier.init.shared.b64 	[global_smem+49216], %r1100;
	mbarrier.init.shared.b64 	[global_smem+49208], %r1099;
	mbarrier.init.shared.b64 	[global_smem+49224], %r1100;
	.loc	1 52 4
	mbarrier.init.shared.b64 	[global_smem+49264], %r1100;
$L__BB0_2:
	.loc	1 25 0
	// begin inline asm
	fence.mbarrier_init.release.cluster;
	// end inline asm
	bar.sync 	0;
	setp.gt.s32 	%p3, %r2, 3;
	@%p3 bra 	$L__BB0_44;
	setp.lt.u32 	%p7, %r2, 4;
	@%p7 bra 	$L__BB0_4;
	bra.uni 	$L__BB0_51;
$L__BB0_4:
	.loc	1 0 0
	ld.param.b32 	%r1817, [matmul_kernel_param_2];
	.loc	1 25 0
	setmaxnreg.inc.sync.aligned.u32 	224;
	.loc	1 39 18
	shr.s32 	%r1121, %r1817, 31;
	shr.u32 	%r1122, %r1121, 27;
	add.s32 	%r1123, %r1817, %r1122;
	shr.s32 	%r1124, %r1123, 5;
	and.b32 	%r1125, %r1123, -32;
	setp.ne.s32 	%p8, %r1817, %r1125;
	setp.gt.s32 	%p9, %r1817, -1;
	and.pred 	%p10, %p9, %p8;
	selp.b32 	%r1126, 1, 0, %p10;
	add.s32 	%r3, %r1124, %r1126;
	.loc	1 46 4
	setp.lt.s32 	%p11, %r3, 1;
	mov.b32 	%r1889, 0;
	.loc	1 49 22
	mov.b32 	%r1837, %r1889;
	mov.b32 	%r1839, %r1889;
	@%p11 bra 	$L__BB0_9;
$L__BB0_5:
	mov.b32 	%r1127, global_smem;
	add.s32 	%r1128, %r1127, 49200;
	mov.b32 	%r1129, 10000000;
	mbarrier.try_wait.parity.acquire.cta.shared::cta.b64 %p12, [%r1128], 0, %r1129;
	not.pred 	%p13, %p12;
	@%p13 bra 	$L__BB0_5;
	.loc	1 46 4
	setp.lt.s32 	%p14, %r3, 2;
	.loc	1 49 22
	shl.b32 	%r1149, %r1, 6;
	shr.u32 	%r1150, %r1, 3;
	or.b32 	%r1151, %r1149, %r1150;
	and.b32 	%r4, %r1151, 2008;
	shl.b32 	%r1152, %r1151, 1;
	and.b32 	%r1153, %r1152, 4016;
	mov.b32 	%r1154, global_smem;
	add.s32 	%r1155, %r1154, %r1153;
	add.s32 	%r1156, %r1155, 40960;
	shr.u32 	%r1157, %r1156, 3;
	and.b32 	%r1158, %r1157, 112;
	xor.b32 	%r1159, %r1158, %r1156;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r1821, %r1822, %r1823, %r1824}, [%r1159];
	add.s32 	%r1160, %r1155, 40992;
	shr.u32 	%r1161, %r1160, 3;
	and.b32 	%r1162, %r1161, 112;
	xor.b32 	%r1163, %r1162, %r1160;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r1825, %r1826, %r1827, %r1828}, [%r1163];
	add.s32 	%r1164, %r1155, 41024;
	shr.u32 	%r1165, %r1164, 3;
	and.b32 	%r1166, %r1165, 112;
	xor.b32 	%r1167, %r1166, %r1164;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r1829, %r1830, %r1831, %r1832}, [%r1167];
	add.s32 	%r1168, %r1155, 41056;
	shr.u32 	%r1169, %r1168, 3;
	and.b32 	%r1170, %r1169, 112;
	xor.b32 	%r1171, %r1170, %r1168;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r1833, %r1834, %r1835, %r1836}, [%r1171];
	.loc	1 25 0
	fence.proxy.async.shared::cta;
	.loc	1 49 22
	add.s32 	%r1172, %r1154, 49216;
	mov.b32 	%r1837, 1;
	mbarrier.arrive.release.cta.shared::cta.b64 _, [%r1172], %r1837;
	mov.b32 	%r1839, %r1837;
	@%p14 bra 	$L__BB0_9;
$L__BB0_7:
	mov.b32 	%r1173, global_smem;
	add.s32 	%r1174, %r1173, 49208;
	mov.b32 	%r1175, 10000000;
	mbarrier.try_wait.parity.acquire.cta.shared::cta.b64 %p15, [%r1174], 0, %r1175;
	not.pred 	%p16, %p15;
	@%p16 bra 	$L__BB0_7;
	shl.b32 	%r1179, %r4, 1;
	mov.b32 	%r1180, global_smem;
	add.s32 	%r1181, %r1180, %r1179;
	add.s32 	%r1182, %r1181, 45056;
	shr.u32 	%r1183, %r1182, 3;
	and.b32 	%r1184, %r1183, 112;
	xor.b32 	%r1185, %r1184, %r1182;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r1840, %r1841, %r1842, %r1843}, [%r1185];
	add.s32 	%r1186, %r1181, 45088;
	shr.u32 	%r1187, %r1186, 3;
	and.b32 	%r1188, %r1187, 112;
	xor.b32 	%r1189, %r1188, %r1186;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r1844, %r1845, %r1846, %r1847}, [%r1189];
	add.s32 	%r1190, %r1181, 45120;
	shr.u32 	%r1191, %r1190, 3;
	and.b32 	%r1192, %r1191, 112;
	xor.b32 	%r1193, %r1192, %r1190;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r1848, %r1849, %r1850, %r1851}, [%r1193];
	add.s32 	%r1194, %r1181, 45152;
	shr.u32 	%r1195, %r1194, 3;
	and.b32 	%r1196, %r1195, 112;
	xor.b32 	%r1197, %r1196, %r1194;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r1852, %r1853, %r1854, %r1855}, [%r1197];
	.loc	1 25 0
	fence.proxy.async.shared::cta;
	.loc	1 49 22
	add.s32 	%r1198, %r1180, 49224;
	mov.b32 	%r1177, 1;
	mbarrier.arrive.release.cta.shared::cta.b64 _, [%r1198], %r1177;
	mov.b32 	%r1839, 2;
	mov.b32 	%r1837, %r1889;
	mov.b32 	%r1889, %r1177;
$L__BB0_9:
	.loc	1 46 4
	setp.lt.s32 	%p17, %r3, 3;
	mov.b32 	%r2057, 0;
	mov.b32 	%r2058, %r2057;
	mov.b32 	%r2059, %r2057;
	mov.b32 	%r2060, %r2057;
	mov.b32 	%r2061, %r2057;
	mov.b32 	%r2062, %r2057;
	mov.b32 	%r2063, %r2057;
	mov.b32 	%r2064, %r2057;
	mov.b32 	%r2065, %r2057;
	mov.b32 	%r2066, %r2057;
	mov.b32 	%r2067, %r2057;
	mov.b32 	%r2068, %r2057;
	mov.b32 	%r2069, %r2057;
	mov.b32 	%r2070, %r2057;
	mov.b32 	%r2071, %r2057;
	mov.b32 	%r2072, %r2057;
	mov.b32 	%r2073, %r2057;
	mov.b32 	%r2074, %r2057;
	mov.b32 	%r2075, %r2057;
	mov.b32 	%r2076, %r2057;
	mov.b32 	%r2085, %r2057;
	mov.b32 	%r2086, %r2057;
	mov.b32 	%r2087, %r2057;
	mov.b32 	%r2088, %r2057;
	mov.b32 	%r2097, %r2057;
	mov.b32 	%r2098, %r2057;
	mov.b32 	%r2099, %r2057;
	mov.b32 	%r2100, %r2057;
	mov.b32 	%r2109, %r2057;
	mov.b32 	%r2110, %r2057;
	mov.b32 	%r2111, %r2057;
	mov.b32 	%r2112, %r2057;
	mov.b32 	%r2077, %r2057;
	mov.b32 	%r2078, %r2057;
	mov.b32 	%r2079, %r2057;
	mov.b32 	%r2080, %r2057;
	mov.b32 	%r2089, %r2057;
	mov.b32 	%r2090, %r2057;
	mov.b32 	%r2091, %r2057;
	mov.b32 	%r2092, %r2057;
	mov.b32 	%r2101, %r2057;
	mov.b32 	%r2102, %r2057;
	mov.b32 	%r2103, %r2057;
	mov.b32 	%r2104, %r2057;
	mov.b32 	%r2113, %r2057;
	mov.b32 	%r2114, %r2057;
	mov.b32 	%r2115, %r2057;
	mov.b32 	%r2116, %r2057;
	mov.b32 	%r2081, %r2057;
	mov.b32 	%r2082, %r2057;
	mov.b32 	%r2083, %r2057;
	mov.b32 	%r2084, %r2057;
	mov.b32 	%r2093, %r2057;
	mov.b32 	%r2094, %r2057;
	mov.b32 	%r2095, %r2057;
	mov.b32 	%r2096, %r2057;
	mov.b32 	%r2105, %r2057;
	mov.b32 	%r2106, %r2057;
	mov.b32 	%r2107, %r2057;
	mov.b32 	%r2108, %r2057;
	mov.b32 	%r2117, %r2057;
	mov.b32 	%r2118, %r2057;
	mov.b32 	%r2119, %r2057;
	mov.b32 	%r2120, %r2057;
	mov.b32 	%r2023, %r2057;
	mov.b32 	%r2024, %r2057;
	@%p17 bra 	$L__BB0_17;
	.loc	1 0 4
	mov.b32 	%r2117, 0f00000000;
	mov.b32 	%r1891, 0;
	mov.b32 	%r2024, %r1891;
	mov.b32 	%r1893, %r1891;
	mov.b32 	%r2118, %r2117;
	mov.b32 	%r2119, %r2117;
	mov.b32 	%r2120, %r2117;
	mov.b32 	%r2105, %r2117;
	mov.b32 	%r2106, %r2117;
	mov.b32 	%r2107, %r2117;
	mov.b32 	%r2108, %r2117;
	mov.b32 	%r2093, %r2117;
	mov.b32 	%r2094, %r2117;
	mov.b32 	%r2095, %r2117;
	mov.b32 	%r2096, %r2117;
	mov.b32 	%r2081, %r2117;
	mov.b32 	%r2082, %r2117;
	mov.b32 	%r2083, %r2117;
	mov.b32 	%r2084, %r2117;
	mov.b32 	%r2113, %r2117;
	mov.b32 	%r2114, %r2117;
	mov.b32 	%r2115, %r2117;
	mov.b32 	%r2116, %r2117;
	mov.b32 	%r2101, %r2117;
	mov.b32 	%r2102, %r2117;
	mov.b32 	%r2103, %r2117;
	mov.b32 	%r2104, %r2117;
	mov.b32 	%r2089, %r2117;
	mov.b32 	%r2090, %r2117;
	mov.b32 	%r2091, %r2117;
	mov.b32 	%r2092, %r2117;
	mov.b32 	%r2077, %r2117;
	mov.b32 	%r2078, %r2117;
	mov.b32 	%r2079, %r2117;
	mov.b32 	%r2080, %r2117;
	mov.b32 	%r2109, %r2117;
	mov.b32 	%r2110, %r2117;
	mov.b32 	%r2111, %r2117;
	mov.b32 	%r2112, %r2117;
	mov.b32 	%r2097, %r2117;
	mov.b32 	%r2098, %r2117;
	mov.b32 	%r2099, %r2117;
	mov.b32 	%r2100, %r2117;
	mov.b32 	%r2085, %r2117;
	mov.b32 	%r2086, %r2117;
	mov.b32 	%r2087, %r2117;
	mov.b32 	%r2088, %r2117;
	mov.b32 	%r2073, %r2117;
	mov.b32 	%r2074, %r2117;
	mov.b32 	%r2075, %r2117;
	mov.b32 	%r2076, %r2117;
	mov.b32 	%r2069, %r2117;
	mov.b32 	%r2070, %r2117;
	mov.b32 	%r2071, %r2117;
	mov.b32 	%r2072, %r2117;
	mov.b32 	%r2065, %r2117;
	mov.b32 	%r2066, %r2117;
	mov.b32 	%r2067, %r2117;
	mov.b32 	%r2068, %r2117;
	mov.b32 	%r2061, %r2117;
	mov.b32 	%r2062, %r2117;
	mov.b32 	%r2063, %r2117;
	mov.b32 	%r2064, %r2117;
	mov.b32 	%r2057, %r2117;
	mov.b32 	%r2058, %r2117;
	mov.b32 	%r2059, %r2117;
	mov.b32 	%r2060, %r2117;
	mov.b32 	%r1958, %r1891;
$L__BB0_11:
	mov.b32 	%r87, %r1855;
	mov.b32 	%r86, %r1854;
	mov.b32 	%r85, %r1853;
	mov.b32 	%r84, %r1852;
	mov.b32 	%r83, %r1851;
	mov.b32 	%r82, %r1850;
	mov.b32 	%r81, %r1849;
	mov.b32 	%r80, %r1848;
	mov.b32 	%r79, %r1847;
	mov.b32 	%r78, %r1846;
	mov.b32 	%r77, %r1845;
	mov.b32 	%r76, %r1844;
	mov.b32 	%r75, %r1843;
	mov.b32 	%r74, %r1842;
	mov.b32 	%r73, %r1841;
	mov.b32 	%r72, %r1840;
	.loc	1 49 22
	shl.b32 	%r1262, %r1891, 3;
	mov.b32 	%r1263, global_smem;
	add.s32 	%r1264, %r1263, %r1262;
	add.s32 	%r293, %r1264, 49152;
$L__BB0_12:
	mov.b32 	%r1265, 10000000;
	mbarrier.try_wait.parity.acquire.cta.shared::cta.b64 %p18, [%r293], %r2024, %r1265;
	not.pred 	%p19, %p18;
	@%p19 bra 	$L__BB0_12;
	.loc	1 47 12
	add.s32 	%r1266, %r1891, 1;
	mul.hi.u32 	%r1267, %r1266, -1431655765;
	shr.u32 	%r1268, %r1267, 1;
	and.b32 	%r1269, %r1268, 1;
	xor.b32 	%r2024, %r1269, %r2024;
	mul.lo.s32 	%r1270, %r1268, 3;
	sub.s32 	%r2023, %r1266, %r1270;
	add.s32 	%r1893, %r1893, 1;
	.loc	1 49 22
	shl.b32 	%r1271, %r1891, 13;
	mov.b32 	%r1272, global_smem;
	add.s32 	%r1273, %r1271, %r1272;
	and.b32 	%r1275, %r1, 16;
	shl.b32 	%r1276, %r1, 6;
	and.b32 	%r1277, %r1276, 960;
	or.b32 	%r1278, %r1275, %r1277;
	shl.b32 	%r1279, %r1, 5;
	and.b32 	%r1280, %r1279, 1024;
	or.b32 	%r1281, %r1280, %r1278;
	add.s32 	%r1282, %r1273, %r1281;
	shr.u32 	%r1283, %r1282, 3;
	and.b32 	%r1284, %r1283, 48;
	xor.b32 	%r1285, %r1284, %r1282;
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r297, %r298, %r299, %r300}, [%r1285];
	add.s32 	%r1286, %r1282, 32;
	shr.u32 	%r1287, %r1286, 3;
	and.b32 	%r1288, %r1287, 48;
	xor.b32 	%r1289, %r1288, %r1286;
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r301, %r302, %r303, %r304}, [%r1289];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r305, %r306, %r307, %r308}, [%r1285+2048];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r309, %r310, %r311, %r312}, [%r1289+2048];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r313, %r314, %r315, %r316}, [%r1285+4096];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r317, %r318, %r319, %r320}, [%r1289+4096];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r321, %r322, %r323, %r324}, [%r1285+6144];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r325, %r326, %r327, %r328}, [%r1289+6144];
	.loc	1 25 0
	fence.proxy.async.shared::cta;
	.loc	1 49 22
	add.s32 	%r1291, %r1272, %r1262;
	add.s32 	%r1292, %r1291, 49176;
	mov.b32 	%r1293, 1;
	mbarrier.arrive.release.cta.shared::cta.b64 _, [%r1292], %r1293;
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1294, %r1295, %r1296, %r1297},
		{%r297, %r298, %r299, %r300},
		{%r1821, %r1822},
		{%r2057, %r2058, %r2059, %r2060};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2057, %r2058, %r2059, %r2060},
		{%r301, %r302, %r303, %r304},
		{%r1823, %r1824},
		{%r1294, %r1295, %r1296, %r1297};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1298, %r1299, %r1300, %r1301},
		{%r297, %r298, %r299, %r300},
		{%r1825, %r1826},
		{%r2061, %r2062, %r2063, %r2064};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2061, %r2062, %r2063, %r2064},
		{%r301, %r302, %r303, %r304},
		{%r1827, %r1828},
		{%r1298, %r1299, %r1300, %r1301};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1302, %r1303, %r1304, %r1305},
		{%r297, %r298, %r299, %r300},
		{%r1829, %r1830},
		{%r2065, %r2066, %r2067, %r2068};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2065, %r2066, %r2067, %r2068},
		{%r301, %r302, %r303, %r304},
		{%r1831, %r1832},
		{%r1302, %r1303, %r1304, %r1305};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1306, %r1307, %r1308, %r1309},
		{%r297, %r298, %r299, %r300},
		{%r1833, %r1834},
		{%r2069, %r2070, %r2071, %r2072};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2069, %r2070, %r2071, %r2072},
		{%r301, %r302, %r303, %r304},
		{%r1835, %r1836},
		{%r1306, %r1307, %r1308, %r1309};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1310, %r1311, %r1312, %r1313},
		{%r305, %r306, %r307, %r308},
		{%r1821, %r1822},
		{%r2073, %r2074, %r2075, %r2076};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2073, %r2074, %r2075, %r2076},
		{%r309, %r310, %r311, %r312},
		{%r1823, %r1824},
		{%r1310, %r1311, %r1312, %r1313};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1314, %r1315, %r1316, %r1317},
		{%r313, %r314, %r315, %r316},
		{%r1821, %r1822},
		{%r2077, %r2078, %r2079, %r2080};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2077, %r2078, %r2079, %r2080},
		{%r317, %r318, %r319, %r320},
		{%r1823, %r1824},
		{%r1314, %r1315, %r1316, %r1317};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1318, %r1319, %r1320, %r1321},
		{%r321, %r322, %r323, %r324},
		{%r1821, %r1822},
		{%r2081, %r2082, %r2083, %r2084};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2081, %r2082, %r2083, %r2084},
		{%r325, %r326, %r327, %r328},
		{%r1823, %r1824},
		{%r1318, %r1319, %r1320, %r1321};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1322, %r1323, %r1324, %r1325},
		{%r305, %r306, %r307, %r308},
		{%r1825, %r1826},
		{%r2085, %r2086, %r2087, %r2088};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2085, %r2086, %r2087, %r2088},
		{%r309, %r310, %r311, %r312},
		{%r1827, %r1828},
		{%r1322, %r1323, %r1324, %r1325};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1326, %r1327, %r1328, %r1329},
		{%r313, %r314, %r315, %r316},
		{%r1825, %r1826},
		{%r2089, %r2090, %r2091, %r2092};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2089, %r2090, %r2091, %r2092},
		{%r317, %r318, %r319, %r320},
		{%r1827, %r1828},
		{%r1326, %r1327, %r1328, %r1329};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1330, %r1331, %r1332, %r1333},
		{%r321, %r322, %r323, %r324},
		{%r1825, %r1826},
		{%r2093, %r2094, %r2095, %r2096};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2093, %r2094, %r2095, %r2096},
		{%r325, %r326, %r327, %r328},
		{%r1827, %r1828},
		{%r1330, %r1331, %r1332, %r1333};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1334, %r1335, %r1336, %r1337},
		{%r305, %r306, %r307, %r308},
		{%r1829, %r1830},
		{%r2097, %r2098, %r2099, %r2100};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2097, %r2098, %r2099, %r2100},
		{%r309, %r310, %r311, %r312},
		{%r1831, %r1832},
		{%r1334, %r1335, %r1336, %r1337};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1338, %r1339, %r1340, %r1341},
		{%r313, %r314, %r315, %r316},
		{%r1829, %r1830},
		{%r2101, %r2102, %r2103, %r2104};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2101, %r2102, %r2103, %r2104},
		{%r317, %r318, %r319, %r320},
		{%r1831, %r1832},
		{%r1338, %r1339, %r1340, %r1341};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1342, %r1343, %r1344, %r1345},
		{%r321, %r322, %r323, %r324},
		{%r1829, %r1830},
		{%r2105, %r2106, %r2107, %r2108};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2105, %r2106, %r2107, %r2108},
		{%r325, %r326, %r327, %r328},
		{%r1831, %r1832},
		{%r1342, %r1343, %r1344, %r1345};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1346, %r1347, %r1348, %r1349},
		{%r305, %r306, %r307, %r308},
		{%r1833, %r1834},
		{%r2109, %r2110, %r2111, %r2112};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2109, %r2110, %r2111, %r2112},
		{%r309, %r310, %r311, %r312},
		{%r1835, %r1836},
		{%r1346, %r1347, %r1348, %r1349};
	shl.b32 	%r1350, %r1837, 3;
	add.s32 	%r1351, %r1272, %r1350;
	add.s32 	%r389, %r1351, 49200;
$L__BB0_14:
	mov.b32 	%r1352, 10000000;
	mbarrier.try_wait.parity.acquire.cta.shared::cta.b64 %p20, [%r389], %r1889, %r1352;
	not.pred 	%p21, %p20;
	@%p21 bra 	$L__BB0_14;
	.loc	1 48 12
	add.s32 	%r1353, %r1837, 1;
	shr.u32 	%r1354, %r1353, 1;
	and.b32 	%r1355, %r1354, 1;
	xor.b32 	%r1889, %r1355, %r1889;
	and.b32 	%r624, %r1353, 1;
	add.s32 	%r1839, %r1839, 1;
	.loc	1 49 22
	shl.b32 	%r1356, %r1837, 12;
	mov.b32 	%r1357, global_smem;
	add.s32 	%r1358, %r1357, %r1356;
	shr.u32 	%r1360, %r1, 2;
	shl.b32 	%r1361, %r1, 7;
	or.b32 	%r1362, %r1361, %r1360;
	and.b32 	%r1363, %r1362, 4016;
	add.s32 	%r1364, %r1358, %r1363;
	add.s32 	%r1365, %r1364, 40960;
	shr.u32 	%r1366, %r1365, 3;
	and.b32 	%r1367, %r1366, 112;
	xor.b32 	%r1368, %r1367, %r1365;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r1840, %r1841, %r1842, %r1843}, [%r1368];
	add.s32 	%r1369, %r1364, 40992;
	shr.u32 	%r1370, %r1369, 3;
	and.b32 	%r1371, %r1370, 112;
	xor.b32 	%r1372, %r1371, %r1369;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r1844, %r1845, %r1846, %r1847}, [%r1372];
	add.s32 	%r1373, %r1364, 41024;
	shr.u32 	%r1374, %r1373, 3;
	and.b32 	%r1375, %r1374, 112;
	xor.b32 	%r1376, %r1375, %r1373;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r1848, %r1849, %r1850, %r1851}, [%r1376];
	add.s32 	%r1377, %r1364, 41056;
	shr.u32 	%r1378, %r1377, 3;
	and.b32 	%r1379, %r1378, 112;
	xor.b32 	%r1380, %r1379, %r1377;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r1852, %r1853, %r1854, %r1855}, [%r1380];
	.loc	1 25 0
	fence.proxy.async.shared::cta;
	.loc	1 49 22
	add.s32 	%r1382, %r1357, %r1350;
	add.s32 	%r1383, %r1382, 49216;
	mov.b32 	%r1384, 1;
	mbarrier.arrive.release.cta.shared::cta.b64 _, [%r1383], %r1384;
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1385, %r1386, %r1387, %r1388},
		{%r313, %r314, %r315, %r316},
		{%r1833, %r1834},
		{%r2113, %r2114, %r2115, %r2116};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2113, %r2114, %r2115, %r2116},
		{%r317, %r318, %r319, %r320},
		{%r1835, %r1836},
		{%r1385, %r1386, %r1387, %r1388};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1389, %r1390, %r1391, %r1392},
		{%r321, %r322, %r323, %r324},
		{%r1833, %r1834},
		{%r2117, %r2118, %r2119, %r2120};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2117, %r2118, %r2119, %r2120},
		{%r325, %r326, %r327, %r328},
		{%r1835, %r1836},
		{%r1389, %r1390, %r1391, %r1392};
	.loc	1 46 4
	add.s32 	%r623, %r1958, 1;
	add.s32 	%r1393, %r3, -3;
	setp.ne.s32 	%p22, %r1958, %r1393;
	mov.b32 	%r1821, %r72;
	mov.b32 	%r1822, %r73;
	mov.b32 	%r1823, %r74;
	mov.b32 	%r1824, %r75;
	mov.b32 	%r1825, %r76;
	mov.b32 	%r1826, %r77;
	mov.b32 	%r1827, %r78;
	mov.b32 	%r1828, %r79;
	mov.b32 	%r1829, %r80;
	mov.b32 	%r1830, %r81;
	mov.b32 	%r1831, %r82;
	mov.b32 	%r1832, %r83;
	mov.b32 	%r1833, %r84;
	mov.b32 	%r1834, %r85;
	mov.b32 	%r1835, %r86;
	mov.b32 	%r1836, %r87;
	mov.b32 	%r1837, %r624;
	mov.b32 	%r1891, %r2023;
	mov.b32 	%r1958, %r623;
	@%p22 bra 	$L__BB0_11;
	.loc	1 0 4
	mov.b32 	%r1821, %r72;
	mov.b32 	%r1822, %r73;
	mov.b32 	%r1823, %r74;
	mov.b32 	%r1824, %r75;
	mov.b32 	%r1825, %r76;
	mov.b32 	%r1826, %r77;
	mov.b32 	%r1827, %r78;
	mov.b32 	%r1828, %r79;
	mov.b32 	%r1829, %r80;
	mov.b32 	%r1830, %r81;
	mov.b32 	%r1831, %r82;
	mov.b32 	%r1832, %r83;
	mov.b32 	%r1833, %r84;
	mov.b32 	%r1834, %r85;
	mov.b32 	%r1835, %r86;
	mov.b32 	%r1836, %r87;
$L__BB0_17:
	.loc	1 46 4
	setp.lt.s32 	%p23, %r3, 1;
	@%p23 bra 	$L__BB0_24;
	.loc	1 49 22
	shl.b32 	%r1394, %r2023, 3;
	mov.b32 	%r1395, global_smem;
	add.s32 	%r1396, %r1395, %r1394;
	add.s32 	%r797, %r1396, 49152;
$L__BB0_19:
	mov.b32 	%r1397, 10000000;
	mbarrier.try_wait.parity.acquire.cta.shared::cta.b64 %p24, [%r797], %r2024, %r1397;
	not.pred 	%p25, %p24;
	@%p25 bra 	$L__BB0_19;
	.loc	1 46 4
	setp.lt.s32 	%p26, %r3, 2;
	.loc	1 47 12
	add.s32 	%r1398, %r2023, 1;
	mul.hi.u32 	%r1399, %r1398, -1431655765;
	shr.u32 	%r1400, %r1399, 1;
	and.b32 	%r1401, %r1400, 1;
	xor.b32 	%r798, %r1401, %r2024;
	mul.lo.s32 	%r1402, %r1400, 3;
	sub.s32 	%r799, %r1398, %r1402;
	.loc	1 49 22
	shl.b32 	%r1403, %r2023, 13;
	mov.b32 	%r1404, global_smem;
	add.s32 	%r1405, %r1403, %r1404;
	shl.b32 	%r1406, %r1, 5;
	and.b32 	%r1407, %r1406, 480;
	shr.u32 	%r1408, %r1, 1;
	and.b32 	%r1409, %r1408, 8;
	or.b32 	%r1410, %r1409, %r1407;
	shl.b32 	%r1411, %r1097, 9;
	and.b32 	%r1412, %r1411, 512;
	or.b32 	%r800, %r1410, %r1412;
	shl.b32 	%r1413, %r800, 1;
	add.s32 	%r1414, %r1405, %r1413;
	shr.u32 	%r1415, %r1414, 3;
	and.b32 	%r1416, %r1415, 48;
	xor.b32 	%r1417, %r1416, %r1414;
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1418, %r1419, %r1420, %r1421}, [%r1417];
	add.s32 	%r1422, %r1414, 32;
	shr.u32 	%r1423, %r1422, 3;
	and.b32 	%r1424, %r1423, 48;
	xor.b32 	%r1425, %r1424, %r1422;
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1426, %r1427, %r1428, %r1429}, [%r1425];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1430, %r1431, %r1432, %r1433}, [%r1417+2048];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1434, %r1435, %r1436, %r1437}, [%r1425+2048];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1438, %r1439, %r1440, %r1441}, [%r1417+4096];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1442, %r1443, %r1444, %r1445}, [%r1425+4096];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1446, %r1447, %r1448, %r1449}, [%r1417+6144];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1450, %r1451, %r1452, %r1453}, [%r1425+6144];
	.loc	1 25 0
	fence.proxy.async.shared::cta;
	.loc	1 49 22
	add.s32 	%r1454, %r797, 24;
	mov.b32 	%r1455, 1;
	mbarrier.arrive.release.cta.shared::cta.b64 _, [%r1454], %r1455;
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1456, %r1457, %r1458, %r1459},
		{%r1418, %r1419, %r1420, %r1421},
		{%r1821, %r1822},
		{%r2057, %r2058, %r2059, %r2060};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2057, %r2058, %r2059, %r2060},
		{%r1426, %r1427, %r1428, %r1429},
		{%r1823, %r1824},
		{%r1456, %r1457, %r1458, %r1459};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1460, %r1461, %r1462, %r1463},
		{%r1418, %r1419, %r1420, %r1421},
		{%r1825, %r1826},
		{%r2061, %r2062, %r2063, %r2064};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2061, %r2062, %r2063, %r2064},
		{%r1426, %r1427, %r1428, %r1429},
		{%r1827, %r1828},
		{%r1460, %r1461, %r1462, %r1463};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1464, %r1465, %r1466, %r1467},
		{%r1418, %r1419, %r1420, %r1421},
		{%r1829, %r1830},
		{%r2065, %r2066, %r2067, %r2068};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2065, %r2066, %r2067, %r2068},
		{%r1426, %r1427, %r1428, %r1429},
		{%r1831, %r1832},
		{%r1464, %r1465, %r1466, %r1467};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1468, %r1469, %r1470, %r1471},
		{%r1418, %r1419, %r1420, %r1421},
		{%r1833, %r1834},
		{%r2069, %r2070, %r2071, %r2072};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2069, %r2070, %r2071, %r2072},
		{%r1426, %r1427, %r1428, %r1429},
		{%r1835, %r1836},
		{%r1468, %r1469, %r1470, %r1471};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1472, %r1473, %r1474, %r1475},
		{%r1430, %r1431, %r1432, %r1433},
		{%r1821, %r1822},
		{%r2073, %r2074, %r2075, %r2076};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2073, %r2074, %r2075, %r2076},
		{%r1434, %r1435, %r1436, %r1437},
		{%r1823, %r1824},
		{%r1472, %r1473, %r1474, %r1475};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1476, %r1477, %r1478, %r1479},
		{%r1438, %r1439, %r1440, %r1441},
		{%r1821, %r1822},
		{%r2077, %r2078, %r2079, %r2080};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2077, %r2078, %r2079, %r2080},
		{%r1442, %r1443, %r1444, %r1445},
		{%r1823, %r1824},
		{%r1476, %r1477, %r1478, %r1479};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1480, %r1481, %r1482, %r1483},
		{%r1446, %r1447, %r1448, %r1449},
		{%r1821, %r1822},
		{%r2081, %r2082, %r2083, %r2084};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2081, %r2082, %r2083, %r2084},
		{%r1450, %r1451, %r1452, %r1453},
		{%r1823, %r1824},
		{%r1480, %r1481, %r1482, %r1483};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1484, %r1485, %r1486, %r1487},
		{%r1430, %r1431, %r1432, %r1433},
		{%r1825, %r1826},
		{%r2085, %r2086, %r2087, %r2088};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2085, %r2086, %r2087, %r2088},
		{%r1434, %r1435, %r1436, %r1437},
		{%r1827, %r1828},
		{%r1484, %r1485, %r1486, %r1487};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1488, %r1489, %r1490, %r1491},
		{%r1438, %r1439, %r1440, %r1441},
		{%r1825, %r1826},
		{%r2089, %r2090, %r2091, %r2092};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2089, %r2090, %r2091, %r2092},
		{%r1442, %r1443, %r1444, %r1445},
		{%r1827, %r1828},
		{%r1488, %r1489, %r1490, %r1491};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1492, %r1493, %r1494, %r1495},
		{%r1446, %r1447, %r1448, %r1449},
		{%r1825, %r1826},
		{%r2093, %r2094, %r2095, %r2096};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2093, %r2094, %r2095, %r2096},
		{%r1450, %r1451, %r1452, %r1453},
		{%r1827, %r1828},
		{%r1492, %r1493, %r1494, %r1495};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1496, %r1497, %r1498, %r1499},
		{%r1430, %r1431, %r1432, %r1433},
		{%r1829, %r1830},
		{%r2097, %r2098, %r2099, %r2100};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2097, %r2098, %r2099, %r2100},
		{%r1434, %r1435, %r1436, %r1437},
		{%r1831, %r1832},
		{%r1496, %r1497, %r1498, %r1499};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1500, %r1501, %r1502, %r1503},
		{%r1438, %r1439, %r1440, %r1441},
		{%r1829, %r1830},
		{%r2101, %r2102, %r2103, %r2104};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2101, %r2102, %r2103, %r2104},
		{%r1442, %r1443, %r1444, %r1445},
		{%r1831, %r1832},
		{%r1500, %r1501, %r1502, %r1503};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1504, %r1505, %r1506, %r1507},
		{%r1446, %r1447, %r1448, %r1449},
		{%r1829, %r1830},
		{%r2105, %r2106, %r2107, %r2108};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2105, %r2106, %r2107, %r2108},
		{%r1450, %r1451, %r1452, %r1453},
		{%r1831, %r1832},
		{%r1504, %r1505, %r1506, %r1507};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1508, %r1509, %r1510, %r1511},
		{%r1430, %r1431, %r1432, %r1433},
		{%r1833, %r1834},
		{%r2109, %r2110, %r2111, %r2112};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2109, %r2110, %r2111, %r2112},
		{%r1434, %r1435, %r1436, %r1437},
		{%r1835, %r1836},
		{%r1508, %r1509, %r1510, %r1511};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1512, %r1513, %r1514, %r1515},
		{%r1438, %r1439, %r1440, %r1441},
		{%r1833, %r1834},
		{%r2113, %r2114, %r2115, %r2116};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2113, %r2114, %r2115, %r2116},
		{%r1442, %r1443, %r1444, %r1445},
		{%r1835, %r1836},
		{%r1512, %r1513, %r1514, %r1515};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1516, %r1517, %r1518, %r1519},
		{%r1446, %r1447, %r1448, %r1449},
		{%r1833, %r1834},
		{%r2117, %r2118, %r2119, %r2120};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2117, %r2118, %r2119, %r2120},
		{%r1450, %r1451, %r1452, %r1453},
		{%r1835, %r1836},
		{%r1516, %r1517, %r1518, %r1519};
	@%p26 bra 	$L__BB0_24;
	shl.b32 	%r1520, %r799, 3;
	mov.b32 	%r1521, global_smem;
	add.s32 	%r1522, %r1521, %r1520;
	add.s32 	%r929, %r1522, 49152;
$L__BB0_22:
	mov.b32 	%r1523, 10000000;
	mbarrier.try_wait.parity.acquire.cta.shared::cta.b64 %p27, [%r929], %r798, %r1523;
	not.pred 	%p28, %p27;
	@%p28 bra 	$L__BB0_22;
	shl.b32 	%r1524, %r799, 13;
	mov.b32 	%r1525, global_smem;
	add.s32 	%r1526, %r1524, %r1525;
	add.s32 	%r1528, %r1526, %r1413;
	shr.u32 	%r1529, %r1528, 3;
	and.b32 	%r1530, %r1529, 48;
	xor.b32 	%r1531, %r1530, %r1528;
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1532, %r1533, %r1534, %r1535}, [%r1531];
	add.s32 	%r1536, %r1528, 32;
	shr.u32 	%r1537, %r1536, 3;
	and.b32 	%r1538, %r1537, 48;
	xor.b32 	%r1539, %r1538, %r1536;
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1540, %r1541, %r1542, %r1543}, [%r1539];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1544, %r1545, %r1546, %r1547}, [%r1531+2048];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1548, %r1549, %r1550, %r1551}, [%r1539+2048];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1552, %r1553, %r1554, %r1555}, [%r1531+4096];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1556, %r1557, %r1558, %r1559}, [%r1539+4096];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1560, %r1561, %r1562, %r1563}, [%r1531+6144];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r1564, %r1565, %r1566, %r1567}, [%r1539+6144];
	.loc	1 25 0
	fence.proxy.async.shared::cta;
	.loc	1 49 22
	add.s32 	%r1568, %r929, 24;
	mov.b32 	%r1569, 1;
	mbarrier.arrive.release.cta.shared::cta.b64 _, [%r1568], %r1569;
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1570, %r1571, %r1572, %r1573},
		{%r1532, %r1533, %r1534, %r1535},
		{%r1840, %r1841},
		{%r2057, %r2058, %r2059, %r2060};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2057, %r2058, %r2059, %r2060},
		{%r1540, %r1541, %r1542, %r1543},
		{%r1842, %r1843},
		{%r1570, %r1571, %r1572, %r1573};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1574, %r1575, %r1576, %r1577},
		{%r1532, %r1533, %r1534, %r1535},
		{%r1844, %r1845},
		{%r2061, %r2062, %r2063, %r2064};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2061, %r2062, %r2063, %r2064},
		{%r1540, %r1541, %r1542, %r1543},
		{%r1846, %r1847},
		{%r1574, %r1575, %r1576, %r1577};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1578, %r1579, %r1580, %r1581},
		{%r1532, %r1533, %r1534, %r1535},
		{%r1848, %r1849},
		{%r2065, %r2066, %r2067, %r2068};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2065, %r2066, %r2067, %r2068},
		{%r1540, %r1541, %r1542, %r1543},
		{%r1850, %r1851},
		{%r1578, %r1579, %r1580, %r1581};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1582, %r1583, %r1584, %r1585},
		{%r1532, %r1533, %r1534, %r1535},
		{%r1852, %r1853},
		{%r2069, %r2070, %r2071, %r2072};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2069, %r2070, %r2071, %r2072},
		{%r1540, %r1541, %r1542, %r1543},
		{%r1854, %r1855},
		{%r1582, %r1583, %r1584, %r1585};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1586, %r1587, %r1588, %r1589},
		{%r1544, %r1545, %r1546, %r1547},
		{%r1840, %r1841},
		{%r2073, %r2074, %r2075, %r2076};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2073, %r2074, %r2075, %r2076},
		{%r1548, %r1549, %r1550, %r1551},
		{%r1842, %r1843},
		{%r1586, %r1587, %r1588, %r1589};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1590, %r1591, %r1592, %r1593},
		{%r1552, %r1553, %r1554, %r1555},
		{%r1840, %r1841},
		{%r2077, %r2078, %r2079, %r2080};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2077, %r2078, %r2079, %r2080},
		{%r1556, %r1557, %r1558, %r1559},
		{%r1842, %r1843},
		{%r1590, %r1591, %r1592, %r1593};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1594, %r1595, %r1596, %r1597},
		{%r1560, %r1561, %r1562, %r1563},
		{%r1840, %r1841},
		{%r2081, %r2082, %r2083, %r2084};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2081, %r2082, %r2083, %r2084},
		{%r1564, %r1565, %r1566, %r1567},
		{%r1842, %r1843},
		{%r1594, %r1595, %r1596, %r1597};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1598, %r1599, %r1600, %r1601},
		{%r1544, %r1545, %r1546, %r1547},
		{%r1844, %r1845},
		{%r2085, %r2086, %r2087, %r2088};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2085, %r2086, %r2087, %r2088},
		{%r1548, %r1549, %r1550, %r1551},
		{%r1846, %r1847},
		{%r1598, %r1599, %r1600, %r1601};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1602, %r1603, %r1604, %r1605},
		{%r1552, %r1553, %r1554, %r1555},
		{%r1844, %r1845},
		{%r2089, %r2090, %r2091, %r2092};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2089, %r2090, %r2091, %r2092},
		{%r1556, %r1557, %r1558, %r1559},
		{%r1846, %r1847},
		{%r1602, %r1603, %r1604, %r1605};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1606, %r1607, %r1608, %r1609},
		{%r1560, %r1561, %r1562, %r1563},
		{%r1844, %r1845},
		{%r2093, %r2094, %r2095, %r2096};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2093, %r2094, %r2095, %r2096},
		{%r1564, %r1565, %r1566, %r1567},
		{%r1846, %r1847},
		{%r1606, %r1607, %r1608, %r1609};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1610, %r1611, %r1612, %r1613},
		{%r1544, %r1545, %r1546, %r1547},
		{%r1848, %r1849},
		{%r2097, %r2098, %r2099, %r2100};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2097, %r2098, %r2099, %r2100},
		{%r1548, %r1549, %r1550, %r1551},
		{%r1850, %r1851},
		{%r1610, %r1611, %r1612, %r1613};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1614, %r1615, %r1616, %r1617},
		{%r1552, %r1553, %r1554, %r1555},
		{%r1848, %r1849},
		{%r2101, %r2102, %r2103, %r2104};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2101, %r2102, %r2103, %r2104},
		{%r1556, %r1557, %r1558, %r1559},
		{%r1850, %r1851},
		{%r1614, %r1615, %r1616, %r1617};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1618, %r1619, %r1620, %r1621},
		{%r1560, %r1561, %r1562, %r1563},
		{%r1848, %r1849},
		{%r2105, %r2106, %r2107, %r2108};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2105, %r2106, %r2107, %r2108},
		{%r1564, %r1565, %r1566, %r1567},
		{%r1850, %r1851},
		{%r1618, %r1619, %r1620, %r1621};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1622, %r1623, %r1624, %r1625},
		{%r1544, %r1545, %r1546, %r1547},
		{%r1852, %r1853},
		{%r2109, %r2110, %r2111, %r2112};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2109, %r2110, %r2111, %r2112},
		{%r1548, %r1549, %r1550, %r1551},
		{%r1854, %r1855},
		{%r1622, %r1623, %r1624, %r1625};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1626, %r1627, %r1628, %r1629},
		{%r1552, %r1553, %r1554, %r1555},
		{%r1852, %r1853},
		{%r2113, %r2114, %r2115, %r2116};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2113, %r2114, %r2115, %r2116},
		{%r1556, %r1557, %r1558, %r1559},
		{%r1854, %r1855},
		{%r1626, %r1627, %r1628, %r1629};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r1630, %r1631, %r1632, %r1633},
		{%r1560, %r1561, %r1562, %r1563},
		{%r1852, %r1853},
		{%r2117, %r2118, %r2119, %r2120};
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32
		{%r2117, %r2118, %r2119, %r2120},
		{%r1564, %r1565, %r1566, %r1567},
		{%r1854, %r1855},
		{%r1630, %r1631, %r1632, %r1633};
$L__BB0_24:
	.loc	1 51 18
	cvt.rn.f16x2.f32 	%r1634, %r2058, %r2057;
	cvt.rn.f16x2.f32 	%r1635, %r2060, %r2059;
	cvt.rn.f16x2.f32 	%r1636, %r2074, %r2073;
	cvt.rn.f16x2.f32 	%r1637, %r2076, %r2075;
	cvt.rn.f16x2.f32 	%r1638, %r2078, %r2077;
	cvt.rn.f16x2.f32 	%r1639, %r2080, %r2079;
	cvt.rn.f16x2.f32 	%r1640, %r2082, %r2081;
	cvt.rn.f16x2.f32 	%r1641, %r2084, %r2083;
	cvt.rn.f16x2.f32 	%r1642, %r2062, %r2061;
	cvt.rn.f16x2.f32 	%r1643, %r2064, %r2063;
	cvt.rn.f16x2.f32 	%r1644, %r2086, %r2085;
	cvt.rn.f16x2.f32 	%r1645, %r2088, %r2087;
	cvt.rn.f16x2.f32 	%r1646, %r2090, %r2089;
	cvt.rn.f16x2.f32 	%r1647, %r2092, %r2091;
	cvt.rn.f16x2.f32 	%r1648, %r2094, %r2093;
	cvt.rn.f16x2.f32 	%r1649, %r2096, %r2095;
	cvt.rn.f16x2.f32 	%r1650, %r2066, %r2065;
	cvt.rn.f16x2.f32 	%r1651, %r2068, %r2067;
	cvt.rn.f16x2.f32 	%r1652, %r2098, %r2097;
	cvt.rn.f16x2.f32 	%r1653, %r2100, %r2099;
	cvt.rn.f16x2.f32 	%r1654, %r2102, %r2101;
	cvt.rn.f16x2.f32 	%r1655, %r2104, %r2103;
	cvt.rn.f16x2.f32 	%r1656, %r2106, %r2105;
	cvt.rn.f16x2.f32 	%r1657, %r2108, %r2107;
	cvt.rn.f16x2.f32 	%r1658, %r2070, %r2069;
	cvt.rn.f16x2.f32 	%r1659, %r2072, %r2071;
	cvt.rn.f16x2.f32 	%r1660, %r2110, %r2109;
	cvt.rn.f16x2.f32 	%r1661, %r2112, %r2111;
	cvt.rn.f16x2.f32 	%r1662, %r2114, %r2113;
	cvt.rn.f16x2.f32 	%r1663, %r2116, %r2115;
	cvt.rn.f16x2.f32 	%r1664, %r2118, %r2117;
	cvt.rn.f16x2.f32 	%r1665, %r2120, %r2119;
	.loc	1 52 4
	shr.u32 	%r1666, %r1, 2;
	shl.b32 	%r1667, %r1, 7;
	or.b32 	%r1668, %r1667, %r1666;
	and.b32 	%r1669, %r1668, 1968;
	shl.b32 	%r1670, %r1, 8;
	and.b32 	%r1671, %r1670, 4096;
	or.b32 	%r1672, %r1671, %r1669;
	shl.b32 	%r1673, %r1097, 11;
	and.b32 	%r1674, %r1673, 2048;
	or.b32 	%r1675, %r1674, %r1672;
	mov.b32 	%r1676, global_smem;
	add.s32 	%r1677, %r1676, %r1675;
	add.s32 	%r1678, %r1677, 24576;
	shr.u32 	%r1679, %r1678, 3;
	and.b32 	%r1680, %r1679, 112;
	xor.b32 	%r1681, %r1680, %r1678;
	stmatrix.sync.aligned.m8n8.x4.shared.b16 [%r1681], {%r1634, %r1635, %r1636, %r1637};
	stmatrix.sync.aligned.m8n8.x4.shared.b16 [%r1681+8192], {%r1638, %r1639, %r1640, %r1641};
	add.s32 	%r1682, %r1677, 24608;
	shr.u32 	%r1683, %r1682, 3;
	and.b32 	%r1684, %r1683, 112;
	xor.b32 	%r1685, %r1684, %r1682;
	stmatrix.sync.aligned.m8n8.x4.shared.b16 [%r1685], {%r1642, %r1643, %r1644, %r1645};
	stmatrix.sync.aligned.m8n8.x4.shared.b16 [%r1685+8192], {%r1646, %r1647, %r1648, %r1649};
	add.s32 	%r1686, %r1677, 24640;
	shr.u32 	%r1687, %r1686, 3;
	and.b32 	%r1688, %r1687, 112;
	xor.b32 	%r1689, %r1688, %r1686;
	stmatrix.sync.aligned.m8n8.x4.shared.b16 [%r1689], {%r1650, %r1651, %r1652, %r1653};
	stmatrix.sync.aligned.m8n8.x4.shared.b16 [%r1689+8192], {%r1654, %r1655, %r1656, %r1657};
	add.s32 	%r1690, %r1677, 24672;
	shr.u32 	%r1691, %r1690, 3;
	and.b32 	%r1692, %r1691, 112;
	xor.b32 	%r1693, %r1692, %r1690;
	stmatrix.sync.aligned.m8n8.x4.shared.b16 [%r1693], {%r1658, %r1659, %r1660, %r1661};
	stmatrix.sync.aligned.m8n8.x4.shared.b16 [%r1693+8192], {%r1662, %r1663, %r1664, %r1665};
	.loc	1 25 0
	fence.proxy.async.shared::cta;
	.loc	1 52 4
	add.s32 	%r1694, %r1676, 49264;
	mov.b32 	%r1695, 1;
	mbarrier.arrive.release.cta.shared::cta.b64 _, [%r1694], %r1695;
	// begin inline asm
	.pragma "next knob FenceCode";

	// end inline asm
	.loc	1 25 0
	add.s32 	%r1696, %r2, -6;
	setp.lt.u32 	%p29, %r1696, 2;
	@%p29 bra 	$L__BB0_50;
	setp.eq.s32 	%p30, %r2, 5;
	@%p30 bra 	$L__BB0_47;
	setp.ne.s32 	%p31, %r2, 4;
	@%p31 bra 	$L__BB0_51;
$L__BB0_27:
	.loc	1 0 0
	ld.param.b32 	%r1818, [matmul_kernel_param_2];
	.loc	1 25 0
	setmaxnreg.dec.sync.aligned.u32 	32;
	.loc	1 39 18
	cvta.param.u64 	%rd1, %rd4;
	// begin inline asm
	prefetch.tensormap [%rd1];
	// end inline asm
	shr.s32 	%r1697, %r1818, 31;
	shr.u32 	%r1698, %r1697, 27;
	add.s32 	%r1699, %r1818, %r1698;
	shr.s32 	%r1700, %r1699, 5;
	and.b32 	%r1701, %r1699, -32;
	setp.ne.s32 	%p32, %r1818, %r1701;
	setp.gt.s32 	%p33, %r1818, -1;
	and.pred 	%p34, %p33, %p32;
	selp.b32 	%r1702, 1, 0, %p34;
	add.s32 	%r1058, %r1700, %r1702;
	.loc	1 48 12
	cvta.param.u64 	%rd2, %rd5;
	// begin inline asm
	prefetch.tensormap [%rd2];
	// end inline asm
	.loc	1 46 4
	setp.lt.s32 	%p35, %r1058, 1;
	@%p35 bra 	$L__BB0_42;
	.loc	1 0 4
	ld.param.b32 	%r1819, [matmul_kernel_param_7];
	ld.param.b32 	%r1815, [matmul_kernel_param_1];
	.loc	1 38 17
	.loc	1 13 10, function_name $L__info_string0, inlined_at 1 38 17
	mov.u32 	%r1707, %clusterid.x;
	.loc	1 15 16, function_name $L__info_string0, inlined_at 1 38 17
	shr.s32 	%r1708, %r1819, 31;
	shr.u32 	%r1709, %r1708, 26;
	add.s32 	%r1710, %r1819, %r1709;
	shr.s32 	%r1711, %r1710, 6;
	setp.gt.s32 	%p36, %r1819, -1;
	and.b32 	%r1712, %r1710, -64;
	setp.ne.s32 	%p37, %r1819, %r1712;
	and.pred 	%p38, %p36, %p37;
	selp.b32 	%r1713, 1, 0, %p38;
	add.s32 	%r1714, %r1711, %r1713;
	.loc	1 16 23, function_name $L__info_string0, inlined_at 1 38 17
	shl.b32 	%r1715, %r1714, 3;
	.loc	1 17 15, function_name $L__info_string0, inlined_at 1 38 17
	div.s32 	%r1716, %r1707, %r1715;
	setp.lt.s32 	%p39, %r1714, 0;
	.loc	1 14 16, function_name $L__info_string0, inlined_at 1 38 17
	shr.s32 	%r1717, %r1815, 31;
	shr.u32 	%r1718, %r1717, 25;
	add.s32 	%r1719, %r1815, %r1718;
	shr.s32 	%r1720, %r1719, 7;
	.loc	1 17 15, function_name $L__info_string0, inlined_at 1 38 17
	mul.lo.s32 	%r1721, %r1716, %r1715;
	setp.ne.s32 	%p40, %r1707, %r1721;
	and.pred 	%p41, %p39, %p40;
	selp.b32 	%r1722, -1, 0, %p41;
	add.s32 	%r1723, %r1716, %r1722;
	.loc	1 18 18, function_name $L__info_string0, inlined_at 1 38 17
	shl.b32 	%r1724, %r1723, 3;
	.loc	1 14 16, function_name $L__info_string0, inlined_at 1 38 17
	setp.gt.s32 	%p42, %r1815, -1;
	and.b32 	%r1725, %r1719, -128;
	setp.ne.s32 	%p43, %r1815, %r1725;
	and.pred 	%p44, %p42, %p43;
	selp.b32 	%r1726, 1, 0, %p44;
	add.s32 	%r1727, %r1720, %r1726;
	.loc	1 19 23, function_name $L__info_string0, inlined_at 1 38 17
	sub.s32 	%r1728, %r1727, %r1724;
	.loc	1 19 19, function_name $L__info_string0, inlined_at 1 38 17
	min.s32 	%r1729, %r1728, 8;
	.loc	1 20 27, function_name $L__info_string0, inlined_at 1 38 17
	rem.s32 	%r1730, %r1707, %r1729;
	.loc	1 21 13, function_name $L__info_string0, inlined_at 1 38 17
	sub.s32 	%r1731, %r1707, %r1721;
	add.s32 	%r1732, %r1731, %r1715;
	setp.ne.s32 	%p45, %r1731, 0;
	selp.b32 	%r1733, %r1732, %r1731, %p45;
	selp.b32 	%r1734, %r1733, %r1731, %p39;
	.loc	1 21 12, function_name $L__info_string0, inlined_at 1 38 17
	div.s32 	%r1735, %r1734, %r1729;
	xor.b32 	%r1736, %r1734, %r1728;
	setp.lt.s32 	%p46, %r1736, 0;
	mul.lo.s32 	%r1737, %r1729, %r1735;
	setp.ne.s32 	%p47, %r1734, %r1737;
	and.pred 	%p48, %p46, %p47;
	selp.b32 	%r1738, -1, 0, %p48;
	add.s32 	%r1739, %r1735, %r1738;
	.loc	1 20 27, function_name $L__info_string0, inlined_at 1 38 17
	add.s32 	%r1740, %r1729, %r1730;
	setp.ne.s32 	%p49, %r1730, 0;
	setp.lt.s32 	%p50, %r1728, 0;
	selp.b32 	%r1741, %r1740, %r1730, %p50;
	selp.b32 	%r1742, %r1741, %r1730, %p49;
	.loc	1 47 12
	add.s32 	%r1743, %r1742, %r1724;
	shl.b32 	%r1059, %r1743, 7;
	shl.b32 	%r1060, %r1739, 6;
	mov.b32 	%r2121, 0;
	mov.b32 	%r2122, 1;
	mov.b32 	%r2123, %r2121;
	mov.b32 	%r2124, %r2121;
	mov.b32 	%r2125, %r2122;
	mov.b32 	%r2126, %r2121;
	mov.b32 	%r2127, %r2121;
$L__BB0_29:
	.loc	1 0 12
	mov.b32 	%r1064, %r2124;
	mov.b32 	%r1061, %r2121;
	.loc	1 47 12
	shl.b32 	%r1744, %r1064, 3;
	mov.b32 	%r1745, global_smem;
	add.s32 	%r1746, %r1745, %r1744;
	add.s32 	%r1074, %r1746, 49176;
$L__BB0_30:
	mov.b32 	%r1747, 10000000;
	mbarrier.try_wait.parity.acquire.cta.shared::cta.b64 %p51, [%r1074], %r2125, %r1747;
	not.pred 	%p52, %p51;
	@%p52 bra 	$L__BB0_30;
	add.s32 	%r1748, %r1064, 1;
	mul.hi.u32 	%r1749, %r1748, -1431655765;
	shr.u32 	%r1750, %r1749, 1;
	and.b32 	%r1751, %r1750, 1;
	xor.b32 	%r2125, %r1751, %r2125;
	mul.lo.s32 	%r1752, %r1750, 3;
	sub.s32 	%r2124, %r1748, %r1752;
	add.s32 	%r2126, %r2126, 1;
	add.s32 	%r1078, %r1074, -24;
	shl.b32 	%r1079, %r2127, 5;
	elect.sync 	%r1753|%p53, -1;
	not.pred 	%p54, %p53;
	@%p54 bra 	$L__BB0_33;
	shl.b32 	%r1754, %r1064, 13;
	mov.b32 	%r1755, global_smem;
	add.s32 	%r1756, %r1754, %r1755;
	mov.b64 	%rd9, 0;
	cp.async.bulk.tensor.2d.shared::cta.global.tile.mbarrier::complete_tx::bytes [%r1756], [%rd1, {%r1079, %r1059}], [%r1078];
$L__BB0_33:
	elect.sync 	%r1757|%p55, -1;
	not.pred 	%p56, %p55;
	@%p56 bra 	$L__BB0_35;
	mov.b32 	%r1758, 8192;
	mbarrier.arrive.expect_tx.release.cta.shared::cta.b64 _, [%r1078], %r1758;
$L__BB0_35:
	.loc	1 48 12
	shl.b32 	%r1759, %r1061, 3;
	mov.b32 	%r1760, global_smem;
	add.s32 	%r1761, %r1760, %r1759;
	add.s32 	%r1080, %r1761, 49216;
$L__BB0_36:
	mov.b32 	%r1762, 10000000;
	mbarrier.try_wait.parity.acquire.cta.shared::cta.b64 %p57, [%r1080], %r2122, %r1762;
	not.pred 	%p58, %p57;
	@%p58 bra 	$L__BB0_36;
	add.s32 	%r1763, %r1061, 1;
	shr.u32 	%r1764, %r1763, 1;
	and.b32 	%r1765, %r1764, 1;
	xor.b32 	%r2122, %r1765, %r2122;
	and.b32 	%r2121, %r1763, 1;
	add.s32 	%r2123, %r2123, 1;
	add.s32 	%r1084, %r1080, -16;
	elect.sync 	%r1766|%p59, -1;
	not.pred 	%p60, %p59;
	@%p60 bra 	$L__BB0_39;
	shl.b32 	%r1767, %r1061, 12;
	mov.b32 	%r1768, global_smem;
	add.s32 	%r1769, %r1768, %r1767;
	mov.b64 	%rd10, 0;
	cp.async.bulk.tensor.2d.shared::cta.global.tile.mbarrier::complete_tx::bytes [%r1769+40960], [%rd2, {%r1060, %r1079}], [%r1084];
$L__BB0_39:
	elect.sync 	%r1770|%p61, -1;
	not.pred 	%p62, %p61;
	@%p62 bra 	$L__BB0_41;
	mov.b32 	%r1771, 4096;
	mbarrier.arrive.expect_tx.release.cta.shared::cta.b64 _, [%r1084], %r1771;
$L__BB0_41:
	.loc	1 46 4
	add.s32 	%r2127, %r2127, 1;
	setp.ne.s32 	%p63, %r2127, %r1058;
	@%p63 bra 	$L__BB0_29;
$L__BB0_42:
	.loc	1 25 0
	add.s32 	%r1772, %r2, -6;
	setp.lt.u32 	%p64, %r1772, 2;
	@%p64 bra 	$L__BB0_50;
	setp.ne.s32 	%p65, %r2, 5;
	@%p65 bra 	$L__BB0_51;
	bra.uni 	$L__BB0_47;
$L__BB0_44:
	add.s32 	%r1101, %r2, -6;
	setp.lt.u32 	%p4, %r1101, 2;
	@%p4 bra 	$L__BB0_50;
	setp.eq.s32 	%p5, %r2, 4;
	@%p5 bra 	$L__BB0_27;
	setp.eq.s32 	%p6, %r2, 5;
	@%p6 bra 	$L__BB0_47;
	bra.uni 	$L__BB0_51;
$L__BB0_47:
	.loc	1 0 0
	ld.param.b32 	%r1820, [matmul_kernel_param_7];
	ld.param.b32 	%r1816, [matmul_kernel_param_1];
	.loc	1 25 0
	setmaxnreg.dec.sync.aligned.u32 	32;
	.loc	1 13 10, function_name $L__info_string0, inlined_at 1 38 17
	mov.u32 	%r1773, %clusterid.x;
	.loc	1 15 16, function_name $L__info_string0, inlined_at 1 38 17
	shr.s32 	%r1774, %r1820, 31;
	shr.u32 	%r1775, %r1774, 26;
	add.s32 	%r1776, %r1820, %r1775;
	shr.s32 	%r1777, %r1776, 6;
	and.b32 	%r1778, %r1776, -64;
	setp.ne.s32 	%p66, %r1820, %r1778;
	setp.gt.s32 	%p67, %r1820, -1;
	and.pred 	%p68, %p67, %p66;
	selp.b32 	%r1779, 1, 0, %p68;
	add.s32 	%r1780, %r1777, %r1779;
	.loc	1 16 23, function_name $L__info_string0, inlined_at 1 38 17
	shl.b32 	%r1781, %r1780, 3;
	.loc	1 17 15, function_name $L__info_string0, inlined_at 1 38 17
	div.s32 	%r1782, %r1773, %r1781;
	mul.lo.s32 	%r1783, %r1782, %r1781;
	setp.ne.s32 	%p69, %r1773, %r1783;
	setp.lt.s32 	%p70, %r1780, 0;
	and.pred 	%p71, %p70, %p69;
	selp.b32 	%r1784, -1, 0, %p71;
	add.s32 	%r1785, %r1782, %r1784;
	.loc	1 14 16, function_name $L__info_string0, inlined_at 1 38 17
	shr.s32 	%r1786, %r1816, 31;
	shr.u32 	%r1787, %r1786, 25;
	add.s32 	%r1788, %r1816, %r1787;
	shr.s32 	%r1789, %r1788, 7;
	and.b32 	%r1790, %r1788, -128;
	setp.ne.s32 	%p72, %r1816, %r1790;
	setp.gt.s32 	%p73, %r1816, -1;
	and.pred 	%p74, %p73, %p72;
	selp.b32 	%r1791, 1, 0, %p74;
	add.s32 	%r1792, %r1789, %r1791;
	.loc	1 18 18, function_name $L__info_string0, inlined_at 1 38 17
	shl.b32 	%r1793, %r1785, 3;
	.loc	1 19 23, function_name $L__info_string0, inlined_at 1 38 17
	sub.s32 	%r1794, %r1792, %r1793;
	.loc	1 19 19, function_name $L__info_string0, inlined_at 1 38 17
	min.s32 	%r1795, %r1794, 8;
	.loc	1 20 27, function_name $L__info_string0, inlined_at 1 38 17
	rem.s32 	%r1796, %r1773, %r1795;
	add.s32 	%r1797, %r1795, %r1796;
	.loc	1 21 13, function_name $L__info_string0, inlined_at 1 38 17
	sub.s32 	%r1798, %r1773, %r1783;
	add.s32 	%r1799, %r1798, %r1781;
	.loc	1 20 27, function_name $L__info_string0, inlined_at 1 38 17
	setp.lt.s32 	%p75, %r1794, 0;
	setp.ne.s32 	%p76, %r1796, 0;
	selp.b32 	%r1800, %r1797, %r1796, %p75;
	selp.b32 	%r1801, %r1800, %r1796, %p76;
	.loc	1 21 13, function_name $L__info_string0, inlined_at 1 38 17
	setp.ne.s32 	%p77, %r1798, 0;
	selp.b32 	%r1802, %r1799, %r1798, %p77;
	selp.b32 	%r1803, %r1802, %r1798, %p70;
	.loc	1 21 12, function_name $L__info_string0, inlined_at 1 38 17
	div.s32 	%r1804, %r1803, %r1795;
	mul.lo.s32 	%r1805, %r1795, %r1804;
	setp.ne.s32 	%p78, %r1803, %r1805;
	xor.b32 	%r1806, %r1803, %r1794;
	setp.lt.s32 	%p79, %r1806, 0;
	and.pred 	%p80, %p79, %p78;
	selp.b32 	%r1807, -1, 0, %p80;
	add.s32 	%r1092, %r1804, %r1807;
	.loc	1 47 12
	add.s32 	%r1093, %r1801, %r1793;
	.loc	1 52 4
	cvta.param.u64 	%rd3, %rd6;
	// begin inline asm
	prefetch.tensormap [%rd3];
	// end inline asm
$L__BB0_48:
	mov.b32 	%r1808, global_smem;
	add.s32 	%r1809, %r1808, 49264;
	mov.b32 	%r1810, 10000000;
	mbarrier.try_wait.parity.acquire.cta.shared::cta.b64 %p81, [%r1809], 0, %r1810;
	not.pred 	%p82, %p81;
	@%p82 bra 	$L__BB0_48;
	shl.b32 	%r1811, %r1093, 7;
	shl.b32 	%r1812, %r1092, 6;
	mov.b32 	%r1813, global_smem;
	add.s32 	%r1814, %r1813, 24576;
	cp.async.bulk.tensor.2d.global.shared::cta.tile.bulk_group [%rd3, {%r1812, %r1811}], [%r1814];
	cp.async.bulk.commit_group;
	.loc	1 25 0
	bra.uni 	$L__BB0_51;
$L__BB0_50:
	setmaxnreg.dec.sync.aligned.u32 	32;
$L__BB0_51:
	ret;

}
	.file	1 "/home/junshan/cuTile/experiment/0210/01/matmul.py"
	.section	.debug_str
	{
$L__info_string0:
.b8 109
.b8 97
.b8 116
.b8 109
.b8 117
.b8 108
.b8 95
.b8 107
.b8 101
.b8 114
.b8 110
.b8 101
.b8 108
.b8 0
	}
