//
// Generated by LLVM NVPTX Back-End
//

.version 8.8
.target sm_120a
.address_size 64

	// .globl	matmul_kernel           // -- Begin function matmul_kernel
.extern .shared .align 16 .b8 global_smem[];
                                        // @matmul_kernel
.visible .entry matmul_kernel(
	.param .u64 .ptr .global .align 1 matmul_kernel_param_0,
	.param .u64 .ptr .global .align 1 matmul_kernel_param_1,
	.param .u64 .ptr .global .align 1 matmul_kernel_param_2,
	.param .u32 matmul_kernel_param_3,
	.param .u32 matmul_kernel_param_4,
	.param .u32 matmul_kernel_param_5,
	.param .u32 matmul_kernel_param_6,
	.param .u32 matmul_kernel_param_7,
	.param .u32 matmul_kernel_param_8,
	.param .u64 .ptr .global .align 1 matmul_kernel_param_9,
	.param .u64 .ptr .global .align 1 matmul_kernel_param_10
)
.reqntid 128
{
	.reg .pred 	%p<39>;
	.reg .b32 	%r<1065>;
	.reg .b64 	%rd<198>;
	.loc	1 25 0                          // dump_ptx.py:25:0
$L__func_begin0:
	.loc	1 25 0                          // dump_ptx.py:25:0

// %bb.0:
	ld.param.b32 	%r42, [matmul_kernel_param_8];
	ld.param.b32 	%r41, [matmul_kernel_param_7];
	ld.param.b32 	%r40, [matmul_kernel_param_5];
	ld.param.b32 	%r39, [matmul_kernel_param_4];
	ld.param.b32 	%r38, [matmul_kernel_param_3];
	ld.param.b64 	%rd48, [matmul_kernel_param_2];
	ld.param.b64 	%rd47, [matmul_kernel_param_1];
	ld.param.b64 	%rd196, [matmul_kernel_param_0];
$L__tmp0:
	.loc	1 36 24                         // dump_ptx.py:36:24
	mov.u32 	%r101, %ctaid.x;
$L__tmp1:
	.loc	2 43 17                         // standard.py:43:17 @[ dump_ptx.py:37:27 ]
	add.s32 	%r102, %r38, 127;
	.loc	2 43 30                         // standard.py:43:30 @[ dump_ptx.py:37:27 ]
	shr.s32 	%r103, %r102, 31;
	shr.u32 	%r104, %r103, 25;
	add.s32 	%r105, %r102, %r104;
	shr.s32 	%r106, %r105, 7;
$L__tmp2:
	.loc	2 43 17                         // standard.py:43:17 @[ dump_ptx.py:38:27 ]
	add.s32 	%r107, %r39, 255;
	.loc	2 43 30                         // standard.py:43:30 @[ dump_ptx.py:38:27 ]
	shr.s32 	%r108, %r107, 31;
	shr.u32 	%r109, %r108, 24;
	add.s32 	%r110, %r107, %r109;
	shr.s32 	%r111, %r110, 8;
$L__tmp3:
	.loc	1 39 38                         // dump_ptx.py:39:38
	shl.b32 	%r112, %r111, 3;
	ld.param.b32 	%r113, [matmul_kernel_param_6];
	.loc	1 40 22                         // dump_ptx.py:40:22
	div.s32 	%r114, %r101, %r112;
	.loc	1 41 29                         // dump_ptx.py:41:29
	shl.b32 	%r115, %r114, 3;
	.loc	1 42 35                         // dump_ptx.py:42:35
	sub.s32 	%r116, %r106, %r115;
	.loc	1 42 48                         // dump_ptx.py:42:48
	min.s32 	%r117, %r116, 8;
	.loc	1 43 34                         // dump_ptx.py:43:34
	mul.lo.s32 	%r118, %r114, %r112;
	sub.s32 	%r119, %r101, %r118;
	.loc	1 44 40                         // dump_ptx.py:44:40
	div.s32 	%r120, %r119, %r117;
	.loc	1 43 54                         // dump_ptx.py:43:54
	mul.lo.s32 	%r121, %r120, %r117;
	sub.s32 	%r122, %r119, %r121;
	.loc	1 43 27                         // dump_ptx.py:43:27
	add.s32 	%r123, %r122, %r115;
	.loc	1 46 23                         // dump_ptx.py:46:23
	shl.b32 	%r1, %r123, 7;
	.loc	1 46 51                         // dump_ptx.py:46:51
	mov.u32 	%r2, %tid.x;
	shr.u32 	%r124, %r2, 3;
	bfe.u32 	%r125, %r2, 3, 4;
	and.b32 	%r3, %r2, 96;
	bfe.u32 	%r4, %r2, 5, 2;
	or.b32 	%r5, %r4, 4;
	or.b32 	%r6, %r4, 8;
	or.b32 	%r7, %r4, 12;
	or.b32 	%r8, %r4, 16;
	or.b32 	%r9, %r4, 20;
	or.b32 	%r10, %r4, 24;
	or.b32 	%r11, %r4, 28;
	or.b32 	%r12, %r4, 32;
	or.b32 	%r13, %r4, 36;
	or.b32 	%r14, %r4, 40;
	or.b32 	%r15, %r4, 44;
	or.b32 	%r16, %r4, 48;
	or.b32 	%r17, %r4, 52;
	or.b32 	%r18, %r4, 56;
	or.b32 	%r19, %r4, 60;
	.loc	1 46 38                         // dump_ptx.py:46:38
	or.b32 	%r126, %r1, %r125;
	or.b32 	%r127, %r126, 16;
	or.b32 	%r128, %r126, 32;
	or.b32 	%r129, %r126, 48;
	or.b32 	%r130, %r126, 64;
	or.b32 	%r131, %r126, 80;
	or.b32 	%r132, %r126, 96;
	or.b32 	%r133, %r1, %r124;
	or.b32 	%r134, %r133, 112;
	.loc	1 46 68                         // dump_ptx.py:46:68
	rem.s32 	%r135, %r126, %r38;
	rem.s32 	%r136, %r127, %r38;
	rem.s32 	%r137, %r128, %r38;
	rem.s32 	%r138, %r129, %r38;
	rem.s32 	%r139, %r130, %r38;
	rem.s32 	%r140, %r131, %r38;
	rem.s32 	%r141, %r132, %r38;
	rem.s32 	%r142, %r134, %r38;
	.loc	1 47 23                         // dump_ptx.py:47:23
	shl.b32 	%r143, %r120, 8;
	.loc	1 47 51                         // dump_ptx.py:47:51
	and.b32 	%r20, %r2, 31;
	shl.b32 	%r144, %r20, 3;
	.loc	1 47 38                         // dump_ptx.py:47:38
	or.b32 	%r21, %r143, %r144;
	.loc	1 47 68                         // dump_ptx.py:47:68
	rem.s32 	%r22, %r21, %r39;
	.loc	1 49 60                         // dump_ptx.py:49:60
	and.b32 	%r23, %r2, 7;
	shl.b32 	%r24, %r23, 3;
	.loc	1 49 53                         // dump_ptx.py:49:53
	mad.lo.s32 	%r145, %r135, %r113, %r24;
	mad.lo.s32 	%r146, %r136, %r113, %r24;
	mad.lo.s32 	%r147, %r137, %r113, %r24;
	mad.lo.s32 	%r148, %r138, %r113, %r24;
	mad.lo.s32 	%r149, %r139, %r113, %r24;
	mad.lo.s32 	%r150, %r140, %r113, %r24;
	mad.lo.s32 	%r151, %r141, %r113, %r24;
	mad.lo.s32 	%r152, %r142, %r113, %r24;
	.loc	1 49 22                         // dump_ptx.py:49:22
	mad.wide.s32 	%rd49, %r145, 2, %rd196;
	mad.wide.s32 	%rd50, %r146, 2, %rd196;
	mad.wide.s32 	%rd51, %r147, 2, %rd196;
	mad.wide.s32 	%rd52, %r148, 2, %rd196;
	mad.wide.s32 	%rd53, %r149, 2, %rd196;
	mad.wide.s32 	%rd54, %r150, 2, %rd196;
	mad.wide.s32 	%rd55, %r151, 2, %rd196;
	mad.wide.s32 	%rd56, %r152, 2, %rd196;
	.loc	1 50 40                         // dump_ptx.py:50:40
	mul.lo.s32 	%r153, %r41, %r4;
	shl.b32 	%r154, %r41, 2;
	add.s32 	%r155, %r153, %r154;
	add.s32 	%r156, %r155, %r154;
	add.s32 	%r157, %r156, %r154;
	add.s32 	%r158, %r157, %r154;
	add.s32 	%r159, %r158, %r154;
	add.s32 	%r160, %r159, %r154;
	add.s32 	%r161, %r160, %r154;
	add.s32 	%r162, %r161, %r154;
	add.s32 	%r163, %r162, %r154;
	add.s32 	%r164, %r163, %r154;
	add.s32 	%r165, %r164, %r154;
	add.s32 	%r166, %r165, %r154;
	add.s32 	%r167, %r166, %r154;
	add.s32 	%r168, %r167, %r154;
	add.s32 	%r169, %r168, %r154;
	.loc	1 50 52                         // dump_ptx.py:50:52
	add.s32 	%r170, %r22, %r153;
	add.s32 	%r171, %r22, %r155;
	add.s32 	%r172, %r22, %r156;
	add.s32 	%r173, %r22, %r157;
	add.s32 	%r174, %r22, %r158;
	add.s32 	%r175, %r22, %r159;
	add.s32 	%r176, %r22, %r160;
	add.s32 	%r177, %r22, %r161;
	add.s32 	%r178, %r22, %r162;
	add.s32 	%r179, %r22, %r163;
	add.s32 	%r180, %r22, %r164;
	add.s32 	%r181, %r22, %r165;
	add.s32 	%r182, %r22, %r166;
	add.s32 	%r183, %r22, %r167;
	add.s32 	%r184, %r22, %r168;
	add.s32 	%r185, %r22, %r169;
	.loc	1 50 22                         // dump_ptx.py:50:22
	mad.wide.s32 	%rd57, %r170, 2, %rd47;
	mad.wide.s32 	%rd65, %r171, 2, %rd47;
	mad.wide.s32 	%rd58, %r172, 2, %rd47;
	mad.wide.s32 	%rd66, %r173, 2, %rd47;
	mad.wide.s32 	%rd59, %r174, 2, %rd47;
	mad.wide.s32 	%rd67, %r175, 2, %rd47;
	mad.wide.s32 	%rd60, %r176, 2, %rd47;
	mad.wide.s32 	%rd68, %r177, 2, %rd47;
	mad.wide.s32 	%rd61, %r178, 2, %rd47;
	mad.wide.s32 	%rd69, %r179, 2, %rd47;
	mad.wide.s32 	%rd62, %r180, 2, %rd47;
	mad.wide.s32 	%rd70, %r181, 2, %rd47;
	mad.wide.s32 	%rd63, %r182, 2, %rd47;
	mad.wide.s32 	%rd71, %r183, 2, %rd47;
	mad.wide.s32 	%rd64, %r184, 2, %rd47;
	mad.wide.s32 	%rd72, %r185, 2, %rd47;
$L__tmp4:
	.loc	2 43 17                         // standard.py:43:17 @[ dump_ptx.py:53:33 ]
	add.s32 	%r186, %r40, 63;
$L__tmp5:
	.loc	1 58 33                         // dump_ptx.py:58:33
	shl.b32 	%r190, %r41, 6;
	.loc	1 53 22                         // dump_ptx.py:53:22
	setp.gt.s32 	%p1, %r186, 63;
	.loc	1 54 51                         // dump_ptx.py:54:51
	setp.lt.s32 	%p2, %r24, %r40;
	.loc	1 54 20                         // dump_ptx.py:54:20
	shl.b32 	%r191, %r2, 4;
	and.b32 	%r192, %r191, 2032;
	shl.b32 	%r26, %r2, 1;
	and.b32 	%r193, %r26, 112;
	xor.b32 	%r27, %r192, %r193;
	mov.b32 	%r194, global_smem;
	add.s32 	%r195, %r194, %r27;
	add.s32 	%r43, %r195, 65536;
	selp.b32 	%r196, 16, 0, %p1;
	selp.b32 	%r45, %r196, 0, %p2;
	// begin inline asm
	cp.async.cg.shared.global [ %r43 + 0 ], [ %rd49 + 0 ], 0x10, %r45;
	// end inline asm
	add.s32 	%r44, %r195, 67584;
	// begin inline asm
	cp.async.cg.shared.global [ %r44 + 0 ], [ %rd50 + 0 ], 0x10, %r45;
	// end inline asm
	add.s32 	%r46, %r195, 69632;
	// begin inline asm
	cp.async.cg.shared.global [ %r46 + 0 ], [ %rd51 + 0 ], 0x10, %r45;
	// end inline asm
	add.s32 	%r47, %r195, 71680;
	// begin inline asm
	cp.async.cg.shared.global [ %r47 + 0 ], [ %rd52 + 0 ], 0x10, %r45;
	// end inline asm
	add.s32 	%r48, %r195, 73728;
	// begin inline asm
	cp.async.cg.shared.global [ %r48 + 0 ], [ %rd53 + 0 ], 0x10, %r45;
	// end inline asm
	add.s32 	%r49, %r195, 75776;
	// begin inline asm
	cp.async.cg.shared.global [ %r49 + 0 ], [ %rd54 + 0 ], 0x10, %r45;
	// end inline asm
	add.s32 	%r50, %r195, 77824;
	// begin inline asm
	cp.async.cg.shared.global [ %r50 + 0 ], [ %rd55 + 0 ], 0x10, %r45;
	// end inline asm
	add.s32 	%r51, %r195, 79872;
	// begin inline asm
	cp.async.cg.shared.global [ %r51 + 0 ], [ %rd56 + 0 ], 0x10, %r45;
	// end inline asm
	cp.async.commit_group;
	.loc	1 55 51                         // dump_ptx.py:55:51
	setp.lt.s32 	%p3, %r4, %r40;
	setp.lt.s32 	%p4, %r8, %r40;
	setp.lt.s32 	%p5, %r12, %r40;
	setp.lt.s32 	%p6, %r16, %r40;
	.loc	1 55 20                         // dump_ptx.py:55:20
	shr.u32 	%r28, %r3, 1;
	xor.b32 	%r29, %r192, %r28;
	add.s32 	%r52, %r194, %r29;
	selp.b32 	%r54, %r196, 0, %p3;
	// begin inline asm
	cp.async.cg.shared.global [ %r52 + 0 ], [ %rd57 + 0 ], 0x10, %r54;
	// end inline asm
	add.s32 	%r53, %r52, 4096;
	// begin inline asm
	cp.async.cg.shared.global [ %r53 + 0 ], [ %rd58 + 0 ], 0x10, %r54;
	// end inline asm
	add.s32 	%r55, %r52, 8192;
	selp.b32 	%r57, %r196, 0, %p4;
	// begin inline asm
	cp.async.cg.shared.global [ %r55 + 0 ], [ %rd59 + 0 ], 0x10, %r57;
	// end inline asm
	add.s32 	%r56, %r52, 12288;
	// begin inline asm
	cp.async.cg.shared.global [ %r56 + 0 ], [ %rd60 + 0 ], 0x10, %r57;
	// end inline asm
	add.s32 	%r58, %r52, 16384;
	selp.b32 	%r60, %r196, 0, %p5;
	// begin inline asm
	cp.async.cg.shared.global [ %r58 + 0 ], [ %rd61 + 0 ], 0x10, %r60;
	// end inline asm
	add.s32 	%r59, %r52, 20480;
	// begin inline asm
	cp.async.cg.shared.global [ %r59 + 0 ], [ %rd62 + 0 ], 0x10, %r60;
	// end inline asm
	add.s32 	%r61, %r52, 24576;
	selp.b32 	%r63, %r196, 0, %p6;
	// begin inline asm
	cp.async.cg.shared.global [ %r61 + 0 ], [ %rd63 + 0 ], 0x10, %r63;
	// end inline asm
	add.s32 	%r62, %r52, 28672;
	// begin inline asm
	cp.async.cg.shared.global [ %r62 + 0 ], [ %rd64 + 0 ], 0x10, %r63;
	// end inline asm
	xor.b32 	%r30, %r29, 64;
	add.s32 	%r197, %r194, %r30;
	add.s32 	%r64, %r197, 2048;
	// begin inline asm
	cp.async.cg.shared.global [ %r64 + 0 ], [ %rd65 + 0 ], 0x10, %r54;
	// end inline asm
	add.s32 	%r65, %r197, 6144;
	// begin inline asm
	cp.async.cg.shared.global [ %r65 + 0 ], [ %rd66 + 0 ], 0x10, %r54;
	// end inline asm
	add.s32 	%r66, %r197, 10240;
	// begin inline asm
	cp.async.cg.shared.global [ %r66 + 0 ], [ %rd67 + 0 ], 0x10, %r57;
	// end inline asm
	add.s32 	%r67, %r197, 14336;
	// begin inline asm
	cp.async.cg.shared.global [ %r67 + 0 ], [ %rd68 + 0 ], 0x10, %r57;
	// end inline asm
	add.s32 	%r68, %r197, 18432;
	// begin inline asm
	cp.async.cg.shared.global [ %r68 + 0 ], [ %rd69 + 0 ], 0x10, %r60;
	// end inline asm
	add.s32 	%r69, %r197, 22528;
	// begin inline asm
	cp.async.cg.shared.global [ %r69 + 0 ], [ %rd70 + 0 ], 0x10, %r60;
	// end inline asm
	add.s32 	%r70, %r197, 26624;
	// begin inline asm
	cp.async.cg.shared.global [ %r70 + 0 ], [ %rd71 + 0 ], 0x10, %r63;
	// end inline asm
	add.s32 	%r71, %r197, 30720;
	// begin inline asm
	cp.async.cg.shared.global [ %r71 + 0 ], [ %rd72 + 0 ], 0x10, %r63;
	// end inline asm
	cp.async.commit_group;
	.loc	1 53 22                         // dump_ptx.py:53:22
	setp.gt.s32 	%p7, %r186, 127;
	.loc	1 57 18                         // dump_ptx.py:57:18
	add.s64 	%rd73, %rd49, 128;
	add.s64 	%rd74, %rd50, 128;
	add.s64 	%rd75, %rd51, 128;
	add.s64 	%rd76, %rd52, 128;
	add.s64 	%rd77, %rd53, 128;
	add.s64 	%rd78, %rd54, 128;
	add.s64 	%rd79, %rd55, 128;
	add.s64 	%rd80, %rd56, 128;
	.loc	1 58 18                         // dump_ptx.py:58:18
	mul.wide.s32 	%rd97, %r190, 2;
	add.s64 	%rd81, %rd57, %rd97;
	add.s64 	%rd89, %rd65, %rd97;
	add.s64 	%rd82, %rd58, %rd97;
	add.s64 	%rd90, %rd66, %rd97;
	add.s64 	%rd83, %rd59, %rd97;
	add.s64 	%rd91, %rd67, %rd97;
	add.s64 	%rd84, %rd60, %rd97;
	add.s64 	%rd92, %rd68, %rd97;
	add.s64 	%rd85, %rd61, %rd97;
	add.s64 	%rd93, %rd69, %rd97;
	add.s64 	%rd86, %rd62, %rd97;
	add.s64 	%rd94, %rd70, %rd97;
	add.s64 	%rd87, %rd63, %rd97;
	add.s64 	%rd95, %rd71, %rd97;
	add.s64 	%rd88, %rd64, %rd97;
	add.s64 	%rd96, %rd72, %rd97;
	.loc	1 54 55                         // dump_ptx.py:54:55
	add.s32 	%r198, %r40, -64;
	.loc	1 54 51                         // dump_ptx.py:54:51
	setp.lt.s32 	%p8, %r24, %r198;
	.loc	1 54 20                         // dump_ptx.py:54:20
	bar.sync 	0;
	add.s32 	%r72, %r195, 81920;
	selp.b32 	%r199, 16, 0, %p8;
	selp.b32 	%r74, %r199, 0, %p7;
	// begin inline asm
	cp.async.cg.shared.global [ %r72 + 0 ], [ %rd73 + 0 ], 0x10, %r74;
	// end inline asm
	add.s32 	%r73, %r195, 83968;
	// begin inline asm
	cp.async.cg.shared.global [ %r73 + 0 ], [ %rd74 + 0 ], 0x10, %r74;
	// end inline asm
	add.s32 	%r75, %r195, 86016;
	// begin inline asm
	cp.async.cg.shared.global [ %r75 + 0 ], [ %rd75 + 0 ], 0x10, %r74;
	// end inline asm
	add.s32 	%r76, %r195, 88064;
	// begin inline asm
	cp.async.cg.shared.global [ %r76 + 0 ], [ %rd76 + 0 ], 0x10, %r74;
	// end inline asm
	add.s32 	%r77, %r195, 90112;
	// begin inline asm
	cp.async.cg.shared.global [ %r77 + 0 ], [ %rd77 + 0 ], 0x10, %r74;
	// end inline asm
	add.s32 	%r78, %r195, 92160;
	// begin inline asm
	cp.async.cg.shared.global [ %r78 + 0 ], [ %rd78 + 0 ], 0x10, %r74;
	// end inline asm
	add.s32 	%r79, %r195, 94208;
	// begin inline asm
	cp.async.cg.shared.global [ %r79 + 0 ], [ %rd79 + 0 ], 0x10, %r74;
	// end inline asm
	add.s32 	%r80, %r195, 96256;
	// begin inline asm
	cp.async.cg.shared.global [ %r80 + 0 ], [ %rd80 + 0 ], 0x10, %r74;
	// end inline asm
	cp.async.commit_group;
	.loc	1 55 51                         // dump_ptx.py:55:51
	setp.lt.s32 	%p9, %r4, %r198;
	setp.lt.s32 	%p10, %r8, %r198;
	setp.lt.s32 	%p11, %r12, %r198;
	setp.lt.s32 	%p12, %r16, %r198;
	.loc	1 55 20                         // dump_ptx.py:55:20
	add.s32 	%r81, %r52, 32768;
	selp.b32 	%r200, 16, 0, %p9;
	selp.b32 	%r83, %r200, 0, %p7;
	// begin inline asm
	cp.async.cg.shared.global [ %r81 + 0 ], [ %rd81 + 0 ], 0x10, %r83;
	// end inline asm
	add.s32 	%r82, %r52, 36864;
	// begin inline asm
	cp.async.cg.shared.global [ %r82 + 0 ], [ %rd82 + 0 ], 0x10, %r83;
	// end inline asm
	add.s32 	%r84, %r52, 40960;
	selp.b32 	%r201, 16, 0, %p10;
	selp.b32 	%r86, %r201, 0, %p7;
	// begin inline asm
	cp.async.cg.shared.global [ %r84 + 0 ], [ %rd83 + 0 ], 0x10, %r86;
	// end inline asm
	add.s32 	%r85, %r52, 45056;
	// begin inline asm
	cp.async.cg.shared.global [ %r85 + 0 ], [ %rd84 + 0 ], 0x10, %r86;
	// end inline asm
	add.s32 	%r87, %r52, 49152;
	selp.b32 	%r202, 16, 0, %p11;
	selp.b32 	%r89, %r202, 0, %p7;
	// begin inline asm
	cp.async.cg.shared.global [ %r87 + 0 ], [ %rd85 + 0 ], 0x10, %r89;
	// end inline asm
	add.s32 	%r88, %r52, 53248;
	// begin inline asm
	cp.async.cg.shared.global [ %r88 + 0 ], [ %rd86 + 0 ], 0x10, %r89;
	// end inline asm
	add.s32 	%r90, %r52, 57344;
	selp.b32 	%r203, 16, 0, %p12;
	selp.b32 	%r92, %r203, 0, %p7;
	// begin inline asm
	cp.async.cg.shared.global [ %r90 + 0 ], [ %rd87 + 0 ], 0x10, %r92;
	// end inline asm
	add.s32 	%r91, %r52, 61440;
	// begin inline asm
	cp.async.cg.shared.global [ %r91 + 0 ], [ %rd88 + 0 ], 0x10, %r92;
	// end inline asm
	add.s32 	%r93, %r197, 34816;
	// begin inline asm
	cp.async.cg.shared.global [ %r93 + 0 ], [ %rd89 + 0 ], 0x10, %r83;
	// end inline asm
	add.s32 	%r94, %r197, 38912;
	// begin inline asm
	cp.async.cg.shared.global [ %r94 + 0 ], [ %rd90 + 0 ], 0x10, %r83;
	// end inline asm
	add.s32 	%r95, %r197, 43008;
	// begin inline asm
	cp.async.cg.shared.global [ %r95 + 0 ], [ %rd91 + 0 ], 0x10, %r86;
	// end inline asm
	add.s32 	%r96, %r197, 47104;
	// begin inline asm
	cp.async.cg.shared.global [ %r96 + 0 ], [ %rd92 + 0 ], 0x10, %r86;
	// end inline asm
	add.s32 	%r97, %r197, 51200;
	// begin inline asm
	cp.async.cg.shared.global [ %r97 + 0 ], [ %rd93 + 0 ], 0x10, %r89;
	// end inline asm
	add.s32 	%r98, %r197, 55296;
	// begin inline asm
	cp.async.cg.shared.global [ %r98 + 0 ], [ %rd94 + 0 ], 0x10, %r89;
	// end inline asm
	add.s32 	%r99, %r197, 59392;
	// begin inline asm
	cp.async.cg.shared.global [ %r99 + 0 ], [ %rd95 + 0 ], 0x10, %r92;
	// end inline asm
	add.s32 	%r100, %r197, 63488;
	// begin inline asm
	cp.async.cg.shared.global [ %r100 + 0 ], [ %rd96 + 0 ], 0x10, %r92;
	// end inline asm
	cp.async.commit_group;
	.loc	1 53 22                         // dump_ptx.py:53:22
	@%p1 bra 	$L__BB0_2;
	bra.uni 	$L__BB0_1;
$L__BB0_2:                              // %.lr.ph
	.loc	1 0 22                          // dump_ptx.py:0:22
	cvt.s64.s32 	%rd1, %r145;
	cvt.s64.s32 	%rd2, %r146;
	cvt.s64.s32 	%rd3, %r147;
	cvt.s64.s32 	%rd4, %r148;
	cvt.s64.s32 	%rd5, %r149;
	cvt.s64.s32 	%rd6, %r150;
	cvt.s64.s32 	%rd7, %r151;
	cvt.s64.s32 	%rd8, %r152;
	cvt.s64.s32 	%rd9, %r170;
	cvt.s64.s32 	%rd10, %r171;
	cvt.s64.s32 	%rd11, %r172;
	cvt.s64.s32 	%rd12, %r173;
	cvt.s64.s32 	%rd13, %r174;
	cvt.s64.s32 	%rd14, %r175;
	cvt.s64.s32 	%rd15, %r176;
	cvt.s64.s32 	%rd16, %r177;
	cvt.s64.s32 	%rd17, %r178;
	cvt.s64.s32 	%rd18, %r179;
	cvt.s64.s32 	%rd19, %r180;
	cvt.s64.s32 	%rd20, %r181;
	shr.s32 	%r187, %r186, 31;
	shr.u32 	%r188, %r187, 26;
	add.s32 	%r189, %r186, %r188;
	shr.s32 	%r25, %r189, 6;
	cvt.s64.s32 	%rd21, %r190;
	add.s32 	%r31, %r25, -2;
	shl.b32 	%r204, %r2, 7;
	and.b32 	%r205, %r204, 1920;
	shl.b32 	%r936, %r23, 4;
	and.b32 	%r206, %r2, 16;
	or.b32 	%r207, %r205, %r936;
	xor.b32 	%r32, %r207, %r206;
	xor.b32 	%r33, %r32, 32;
	xor.b32 	%r34, %r32, 64;
	xor.b32 	%r35, %r32, 96;
	shl.b32 	%r208, %r20, 9;
	or.b32 	%r209, %r208, %r936;
	xor.b32 	%r36, %r209, %r28;
	xor.b32 	%r37, %r36, 64;
	add.s32 	%r676, %r40, -128;
	.loc	1 53 22                         // dump_ptx.py:53:22
	mad.lo.s32 	%r210, %r41, %r19, %r22;
	mul.wide.s32 	%rd22, %r210, 2;
	shl.b64 	%rd98, %rd21, 2;
	add.s64 	%rd197, %rd47, %rd98;
	shl.b64 	%rd23, %rd21, 1;
	mad.lo.s32 	%r211, %r41, %r18, %r22;
	mul.wide.s32 	%rd24, %r211, 2;
	mad.lo.s32 	%r212, %r41, %r17, %r22;
	mul.wide.s32 	%rd25, %r212, 2;
	mad.lo.s32 	%r213, %r41, %r16, %r22;
	mul.wide.s32 	%rd26, %r213, 2;
	shl.b64 	%rd27, %rd20, 1;
	shl.b64 	%rd28, %rd19, 1;
	shl.b64 	%rd29, %rd18, 1;
	shl.b64 	%rd30, %rd17, 1;
	shl.b64 	%rd31, %rd16, 1;
	shl.b64 	%rd32, %rd15, 1;
	shl.b64 	%rd33, %rd14, 1;
	shl.b64 	%rd34, %rd13, 1;
	shl.b64 	%rd35, %rd12, 1;
	shl.b64 	%rd36, %rd11, 1;
	shl.b64 	%rd37, %rd10, 1;
	shl.b64 	%rd38, %rd9, 1;
	shl.b64 	%rd99, %rd8, 1;
	add.s64 	%rd39, %rd99, 256;
	shl.b64 	%rd100, %rd7, 1;
	add.s64 	%rd40, %rd100, 256;
	shl.b64 	%rd101, %rd6, 1;
	add.s64 	%rd41, %rd101, 256;
	shl.b64 	%rd102, %rd5, 1;
	add.s64 	%rd42, %rd102, 256;
	shl.b64 	%rd103, %rd4, 1;
	add.s64 	%rd43, %rd103, 256;
	shl.b64 	%rd104, %rd3, 1;
	add.s64 	%rd44, %rd104, 256;
	shl.b64 	%rd105, %rd2, 1;
	add.s64 	%rd45, %rd105, 256;
	shl.b64 	%rd106, %rd1, 1;
	add.s64 	%rd46, %rd106, 256;
	mov.b32 	%r935, 0;
	mov.b32 	%r679, 0f00000000;
	mov.b32 	%r678, 1;
	mov.b32 	%r677, -1;
	mov.b32 	%r680, %r679;
	mov.b32 	%r681, %r679;
	mov.b32 	%r682, %r679;
	mov.b32 	%r683, %r679;
	mov.b32 	%r684, %r679;
	mov.b32 	%r685, %r679;
	mov.b32 	%r686, %r679;
	mov.b32 	%r687, %r679;
	mov.b32 	%r688, %r679;
	mov.b32 	%r689, %r679;
	mov.b32 	%r690, %r679;
	mov.b32 	%r691, %r679;
	mov.b32 	%r692, %r679;
	mov.b32 	%r693, %r679;
	mov.b32 	%r694, %r679;
	mov.b32 	%r695, %r679;
	mov.b32 	%r696, %r679;
	mov.b32 	%r697, %r679;
	mov.b32 	%r698, %r679;
	mov.b32 	%r699, %r679;
	mov.b32 	%r700, %r679;
	mov.b32 	%r701, %r679;
	mov.b32 	%r702, %r679;
	mov.b32 	%r703, %r679;
	mov.b32 	%r704, %r679;
	mov.b32 	%r705, %r679;
	mov.b32 	%r706, %r679;
	mov.b32 	%r707, %r679;
	mov.b32 	%r708, %r679;
	mov.b32 	%r709, %r679;
	mov.b32 	%r710, %r679;
	mov.b32 	%r711, %r679;
	mov.b32 	%r712, %r679;
	mov.b32 	%r713, %r679;
	mov.b32 	%r714, %r679;
	mov.b32 	%r715, %r679;
	mov.b32 	%r716, %r679;
	mov.b32 	%r717, %r679;
	mov.b32 	%r718, %r679;
	mov.b32 	%r719, %r679;
	mov.b32 	%r720, %r679;
	mov.b32 	%r721, %r679;
	mov.b32 	%r722, %r679;
	mov.b32 	%r723, %r679;
	mov.b32 	%r724, %r679;
	mov.b32 	%r725, %r679;
	mov.b32 	%r726, %r679;
	mov.b32 	%r727, %r679;
	mov.b32 	%r728, %r679;
	mov.b32 	%r729, %r679;
	mov.b32 	%r730, %r679;
	mov.b32 	%r731, %r679;
	mov.b32 	%r732, %r679;
	mov.b32 	%r733, %r679;
	mov.b32 	%r734, %r679;
	mov.b32 	%r735, %r679;
	mov.b32 	%r736, %r679;
	mov.b32 	%r737, %r679;
	mov.b32 	%r738, %r679;
	mov.b32 	%r739, %r679;
	mov.b32 	%r740, %r679;
	mov.b32 	%r741, %r679;
	mov.b32 	%r742, %r679;
	mov.b32 	%r743, %r679;
	mov.b32 	%r744, %r679;
	mov.b32 	%r745, %r679;
	mov.b32 	%r746, %r679;
	mov.b32 	%r747, %r679;
	mov.b32 	%r748, %r679;
	mov.b32 	%r749, %r679;
	mov.b32 	%r750, %r679;
	mov.b32 	%r751, %r679;
	mov.b32 	%r752, %r679;
	mov.b32 	%r753, %r679;
	mov.b32 	%r754, %r679;
	mov.b32 	%r755, %r679;
	mov.b32 	%r756, %r679;
	mov.b32 	%r757, %r679;
	mov.b32 	%r758, %r679;
	mov.b32 	%r759, %r679;
	mov.b32 	%r760, %r679;
	mov.b32 	%r761, %r679;
	mov.b32 	%r762, %r679;
	mov.b32 	%r763, %r679;
	mov.b32 	%r764, %r679;
	mov.b32 	%r765, %r679;
	mov.b32 	%r766, %r679;
	mov.b32 	%r767, %r679;
	mov.b32 	%r768, %r679;
	mov.b32 	%r769, %r679;
	mov.b32 	%r770, %r679;
	mov.b32 	%r771, %r679;
	mov.b32 	%r772, %r679;
	mov.b32 	%r773, %r679;
	mov.b32 	%r774, %r679;
	mov.b32 	%r775, %r679;
	mov.b32 	%r776, %r679;
	mov.b32 	%r777, %r679;
	mov.b32 	%r778, %r679;
	mov.b32 	%r779, %r679;
	mov.b32 	%r780, %r679;
	mov.b32 	%r781, %r679;
	mov.b32 	%r782, %r679;
	mov.b32 	%r783, %r679;
	mov.b32 	%r784, %r679;
	mov.b32 	%r785, %r679;
	mov.b32 	%r786, %r679;
	mov.b32 	%r787, %r679;
	mov.b32 	%r788, %r679;
	mov.b32 	%r789, %r679;
	mov.b32 	%r790, %r679;
	mov.b32 	%r791, %r679;
	mov.b32 	%r792, %r679;
	mov.b32 	%r793, %r679;
	mov.b32 	%r794, %r679;
	mov.b32 	%r795, %r679;
	mov.b32 	%r796, %r679;
	mov.b32 	%r797, %r679;
	mov.b32 	%r798, %r679;
	mov.b32 	%r799, %r679;
	mov.b32 	%r800, %r679;
	mov.b32 	%r801, %r679;
	mov.b32 	%r802, %r679;
	mov.b32 	%r803, %r679;
	mov.b32 	%r804, %r679;
	mov.b32 	%r805, %r679;
	mov.b32 	%r806, %r679;
	mov.b32 	%r807, %r679;
	mov.b32 	%r808, %r679;
	mov.b32 	%r809, %r679;
	mov.b32 	%r810, %r679;
	mov.b32 	%r811, %r679;
	mov.b32 	%r812, %r679;
	mov.b32 	%r813, %r679;
	mov.b32 	%r814, %r679;
	mov.b32 	%r815, %r679;
	mov.b32 	%r816, %r679;
	mov.b32 	%r817, %r679;
	mov.b32 	%r818, %r679;
	mov.b32 	%r819, %r679;
	mov.b32 	%r820, %r679;
	mov.b32 	%r821, %r679;
	mov.b32 	%r822, %r679;
	mov.b32 	%r823, %r679;
	mov.b32 	%r824, %r679;
	mov.b32 	%r825, %r679;
	mov.b32 	%r826, %r679;
	mov.b32 	%r827, %r679;
	mov.b32 	%r828, %r679;
	mov.b32 	%r829, %r679;
	mov.b32 	%r830, %r679;
	mov.b32 	%r831, %r679;
	mov.b32 	%r832, %r679;
	mov.b32 	%r833, %r679;
	mov.b32 	%r834, %r679;
	mov.b32 	%r835, %r679;
	mov.b32 	%r836, %r679;
	mov.b32 	%r837, %r679;
	mov.b32 	%r838, %r679;
	mov.b32 	%r839, %r679;
	mov.b32 	%r840, %r679;
	mov.b32 	%r841, %r679;
	mov.b32 	%r842, %r679;
	mov.b32 	%r843, %r679;
	mov.b32 	%r844, %r679;
	mov.b32 	%r845, %r679;
	mov.b32 	%r846, %r679;
	mov.b32 	%r847, %r679;
	mov.b32 	%r848, %r679;
	mov.b32 	%r849, %r679;
	mov.b32 	%r850, %r679;
	mov.b32 	%r851, %r679;
	mov.b32 	%r852, %r679;
	mov.b32 	%r853, %r679;
	mov.b32 	%r854, %r679;
	mov.b32 	%r855, %r679;
	mov.b32 	%r856, %r679;
	mov.b32 	%r857, %r679;
	mov.b32 	%r858, %r679;
	mov.b32 	%r859, %r679;
	mov.b32 	%r860, %r679;
	mov.b32 	%r861, %r679;
	mov.b32 	%r862, %r679;
	mov.b32 	%r863, %r679;
	mov.b32 	%r864, %r679;
	mov.b32 	%r865, %r679;
	mov.b32 	%r866, %r679;
	mov.b32 	%r867, %r679;
	mov.b32 	%r868, %r679;
	mov.b32 	%r869, %r679;
	mov.b32 	%r870, %r679;
	mov.b32 	%r871, %r679;
	mov.b32 	%r872, %r679;
	mov.b32 	%r873, %r679;
	mov.b32 	%r874, %r679;
	mov.b32 	%r875, %r679;
	mov.b32 	%r876, %r679;
	mov.b32 	%r877, %r679;
	mov.b32 	%r878, %r679;
	mov.b32 	%r879, %r679;
	mov.b32 	%r880, %r679;
	mov.b32 	%r881, %r679;
	mov.b32 	%r882, %r679;
	mov.b32 	%r883, %r679;
	mov.b32 	%r884, %r679;
	mov.b32 	%r885, %r679;
	mov.b32 	%r886, %r679;
	mov.b32 	%r887, %r679;
	mov.b32 	%r888, %r679;
	mov.b32 	%r889, %r679;
	mov.b32 	%r890, %r679;
	mov.b32 	%r891, %r679;
	mov.b32 	%r892, %r679;
	mov.b32 	%r893, %r679;
	mov.b32 	%r894, %r679;
	mov.b32 	%r895, %r679;
	mov.b32 	%r896, %r679;
	mov.b32 	%r897, %r679;
	mov.b32 	%r898, %r679;
	mov.b32 	%r899, %r679;
	mov.b32 	%r900, %r679;
	mov.b32 	%r901, %r679;
	mov.b32 	%r902, %r679;
	mov.b32 	%r903, %r679;
	mov.b32 	%r904, %r679;
	mov.b32 	%r905, %r679;
	mov.b32 	%r906, %r679;
	mov.b32 	%r907, %r679;
	mov.b32 	%r908, %r679;
	mov.b32 	%r909, %r679;
	mov.b32 	%r910, %r679;
	mov.b32 	%r911, %r679;
	mov.b32 	%r912, %r679;
	mov.b32 	%r913, %r679;
	mov.b32 	%r914, %r679;
	mov.b32 	%r915, %r679;
	mov.b32 	%r916, %r679;
	mov.b32 	%r917, %r679;
	mov.b32 	%r918, %r679;
	mov.b32 	%r919, %r679;
	mov.b32 	%r920, %r679;
	mov.b32 	%r921, %r679;
	mov.b32 	%r922, %r679;
	mov.b32 	%r923, %r679;
	mov.b32 	%r924, %r679;
	mov.b32 	%r925, %r679;
	mov.b32 	%r926, %r679;
	mov.b32 	%r927, %r679;
	mov.b32 	%r928, %r679;
	mov.b32 	%r929, %r679;
	mov.b32 	%r930, %r679;
	mov.b32 	%r931, %r679;
	mov.b32 	%r932, %r679;
	mov.b32 	%r933, %r679;
	mov.b32 	%r934, %r679;
$L__BB0_3:                              // =>This Inner Loop Header: Depth=1
	setp.lt.s32 	%p13, %r935, %r31;
	add.s32 	%r435, %r677, 1;
	setp.gt.s32 	%p14, %r435, 1;
	selp.b32 	%r677, 0, %r435, %p14;
	.loc	1 54 20                         // dump_ptx.py:54:20
	cp.async.wait_group 	2;
	bar.sync 	0;
	shl.b32 	%r436, %r677, 14;
	add.s32 	%r437, %r194, %r436;
	add.s32 	%r438, %r437, %r32;
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r214, %r215, %r216, %r217}, [%r438+65536];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r234, %r235, %r236, %r237}, [%r438+67584];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r238, %r239, %r240, %r241}, [%r438+69632];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r242, %r243, %r244, %r245}, [%r438+71680];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r246, %r247, %r248, %r249}, [%r438+73728];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r250, %r251, %r252, %r253}, [%r438+75776];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r254, %r255, %r256, %r257}, [%r438+77824];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r258, %r259, %r260, %r261}, [%r438+79872];
	add.s32 	%r439, %r437, %r33;
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r262, %r263, %r264, %r265}, [%r439+65536];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r282, %r283, %r284, %r285}, [%r439+67584];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r286, %r287, %r288, %r289}, [%r439+69632];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r290, %r291, %r292, %r293}, [%r439+71680];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r294, %r295, %r296, %r297}, [%r439+73728];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r298, %r299, %r300, %r301}, [%r439+75776];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r302, %r303, %r304, %r305}, [%r439+77824];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r306, %r307, %r308, %r309}, [%r439+79872];
	add.s32 	%r440, %r437, %r34;
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r310, %r311, %r312, %r313}, [%r440+65536];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r330, %r331, %r332, %r333}, [%r440+67584];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r334, %r335, %r336, %r337}, [%r440+69632];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r338, %r339, %r340, %r341}, [%r440+71680];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r342, %r343, %r344, %r345}, [%r440+73728];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r346, %r347, %r348, %r349}, [%r440+75776];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r350, %r351, %r352, %r353}, [%r440+77824];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r354, %r355, %r356, %r357}, [%r440+79872];
	add.s32 	%r441, %r437, %r35;
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r358, %r359, %r360, %r361}, [%r441+65536];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r378, %r379, %r380, %r381}, [%r441+67584];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r382, %r383, %r384, %r385}, [%r441+69632];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r386, %r387, %r388, %r389}, [%r441+71680];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r390, %r391, %r392, %r393}, [%r441+73728];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r394, %r395, %r396, %r397}, [%r441+75776];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r398, %r399, %r400, %r401}, [%r441+77824];
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%r402, %r403, %r404, %r405}, [%r441+79872];
	.loc	1 55 20                         // dump_ptx.py:55:20
	shl.b32 	%r442, %r677, 15;
	add.s32 	%r443, %r194, %r442;
	add.s32 	%r444, %r443, %r36;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r218, %r219, %r266, %r267}, [%r444];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r314, %r315, %r362, %r363}, [%r444+16384];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r222, %r223, %r270, %r271}, [%r444+128];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r318, %r319, %r366, %r367}, [%r444+16512];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r226, %r227, %r274, %r275}, [%r444+256];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r322, %r323, %r370, %r371}, [%r444+16640];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r230, %r231, %r278, %r279}, [%r444+384];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r326, %r327, %r374, %r375}, [%r444+16768];
	add.s32 	%r445, %r443, %r37;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r220, %r221, %r268, %r269}, [%r445];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r316, %r317, %r364, %r365}, [%r445+16384];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r224, %r225, %r272, %r273}, [%r445+128];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r320, %r321, %r368, %r369}, [%r445+16512];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r228, %r229, %r276, %r277}, [%r445+256];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r324, %r325, %r372, %r373}, [%r445+16640];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r232, %r233, %r280, %r281}, [%r445+384];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r328, %r329, %r376, %r377}, [%r445+16768];
	.loc	1 56 35                         // dump_ptx.py:56:35
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r679, %r680, %r681, %r682 }, { %r214, %r215, %r216, %r217 }, { %r218, %r219 }, { %r679, %r680, %r681, %r682 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r683, %r684, %r685, %r686 }, { %r214, %r215, %r216, %r217 }, { %r220, %r221 }, { %r683, %r684, %r685, %r686 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r687, %r688, %r689, %r690 }, { %r214, %r215, %r216, %r217 }, { %r222, %r223 }, { %r687, %r688, %r689, %r690 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r691, %r692, %r693, %r694 }, { %r214, %r215, %r216, %r217 }, { %r224, %r225 }, { %r691, %r692, %r693, %r694 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r695, %r696, %r697, %r698 }, { %r214, %r215, %r216, %r217 }, { %r226, %r227 }, { %r695, %r696, %r697, %r698 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r699, %r700, %r701, %r702 }, { %r214, %r215, %r216, %r217 }, { %r228, %r229 }, { %r699, %r700, %r701, %r702 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r703, %r704, %r705, %r706 }, { %r214, %r215, %r216, %r217 }, { %r230, %r231 }, { %r703, %r704, %r705, %r706 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r707, %r708, %r709, %r710 }, { %r214, %r215, %r216, %r217 }, { %r232, %r233 }, { %r707, %r708, %r709, %r710 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r711, %r712, %r713, %r714 }, { %r234, %r235, %r236, %r237 }, { %r218, %r219 }, { %r711, %r712, %r713, %r714 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r715, %r716, %r717, %r718 }, { %r234, %r235, %r236, %r237 }, { %r220, %r221 }, { %r715, %r716, %r717, %r718 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r719, %r720, %r721, %r722 }, { %r234, %r235, %r236, %r237 }, { %r222, %r223 }, { %r719, %r720, %r721, %r722 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r723, %r724, %r725, %r726 }, { %r234, %r235, %r236, %r237 }, { %r224, %r225 }, { %r723, %r724, %r725, %r726 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r727, %r728, %r729, %r730 }, { %r234, %r235, %r236, %r237 }, { %r226, %r227 }, { %r727, %r728, %r729, %r730 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r731, %r732, %r733, %r734 }, { %r234, %r235, %r236, %r237 }, { %r228, %r229 }, { %r731, %r732, %r733, %r734 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r735, %r736, %r737, %r738 }, { %r234, %r235, %r236, %r237 }, { %r230, %r231 }, { %r735, %r736, %r737, %r738 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r739, %r740, %r741, %r742 }, { %r234, %r235, %r236, %r237 }, { %r232, %r233 }, { %r739, %r740, %r741, %r742 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r743, %r744, %r745, %r746 }, { %r238, %r239, %r240, %r241 }, { %r218, %r219 }, { %r743, %r744, %r745, %r746 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r747, %r748, %r749, %r750 }, { %r238, %r239, %r240, %r241 }, { %r220, %r221 }, { %r747, %r748, %r749, %r750 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r751, %r752, %r753, %r754 }, { %r238, %r239, %r240, %r241 }, { %r222, %r223 }, { %r751, %r752, %r753, %r754 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r755, %r756, %r757, %r758 }, { %r238, %r239, %r240, %r241 }, { %r224, %r225 }, { %r755, %r756, %r757, %r758 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r759, %r760, %r761, %r762 }, { %r238, %r239, %r240, %r241 }, { %r226, %r227 }, { %r759, %r760, %r761, %r762 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r763, %r764, %r765, %r766 }, { %r238, %r239, %r240, %r241 }, { %r228, %r229 }, { %r763, %r764, %r765, %r766 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r767, %r768, %r769, %r770 }, { %r238, %r239, %r240, %r241 }, { %r230, %r231 }, { %r767, %r768, %r769, %r770 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r771, %r772, %r773, %r774 }, { %r238, %r239, %r240, %r241 }, { %r232, %r233 }, { %r771, %r772, %r773, %r774 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r775, %r776, %r777, %r778 }, { %r242, %r243, %r244, %r245 }, { %r218, %r219 }, { %r775, %r776, %r777, %r778 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r779, %r780, %r781, %r782 }, { %r242, %r243, %r244, %r245 }, { %r220, %r221 }, { %r779, %r780, %r781, %r782 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r783, %r784, %r785, %r786 }, { %r242, %r243, %r244, %r245 }, { %r222, %r223 }, { %r783, %r784, %r785, %r786 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r787, %r788, %r789, %r790 }, { %r242, %r243, %r244, %r245 }, { %r224, %r225 }, { %r787, %r788, %r789, %r790 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r791, %r792, %r793, %r794 }, { %r242, %r243, %r244, %r245 }, { %r226, %r227 }, { %r791, %r792, %r793, %r794 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r795, %r796, %r797, %r798 }, { %r242, %r243, %r244, %r245 }, { %r228, %r229 }, { %r795, %r796, %r797, %r798 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r799, %r800, %r801, %r802 }, { %r242, %r243, %r244, %r245 }, { %r230, %r231 }, { %r799, %r800, %r801, %r802 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r803, %r804, %r805, %r806 }, { %r242, %r243, %r244, %r245 }, { %r232, %r233 }, { %r803, %r804, %r805, %r806 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r807, %r808, %r809, %r810 }, { %r246, %r247, %r248, %r249 }, { %r218, %r219 }, { %r807, %r808, %r809, %r810 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r811, %r812, %r813, %r814 }, { %r246, %r247, %r248, %r249 }, { %r220, %r221 }, { %r811, %r812, %r813, %r814 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r815, %r816, %r817, %r818 }, { %r246, %r247, %r248, %r249 }, { %r222, %r223 }, { %r815, %r816, %r817, %r818 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r819, %r820, %r821, %r822 }, { %r246, %r247, %r248, %r249 }, { %r224, %r225 }, { %r819, %r820, %r821, %r822 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r823, %r824, %r825, %r826 }, { %r246, %r247, %r248, %r249 }, { %r226, %r227 }, { %r823, %r824, %r825, %r826 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r827, %r828, %r829, %r830 }, { %r246, %r247, %r248, %r249 }, { %r228, %r229 }, { %r827, %r828, %r829, %r830 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r831, %r832, %r833, %r834 }, { %r246, %r247, %r248, %r249 }, { %r230, %r231 }, { %r831, %r832, %r833, %r834 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r835, %r836, %r837, %r838 }, { %r246, %r247, %r248, %r249 }, { %r232, %r233 }, { %r835, %r836, %r837, %r838 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r839, %r840, %r841, %r842 }, { %r250, %r251, %r252, %r253 }, { %r218, %r219 }, { %r839, %r840, %r841, %r842 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r843, %r844, %r845, %r846 }, { %r250, %r251, %r252, %r253 }, { %r220, %r221 }, { %r843, %r844, %r845, %r846 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r847, %r848, %r849, %r850 }, { %r250, %r251, %r252, %r253 }, { %r222, %r223 }, { %r847, %r848, %r849, %r850 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r851, %r852, %r853, %r854 }, { %r250, %r251, %r252, %r253 }, { %r224, %r225 }, { %r851, %r852, %r853, %r854 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r855, %r856, %r857, %r858 }, { %r250, %r251, %r252, %r253 }, { %r226, %r227 }, { %r855, %r856, %r857, %r858 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r859, %r860, %r861, %r862 }, { %r250, %r251, %r252, %r253 }, { %r228, %r229 }, { %r859, %r860, %r861, %r862 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r863, %r864, %r865, %r866 }, { %r250, %r251, %r252, %r253 }, { %r230, %r231 }, { %r863, %r864, %r865, %r866 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r867, %r868, %r869, %r870 }, { %r250, %r251, %r252, %r253 }, { %r232, %r233 }, { %r867, %r868, %r869, %r870 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r871, %r872, %r873, %r874 }, { %r254, %r255, %r256, %r257 }, { %r218, %r219 }, { %r871, %r872, %r873, %r874 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r875, %r876, %r877, %r878 }, { %r254, %r255, %r256, %r257 }, { %r220, %r221 }, { %r875, %r876, %r877, %r878 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r879, %r880, %r881, %r882 }, { %r254, %r255, %r256, %r257 }, { %r222, %r223 }, { %r879, %r880, %r881, %r882 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r883, %r884, %r885, %r886 }, { %r254, %r255, %r256, %r257 }, { %r224, %r225 }, { %r883, %r884, %r885, %r886 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r887, %r888, %r889, %r890 }, { %r254, %r255, %r256, %r257 }, { %r226, %r227 }, { %r887, %r888, %r889, %r890 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r891, %r892, %r893, %r894 }, { %r254, %r255, %r256, %r257 }, { %r228, %r229 }, { %r891, %r892, %r893, %r894 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r895, %r896, %r897, %r898 }, { %r254, %r255, %r256, %r257 }, { %r230, %r231 }, { %r895, %r896, %r897, %r898 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r899, %r900, %r901, %r902 }, { %r254, %r255, %r256, %r257 }, { %r232, %r233 }, { %r899, %r900, %r901, %r902 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r903, %r904, %r905, %r906 }, { %r258, %r259, %r260, %r261 }, { %r218, %r219 }, { %r903, %r904, %r905, %r906 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r907, %r908, %r909, %r910 }, { %r258, %r259, %r260, %r261 }, { %r220, %r221 }, { %r907, %r908, %r909, %r910 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r911, %r912, %r913, %r914 }, { %r258, %r259, %r260, %r261 }, { %r222, %r223 }, { %r911, %r912, %r913, %r914 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r915, %r916, %r917, %r918 }, { %r258, %r259, %r260, %r261 }, { %r224, %r225 }, { %r915, %r916, %r917, %r918 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r919, %r920, %r921, %r922 }, { %r258, %r259, %r260, %r261 }, { %r226, %r227 }, { %r919, %r920, %r921, %r922 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r923, %r924, %r925, %r926 }, { %r258, %r259, %r260, %r261 }, { %r228, %r229 }, { %r923, %r924, %r925, %r926 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r927, %r928, %r929, %r930 }, { %r258, %r259, %r260, %r261 }, { %r230, %r231 }, { %r927, %r928, %r929, %r930 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r931, %r932, %r933, %r934 }, { %r258, %r259, %r260, %r261 }, { %r232, %r233 }, { %r931, %r932, %r933, %r934 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r679, %r680, %r681, %r682 }, { %r262, %r263, %r264, %r265 }, { %r266, %r267 }, { %r679, %r680, %r681, %r682 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r683, %r684, %r685, %r686 }, { %r262, %r263, %r264, %r265 }, { %r268, %r269 }, { %r683, %r684, %r685, %r686 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r687, %r688, %r689, %r690 }, { %r262, %r263, %r264, %r265 }, { %r270, %r271 }, { %r687, %r688, %r689, %r690 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r691, %r692, %r693, %r694 }, { %r262, %r263, %r264, %r265 }, { %r272, %r273 }, { %r691, %r692, %r693, %r694 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r695, %r696, %r697, %r698 }, { %r262, %r263, %r264, %r265 }, { %r274, %r275 }, { %r695, %r696, %r697, %r698 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r699, %r700, %r701, %r702 }, { %r262, %r263, %r264, %r265 }, { %r276, %r277 }, { %r699, %r700, %r701, %r702 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r703, %r704, %r705, %r706 }, { %r262, %r263, %r264, %r265 }, { %r278, %r279 }, { %r703, %r704, %r705, %r706 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r707, %r708, %r709, %r710 }, { %r262, %r263, %r264, %r265 }, { %r280, %r281 }, { %r707, %r708, %r709, %r710 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r711, %r712, %r713, %r714 }, { %r282, %r283, %r284, %r285 }, { %r266, %r267 }, { %r711, %r712, %r713, %r714 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r715, %r716, %r717, %r718 }, { %r282, %r283, %r284, %r285 }, { %r268, %r269 }, { %r715, %r716, %r717, %r718 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r719, %r720, %r721, %r722 }, { %r282, %r283, %r284, %r285 }, { %r270, %r271 }, { %r719, %r720, %r721, %r722 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r723, %r724, %r725, %r726 }, { %r282, %r283, %r284, %r285 }, { %r272, %r273 }, { %r723, %r724, %r725, %r726 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r727, %r728, %r729, %r730 }, { %r282, %r283, %r284, %r285 }, { %r274, %r275 }, { %r727, %r728, %r729, %r730 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r731, %r732, %r733, %r734 }, { %r282, %r283, %r284, %r285 }, { %r276, %r277 }, { %r731, %r732, %r733, %r734 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r735, %r736, %r737, %r738 }, { %r282, %r283, %r284, %r285 }, { %r278, %r279 }, { %r735, %r736, %r737, %r738 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r739, %r740, %r741, %r742 }, { %r282, %r283, %r284, %r285 }, { %r280, %r281 }, { %r739, %r740, %r741, %r742 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r743, %r744, %r745, %r746 }, { %r286, %r287, %r288, %r289 }, { %r266, %r267 }, { %r743, %r744, %r745, %r746 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r747, %r748, %r749, %r750 }, { %r286, %r287, %r288, %r289 }, { %r268, %r269 }, { %r747, %r748, %r749, %r750 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r751, %r752, %r753, %r754 }, { %r286, %r287, %r288, %r289 }, { %r270, %r271 }, { %r751, %r752, %r753, %r754 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r755, %r756, %r757, %r758 }, { %r286, %r287, %r288, %r289 }, { %r272, %r273 }, { %r755, %r756, %r757, %r758 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r759, %r760, %r761, %r762 }, { %r286, %r287, %r288, %r289 }, { %r274, %r275 }, { %r759, %r760, %r761, %r762 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r763, %r764, %r765, %r766 }, { %r286, %r287, %r288, %r289 }, { %r276, %r277 }, { %r763, %r764, %r765, %r766 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r767, %r768, %r769, %r770 }, { %r286, %r287, %r288, %r289 }, { %r278, %r279 }, { %r767, %r768, %r769, %r770 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r771, %r772, %r773, %r774 }, { %r286, %r287, %r288, %r289 }, { %r280, %r281 }, { %r771, %r772, %r773, %r774 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r775, %r776, %r777, %r778 }, { %r290, %r291, %r292, %r293 }, { %r266, %r267 }, { %r775, %r776, %r777, %r778 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r779, %r780, %r781, %r782 }, { %r290, %r291, %r292, %r293 }, { %r268, %r269 }, { %r779, %r780, %r781, %r782 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r783, %r784, %r785, %r786 }, { %r290, %r291, %r292, %r293 }, { %r270, %r271 }, { %r783, %r784, %r785, %r786 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r787, %r788, %r789, %r790 }, { %r290, %r291, %r292, %r293 }, { %r272, %r273 }, { %r787, %r788, %r789, %r790 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r791, %r792, %r793, %r794 }, { %r290, %r291, %r292, %r293 }, { %r274, %r275 }, { %r791, %r792, %r793, %r794 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r795, %r796, %r797, %r798 }, { %r290, %r291, %r292, %r293 }, { %r276, %r277 }, { %r795, %r796, %r797, %r798 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r799, %r800, %r801, %r802 }, { %r290, %r291, %r292, %r293 }, { %r278, %r279 }, { %r799, %r800, %r801, %r802 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r803, %r804, %r805, %r806 }, { %r290, %r291, %r292, %r293 }, { %r280, %r281 }, { %r803, %r804, %r805, %r806 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r807, %r808, %r809, %r810 }, { %r294, %r295, %r296, %r297 }, { %r266, %r267 }, { %r807, %r808, %r809, %r810 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r811, %r812, %r813, %r814 }, { %r294, %r295, %r296, %r297 }, { %r268, %r269 }, { %r811, %r812, %r813, %r814 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r815, %r816, %r817, %r818 }, { %r294, %r295, %r296, %r297 }, { %r270, %r271 }, { %r815, %r816, %r817, %r818 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r819, %r820, %r821, %r822 }, { %r294, %r295, %r296, %r297 }, { %r272, %r273 }, { %r819, %r820, %r821, %r822 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r823, %r824, %r825, %r826 }, { %r294, %r295, %r296, %r297 }, { %r274, %r275 }, { %r823, %r824, %r825, %r826 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r827, %r828, %r829, %r830 }, { %r294, %r295, %r296, %r297 }, { %r276, %r277 }, { %r827, %r828, %r829, %r830 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r831, %r832, %r833, %r834 }, { %r294, %r295, %r296, %r297 }, { %r278, %r279 }, { %r831, %r832, %r833, %r834 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r835, %r836, %r837, %r838 }, { %r294, %r295, %r296, %r297 }, { %r280, %r281 }, { %r835, %r836, %r837, %r838 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r839, %r840, %r841, %r842 }, { %r298, %r299, %r300, %r301 }, { %r266, %r267 }, { %r839, %r840, %r841, %r842 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r843, %r844, %r845, %r846 }, { %r298, %r299, %r300, %r301 }, { %r268, %r269 }, { %r843, %r844, %r845, %r846 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r847, %r848, %r849, %r850 }, { %r298, %r299, %r300, %r301 }, { %r270, %r271 }, { %r847, %r848, %r849, %r850 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r851, %r852, %r853, %r854 }, { %r298, %r299, %r300, %r301 }, { %r272, %r273 }, { %r851, %r852, %r853, %r854 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r855, %r856, %r857, %r858 }, { %r298, %r299, %r300, %r301 }, { %r274, %r275 }, { %r855, %r856, %r857, %r858 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r859, %r860, %r861, %r862 }, { %r298, %r299, %r300, %r301 }, { %r276, %r277 }, { %r859, %r860, %r861, %r862 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r863, %r864, %r865, %r866 }, { %r298, %r299, %r300, %r301 }, { %r278, %r279 }, { %r863, %r864, %r865, %r866 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r867, %r868, %r869, %r870 }, { %r298, %r299, %r300, %r301 }, { %r280, %r281 }, { %r867, %r868, %r869, %r870 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r871, %r872, %r873, %r874 }, { %r302, %r303, %r304, %r305 }, { %r266, %r267 }, { %r871, %r872, %r873, %r874 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r875, %r876, %r877, %r878 }, { %r302, %r303, %r304, %r305 }, { %r268, %r269 }, { %r875, %r876, %r877, %r878 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r879, %r880, %r881, %r882 }, { %r302, %r303, %r304, %r305 }, { %r270, %r271 }, { %r879, %r880, %r881, %r882 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r883, %r884, %r885, %r886 }, { %r302, %r303, %r304, %r305 }, { %r272, %r273 }, { %r883, %r884, %r885, %r886 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r887, %r888, %r889, %r890 }, { %r302, %r303, %r304, %r305 }, { %r274, %r275 }, { %r887, %r888, %r889, %r890 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r891, %r892, %r893, %r894 }, { %r302, %r303, %r304, %r305 }, { %r276, %r277 }, { %r891, %r892, %r893, %r894 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r895, %r896, %r897, %r898 }, { %r302, %r303, %r304, %r305 }, { %r278, %r279 }, { %r895, %r896, %r897, %r898 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r899, %r900, %r901, %r902 }, { %r302, %r303, %r304, %r305 }, { %r280, %r281 }, { %r899, %r900, %r901, %r902 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r903, %r904, %r905, %r906 }, { %r306, %r307, %r308, %r309 }, { %r266, %r267 }, { %r903, %r904, %r905, %r906 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r907, %r908, %r909, %r910 }, { %r306, %r307, %r308, %r309 }, { %r268, %r269 }, { %r907, %r908, %r909, %r910 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r911, %r912, %r913, %r914 }, { %r306, %r307, %r308, %r309 }, { %r270, %r271 }, { %r911, %r912, %r913, %r914 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r915, %r916, %r917, %r918 }, { %r306, %r307, %r308, %r309 }, { %r272, %r273 }, { %r915, %r916, %r917, %r918 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r919, %r920, %r921, %r922 }, { %r306, %r307, %r308, %r309 }, { %r274, %r275 }, { %r919, %r920, %r921, %r922 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r923, %r924, %r925, %r926 }, { %r306, %r307, %r308, %r309 }, { %r276, %r277 }, { %r923, %r924, %r925, %r926 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r927, %r928, %r929, %r930 }, { %r306, %r307, %r308, %r309 }, { %r278, %r279 }, { %r927, %r928, %r929, %r930 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r931, %r932, %r933, %r934 }, { %r306, %r307, %r308, %r309 }, { %r280, %r281 }, { %r931, %r932, %r933, %r934 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r679, %r680, %r681, %r682 }, { %r310, %r311, %r312, %r313 }, { %r314, %r315 }, { %r679, %r680, %r681, %r682 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r683, %r684, %r685, %r686 }, { %r310, %r311, %r312, %r313 }, { %r316, %r317 }, { %r683, %r684, %r685, %r686 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r687, %r688, %r689, %r690 }, { %r310, %r311, %r312, %r313 }, { %r318, %r319 }, { %r687, %r688, %r689, %r690 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r691, %r692, %r693, %r694 }, { %r310, %r311, %r312, %r313 }, { %r320, %r321 }, { %r691, %r692, %r693, %r694 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r695, %r696, %r697, %r698 }, { %r310, %r311, %r312, %r313 }, { %r322, %r323 }, { %r695, %r696, %r697, %r698 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r699, %r700, %r701, %r702 }, { %r310, %r311, %r312, %r313 }, { %r324, %r325 }, { %r699, %r700, %r701, %r702 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r703, %r704, %r705, %r706 }, { %r310, %r311, %r312, %r313 }, { %r326, %r327 }, { %r703, %r704, %r705, %r706 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r707, %r708, %r709, %r710 }, { %r310, %r311, %r312, %r313 }, { %r328, %r329 }, { %r707, %r708, %r709, %r710 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r711, %r712, %r713, %r714 }, { %r330, %r331, %r332, %r333 }, { %r314, %r315 }, { %r711, %r712, %r713, %r714 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r715, %r716, %r717, %r718 }, { %r330, %r331, %r332, %r333 }, { %r316, %r317 }, { %r715, %r716, %r717, %r718 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r719, %r720, %r721, %r722 }, { %r330, %r331, %r332, %r333 }, { %r318, %r319 }, { %r719, %r720, %r721, %r722 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r723, %r724, %r725, %r726 }, { %r330, %r331, %r332, %r333 }, { %r320, %r321 }, { %r723, %r724, %r725, %r726 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r727, %r728, %r729, %r730 }, { %r330, %r331, %r332, %r333 }, { %r322, %r323 }, { %r727, %r728, %r729, %r730 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r731, %r732, %r733, %r734 }, { %r330, %r331, %r332, %r333 }, { %r324, %r325 }, { %r731, %r732, %r733, %r734 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r735, %r736, %r737, %r738 }, { %r330, %r331, %r332, %r333 }, { %r326, %r327 }, { %r735, %r736, %r737, %r738 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r739, %r740, %r741, %r742 }, { %r330, %r331, %r332, %r333 }, { %r328, %r329 }, { %r739, %r740, %r741, %r742 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r743, %r744, %r745, %r746 }, { %r334, %r335, %r336, %r337 }, { %r314, %r315 }, { %r743, %r744, %r745, %r746 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r747, %r748, %r749, %r750 }, { %r334, %r335, %r336, %r337 }, { %r316, %r317 }, { %r747, %r748, %r749, %r750 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r751, %r752, %r753, %r754 }, { %r334, %r335, %r336, %r337 }, { %r318, %r319 }, { %r751, %r752, %r753, %r754 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r755, %r756, %r757, %r758 }, { %r334, %r335, %r336, %r337 }, { %r320, %r321 }, { %r755, %r756, %r757, %r758 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r759, %r760, %r761, %r762 }, { %r334, %r335, %r336, %r337 }, { %r322, %r323 }, { %r759, %r760, %r761, %r762 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r763, %r764, %r765, %r766 }, { %r334, %r335, %r336, %r337 }, { %r324, %r325 }, { %r763, %r764, %r765, %r766 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r767, %r768, %r769, %r770 }, { %r334, %r335, %r336, %r337 }, { %r326, %r327 }, { %r767, %r768, %r769, %r770 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r771, %r772, %r773, %r774 }, { %r334, %r335, %r336, %r337 }, { %r328, %r329 }, { %r771, %r772, %r773, %r774 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r775, %r776, %r777, %r778 }, { %r338, %r339, %r340, %r341 }, { %r314, %r315 }, { %r775, %r776, %r777, %r778 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r779, %r780, %r781, %r782 }, { %r338, %r339, %r340, %r341 }, { %r316, %r317 }, { %r779, %r780, %r781, %r782 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r783, %r784, %r785, %r786 }, { %r338, %r339, %r340, %r341 }, { %r318, %r319 }, { %r783, %r784, %r785, %r786 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r787, %r788, %r789, %r790 }, { %r338, %r339, %r340, %r341 }, { %r320, %r321 }, { %r787, %r788, %r789, %r790 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r791, %r792, %r793, %r794 }, { %r338, %r339, %r340, %r341 }, { %r322, %r323 }, { %r791, %r792, %r793, %r794 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r795, %r796, %r797, %r798 }, { %r338, %r339, %r340, %r341 }, { %r324, %r325 }, { %r795, %r796, %r797, %r798 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r799, %r800, %r801, %r802 }, { %r338, %r339, %r340, %r341 }, { %r326, %r327 }, { %r799, %r800, %r801, %r802 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r803, %r804, %r805, %r806 }, { %r338, %r339, %r340, %r341 }, { %r328, %r329 }, { %r803, %r804, %r805, %r806 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r807, %r808, %r809, %r810 }, { %r342, %r343, %r344, %r345 }, { %r314, %r315 }, { %r807, %r808, %r809, %r810 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r811, %r812, %r813, %r814 }, { %r342, %r343, %r344, %r345 }, { %r316, %r317 }, { %r811, %r812, %r813, %r814 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r815, %r816, %r817, %r818 }, { %r342, %r343, %r344, %r345 }, { %r318, %r319 }, { %r815, %r816, %r817, %r818 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r819, %r820, %r821, %r822 }, { %r342, %r343, %r344, %r345 }, { %r320, %r321 }, { %r819, %r820, %r821, %r822 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r823, %r824, %r825, %r826 }, { %r342, %r343, %r344, %r345 }, { %r322, %r323 }, { %r823, %r824, %r825, %r826 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r827, %r828, %r829, %r830 }, { %r342, %r343, %r344, %r345 }, { %r324, %r325 }, { %r827, %r828, %r829, %r830 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r831, %r832, %r833, %r834 }, { %r342, %r343, %r344, %r345 }, { %r326, %r327 }, { %r831, %r832, %r833, %r834 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r835, %r836, %r837, %r838 }, { %r342, %r343, %r344, %r345 }, { %r328, %r329 }, { %r835, %r836, %r837, %r838 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r839, %r840, %r841, %r842 }, { %r346, %r347, %r348, %r349 }, { %r314, %r315 }, { %r839, %r840, %r841, %r842 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r843, %r844, %r845, %r846 }, { %r346, %r347, %r348, %r349 }, { %r316, %r317 }, { %r843, %r844, %r845, %r846 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r847, %r848, %r849, %r850 }, { %r346, %r347, %r348, %r349 }, { %r318, %r319 }, { %r847, %r848, %r849, %r850 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r851, %r852, %r853, %r854 }, { %r346, %r347, %r348, %r349 }, { %r320, %r321 }, { %r851, %r852, %r853, %r854 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r855, %r856, %r857, %r858 }, { %r346, %r347, %r348, %r349 }, { %r322, %r323 }, { %r855, %r856, %r857, %r858 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r859, %r860, %r861, %r862 }, { %r346, %r347, %r348, %r349 }, { %r324, %r325 }, { %r859, %r860, %r861, %r862 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r863, %r864, %r865, %r866 }, { %r346, %r347, %r348, %r349 }, { %r326, %r327 }, { %r863, %r864, %r865, %r866 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r867, %r868, %r869, %r870 }, { %r346, %r347, %r348, %r349 }, { %r328, %r329 }, { %r867, %r868, %r869, %r870 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r871, %r872, %r873, %r874 }, { %r350, %r351, %r352, %r353 }, { %r314, %r315 }, { %r871, %r872, %r873, %r874 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r875, %r876, %r877, %r878 }, { %r350, %r351, %r352, %r353 }, { %r316, %r317 }, { %r875, %r876, %r877, %r878 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r879, %r880, %r881, %r882 }, { %r350, %r351, %r352, %r353 }, { %r318, %r319 }, { %r879, %r880, %r881, %r882 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r883, %r884, %r885, %r886 }, { %r350, %r351, %r352, %r353 }, { %r320, %r321 }, { %r883, %r884, %r885, %r886 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r887, %r888, %r889, %r890 }, { %r350, %r351, %r352, %r353 }, { %r322, %r323 }, { %r887, %r888, %r889, %r890 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r891, %r892, %r893, %r894 }, { %r350, %r351, %r352, %r353 }, { %r324, %r325 }, { %r891, %r892, %r893, %r894 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r895, %r896, %r897, %r898 }, { %r350, %r351, %r352, %r353 }, { %r326, %r327 }, { %r895, %r896, %r897, %r898 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r899, %r900, %r901, %r902 }, { %r350, %r351, %r352, %r353 }, { %r328, %r329 }, { %r899, %r900, %r901, %r902 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r903, %r904, %r905, %r906 }, { %r354, %r355, %r356, %r357 }, { %r314, %r315 }, { %r903, %r904, %r905, %r906 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r907, %r908, %r909, %r910 }, { %r354, %r355, %r356, %r357 }, { %r316, %r317 }, { %r907, %r908, %r909, %r910 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r911, %r912, %r913, %r914 }, { %r354, %r355, %r356, %r357 }, { %r318, %r319 }, { %r911, %r912, %r913, %r914 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r915, %r916, %r917, %r918 }, { %r354, %r355, %r356, %r357 }, { %r320, %r321 }, { %r915, %r916, %r917, %r918 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r919, %r920, %r921, %r922 }, { %r354, %r355, %r356, %r357 }, { %r322, %r323 }, { %r919, %r920, %r921, %r922 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r923, %r924, %r925, %r926 }, { %r354, %r355, %r356, %r357 }, { %r324, %r325 }, { %r923, %r924, %r925, %r926 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r927, %r928, %r929, %r930 }, { %r354, %r355, %r356, %r357 }, { %r326, %r327 }, { %r927, %r928, %r929, %r930 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r931, %r932, %r933, %r934 }, { %r354, %r355, %r356, %r357 }, { %r328, %r329 }, { %r931, %r932, %r933, %r934 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r679, %r680, %r681, %r682 }, { %r358, %r359, %r360, %r361 }, { %r362, %r363 }, { %r679, %r680, %r681, %r682 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r683, %r684, %r685, %r686 }, { %r358, %r359, %r360, %r361 }, { %r364, %r365 }, { %r683, %r684, %r685, %r686 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r687, %r688, %r689, %r690 }, { %r358, %r359, %r360, %r361 }, { %r366, %r367 }, { %r687, %r688, %r689, %r690 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r691, %r692, %r693, %r694 }, { %r358, %r359, %r360, %r361 }, { %r368, %r369 }, { %r691, %r692, %r693, %r694 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r695, %r696, %r697, %r698 }, { %r358, %r359, %r360, %r361 }, { %r370, %r371 }, { %r695, %r696, %r697, %r698 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r699, %r700, %r701, %r702 }, { %r358, %r359, %r360, %r361 }, { %r372, %r373 }, { %r699, %r700, %r701, %r702 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r703, %r704, %r705, %r706 }, { %r358, %r359, %r360, %r361 }, { %r374, %r375 }, { %r703, %r704, %r705, %r706 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r707, %r708, %r709, %r710 }, { %r358, %r359, %r360, %r361 }, { %r376, %r377 }, { %r707, %r708, %r709, %r710 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r711, %r712, %r713, %r714 }, { %r378, %r379, %r380, %r381 }, { %r362, %r363 }, { %r711, %r712, %r713, %r714 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r715, %r716, %r717, %r718 }, { %r378, %r379, %r380, %r381 }, { %r364, %r365 }, { %r715, %r716, %r717, %r718 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r719, %r720, %r721, %r722 }, { %r378, %r379, %r380, %r381 }, { %r366, %r367 }, { %r719, %r720, %r721, %r722 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r723, %r724, %r725, %r726 }, { %r378, %r379, %r380, %r381 }, { %r368, %r369 }, { %r723, %r724, %r725, %r726 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r727, %r728, %r729, %r730 }, { %r378, %r379, %r380, %r381 }, { %r370, %r371 }, { %r727, %r728, %r729, %r730 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r731, %r732, %r733, %r734 }, { %r378, %r379, %r380, %r381 }, { %r372, %r373 }, { %r731, %r732, %r733, %r734 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r735, %r736, %r737, %r738 }, { %r378, %r379, %r380, %r381 }, { %r374, %r375 }, { %r735, %r736, %r737, %r738 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r739, %r740, %r741, %r742 }, { %r378, %r379, %r380, %r381 }, { %r376, %r377 }, { %r739, %r740, %r741, %r742 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r743, %r744, %r745, %r746 }, { %r382, %r383, %r384, %r385 }, { %r362, %r363 }, { %r743, %r744, %r745, %r746 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r747, %r748, %r749, %r750 }, { %r382, %r383, %r384, %r385 }, { %r364, %r365 }, { %r747, %r748, %r749, %r750 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r751, %r752, %r753, %r754 }, { %r382, %r383, %r384, %r385 }, { %r366, %r367 }, { %r751, %r752, %r753, %r754 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r755, %r756, %r757, %r758 }, { %r382, %r383, %r384, %r385 }, { %r368, %r369 }, { %r755, %r756, %r757, %r758 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r759, %r760, %r761, %r762 }, { %r382, %r383, %r384, %r385 }, { %r370, %r371 }, { %r759, %r760, %r761, %r762 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r763, %r764, %r765, %r766 }, { %r382, %r383, %r384, %r385 }, { %r372, %r373 }, { %r763, %r764, %r765, %r766 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r767, %r768, %r769, %r770 }, { %r382, %r383, %r384, %r385 }, { %r374, %r375 }, { %r767, %r768, %r769, %r770 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r771, %r772, %r773, %r774 }, { %r382, %r383, %r384, %r385 }, { %r376, %r377 }, { %r771, %r772, %r773, %r774 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r775, %r776, %r777, %r778 }, { %r386, %r387, %r388, %r389 }, { %r362, %r363 }, { %r775, %r776, %r777, %r778 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r779, %r780, %r781, %r782 }, { %r386, %r387, %r388, %r389 }, { %r364, %r365 }, { %r779, %r780, %r781, %r782 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r783, %r784, %r785, %r786 }, { %r386, %r387, %r388, %r389 }, { %r366, %r367 }, { %r783, %r784, %r785, %r786 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r787, %r788, %r789, %r790 }, { %r386, %r387, %r388, %r389 }, { %r368, %r369 }, { %r787, %r788, %r789, %r790 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r791, %r792, %r793, %r794 }, { %r386, %r387, %r388, %r389 }, { %r370, %r371 }, { %r791, %r792, %r793, %r794 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r795, %r796, %r797, %r798 }, { %r386, %r387, %r388, %r389 }, { %r372, %r373 }, { %r795, %r796, %r797, %r798 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r799, %r800, %r801, %r802 }, { %r386, %r387, %r388, %r389 }, { %r374, %r375 }, { %r799, %r800, %r801, %r802 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r803, %r804, %r805, %r806 }, { %r386, %r387, %r388, %r389 }, { %r376, %r377 }, { %r803, %r804, %r805, %r806 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r807, %r808, %r809, %r810 }, { %r390, %r391, %r392, %r393 }, { %r362, %r363 }, { %r807, %r808, %r809, %r810 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r811, %r812, %r813, %r814 }, { %r390, %r391, %r392, %r393 }, { %r364, %r365 }, { %r811, %r812, %r813, %r814 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r815, %r816, %r817, %r818 }, { %r390, %r391, %r392, %r393 }, { %r366, %r367 }, { %r815, %r816, %r817, %r818 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r819, %r820, %r821, %r822 }, { %r390, %r391, %r392, %r393 }, { %r368, %r369 }, { %r819, %r820, %r821, %r822 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r823, %r824, %r825, %r826 }, { %r390, %r391, %r392, %r393 }, { %r370, %r371 }, { %r823, %r824, %r825, %r826 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r827, %r828, %r829, %r830 }, { %r390, %r391, %r392, %r393 }, { %r372, %r373 }, { %r827, %r828, %r829, %r830 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r831, %r832, %r833, %r834 }, { %r390, %r391, %r392, %r393 }, { %r374, %r375 }, { %r831, %r832, %r833, %r834 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r835, %r836, %r837, %r838 }, { %r390, %r391, %r392, %r393 }, { %r376, %r377 }, { %r835, %r836, %r837, %r838 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r839, %r840, %r841, %r842 }, { %r394, %r395, %r396, %r397 }, { %r362, %r363 }, { %r839, %r840, %r841, %r842 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r843, %r844, %r845, %r846 }, { %r394, %r395, %r396, %r397 }, { %r364, %r365 }, { %r843, %r844, %r845, %r846 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r847, %r848, %r849, %r850 }, { %r394, %r395, %r396, %r397 }, { %r366, %r367 }, { %r847, %r848, %r849, %r850 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r851, %r852, %r853, %r854 }, { %r394, %r395, %r396, %r397 }, { %r368, %r369 }, { %r851, %r852, %r853, %r854 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r855, %r856, %r857, %r858 }, { %r394, %r395, %r396, %r397 }, { %r370, %r371 }, { %r855, %r856, %r857, %r858 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r859, %r860, %r861, %r862 }, { %r394, %r395, %r396, %r397 }, { %r372, %r373 }, { %r859, %r860, %r861, %r862 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r863, %r864, %r865, %r866 }, { %r394, %r395, %r396, %r397 }, { %r374, %r375 }, { %r863, %r864, %r865, %r866 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r867, %r868, %r869, %r870 }, { %r394, %r395, %r396, %r397 }, { %r376, %r377 }, { %r867, %r868, %r869, %r870 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r871, %r872, %r873, %r874 }, { %r398, %r399, %r400, %r401 }, { %r362, %r363 }, { %r871, %r872, %r873, %r874 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r875, %r876, %r877, %r878 }, { %r398, %r399, %r400, %r401 }, { %r364, %r365 }, { %r875, %r876, %r877, %r878 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r879, %r880, %r881, %r882 }, { %r398, %r399, %r400, %r401 }, { %r366, %r367 }, { %r879, %r880, %r881, %r882 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r883, %r884, %r885, %r886 }, { %r398, %r399, %r400, %r401 }, { %r368, %r369 }, { %r883, %r884, %r885, %r886 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r887, %r888, %r889, %r890 }, { %r398, %r399, %r400, %r401 }, { %r370, %r371 }, { %r887, %r888, %r889, %r890 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r891, %r892, %r893, %r894 }, { %r398, %r399, %r400, %r401 }, { %r372, %r373 }, { %r891, %r892, %r893, %r894 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r895, %r896, %r897, %r898 }, { %r398, %r399, %r400, %r401 }, { %r374, %r375 }, { %r895, %r896, %r897, %r898 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r899, %r900, %r901, %r902 }, { %r398, %r399, %r400, %r401 }, { %r376, %r377 }, { %r899, %r900, %r901, %r902 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r903, %r904, %r905, %r906 }, { %r402, %r403, %r404, %r405 }, { %r362, %r363 }, { %r903, %r904, %r905, %r906 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r907, %r908, %r909, %r910 }, { %r402, %r403, %r404, %r405 }, { %r364, %r365 }, { %r907, %r908, %r909, %r910 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r911, %r912, %r913, %r914 }, { %r402, %r403, %r404, %r405 }, { %r366, %r367 }, { %r911, %r912, %r913, %r914 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r915, %r916, %r917, %r918 }, { %r402, %r403, %r404, %r405 }, { %r368, %r369 }, { %r915, %r916, %r917, %r918 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r919, %r920, %r921, %r922 }, { %r402, %r403, %r404, %r405 }, { %r370, %r371 }, { %r919, %r920, %r921, %r922 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r923, %r924, %r925, %r926 }, { %r402, %r403, %r404, %r405 }, { %r372, %r373 }, { %r923, %r924, %r925, %r926 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r927, %r928, %r929, %r930 }, { %r402, %r403, %r404, %r405 }, { %r374, %r375 }, { %r927, %r928, %r929, %r930 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %r931, %r932, %r933, %r934 }, { %r402, %r403, %r404, %r405 }, { %r376, %r377 }, { %r931, %r932, %r933, %r934 };
	// end inline asm
	.loc	1 57 18                         // dump_ptx.py:57:18
	add.s64 	%rd107, %rd196, %rd46;
	add.s64 	%rd108, %rd196, %rd45;
	add.s64 	%rd109, %rd196, %rd44;
	add.s64 	%rd110, %rd196, %rd43;
	add.s64 	%rd111, %rd196, %rd42;
	add.s64 	%rd112, %rd196, %rd41;
	add.s64 	%rd113, %rd196, %rd40;
	.loc	1 58 18                         // dump_ptx.py:58:18
	add.s64 	%rd114, %rd196, %rd39;
	add.s64 	%rd115, %rd197, %rd38;
	add.s64 	%rd123, %rd197, %rd37;
	add.s64 	%rd116, %rd197, %rd36;
	add.s64 	%rd124, %rd197, %rd35;
	add.s64 	%rd117, %rd197, %rd34;
	add.s64 	%rd125, %rd197, %rd33;
	add.s64 	%rd118, %rd197, %rd32;
	add.s64 	%rd126, %rd197, %rd31;
	add.s64 	%rd119, %rd197, %rd30;
	add.s64 	%rd127, %rd197, %rd29;
	add.s64 	%rd120, %rd197, %rd28;
	add.s64 	%rd128, %rd197, %rd27;
	add.s64 	%rd121, %rd197, %rd26;
	add.s64 	%rd129, %rd197, %rd25;
	add.s64 	%rd122, %rd197, %rd24;
	.loc	1 53 22                         // dump_ptx.py:53:22
	add.s64 	%rd130, %rd197, %rd22;
	add.s32 	%r446, %r678, 1;
	setp.gt.s32 	%p15, %r446, 1;
	selp.b32 	%r678, 0, %r446, %p15;
	.loc	1 54 51                         // dump_ptx.py:54:51
	setp.lt.s32 	%p16, %r24, %r676;
	.loc	1 54 20                         // dump_ptx.py:54:20
	shl.b32 	%r447, %r678, 14;
	add.s32 	%r448, %r194, %r447;
	bar.sync 	0;
	add.s32 	%r449, %r448, %r27;
	add.s32 	%r406, %r449, 65536;
	selp.b32 	%r450, 16, 0, %p16;
	selp.b32 	%r408, %r450, 0, %p13;
	// begin inline asm
	cp.async.cg.shared.global [ %r406 + 0 ], [ %rd107 + 0 ], 0x10, %r408;
	// end inline asm
	add.s32 	%r407, %r449, 67584;
	// begin inline asm
	cp.async.cg.shared.global [ %r407 + 0 ], [ %rd108 + 0 ], 0x10, %r408;
	// end inline asm
	add.s32 	%r409, %r449, 69632;
	// begin inline asm
	cp.async.cg.shared.global [ %r409 + 0 ], [ %rd109 + 0 ], 0x10, %r408;
	// end inline asm
	add.s32 	%r410, %r449, 71680;
	// begin inline asm
	cp.async.cg.shared.global [ %r410 + 0 ], [ %rd110 + 0 ], 0x10, %r408;
	// end inline asm
	add.s32 	%r411, %r449, 73728;
	// begin inline asm
	cp.async.cg.shared.global [ %r411 + 0 ], [ %rd111 + 0 ], 0x10, %r408;
	// end inline asm
	add.s32 	%r412, %r449, 75776;
	// begin inline asm
	cp.async.cg.shared.global [ %r412 + 0 ], [ %rd112 + 0 ], 0x10, %r408;
	// end inline asm
	add.s32 	%r413, %r449, 77824;
	// begin inline asm
	cp.async.cg.shared.global [ %r413 + 0 ], [ %rd113 + 0 ], 0x10, %r408;
	// end inline asm
	add.s32 	%r414, %r449, 79872;
	// begin inline asm
	cp.async.cg.shared.global [ %r414 + 0 ], [ %rd114 + 0 ], 0x10, %r408;
	// end inline asm
	cp.async.commit_group;
	.loc	1 55 51                         // dump_ptx.py:55:51
	setp.lt.s32 	%p17, %r4, %r676;
	setp.lt.s32 	%p18, %r8, %r676;
	setp.lt.s32 	%p19, %r12, %r676;
	setp.lt.s32 	%p20, %r16, %r676;
	.loc	1 55 20                         // dump_ptx.py:55:20
	shl.b32 	%r451, %r678, 15;
	add.s32 	%r452, %r194, %r451;
	add.s32 	%r415, %r452, %r29;
	selp.b32 	%r453, 16, 0, %p17;
	selp.b32 	%r417, %r453, 0, %p13;
	// begin inline asm
	cp.async.cg.shared.global [ %r415 + 0 ], [ %rd115 + 0 ], 0x10, %r417;
	// end inline asm
	add.s32 	%r416, %r415, 4096;
	// begin inline asm
	cp.async.cg.shared.global [ %r416 + 0 ], [ %rd116 + 0 ], 0x10, %r417;
	// end inline asm
	add.s32 	%r418, %r415, 8192;
	selp.b32 	%r454, 16, 0, %p18;
	selp.b32 	%r420, %r454, 0, %p13;
	// begin inline asm
	cp.async.cg.shared.global [ %r418 + 0 ], [ %rd117 + 0 ], 0x10, %r420;
	// end inline asm
	add.s32 	%r419, %r415, 12288;
	// begin inline asm
	cp.async.cg.shared.global [ %r419 + 0 ], [ %rd118 + 0 ], 0x10, %r420;
	// end inline asm
	add.s32 	%r421, %r415, 16384;
	selp.b32 	%r455, 16, 0, %p19;
	selp.b32 	%r423, %r455, 0, %p13;
	// begin inline asm
	cp.async.cg.shared.global [ %r421 + 0 ], [ %rd119 + 0 ], 0x10, %r423;
	// end inline asm
	add.s32 	%r422, %r415, 20480;
	// begin inline asm
	cp.async.cg.shared.global [ %r422 + 0 ], [ %rd120 + 0 ], 0x10, %r423;
	// end inline asm
	add.s32 	%r424, %r415, 24576;
	selp.b32 	%r456, 16, 0, %p20;
	selp.b32 	%r426, %r456, 0, %p13;
	// begin inline asm
	cp.async.cg.shared.global [ %r424 + 0 ], [ %rd121 + 0 ], 0x10, %r426;
	// end inline asm
	add.s32 	%r425, %r415, 28672;
	// begin inline asm
	cp.async.cg.shared.global [ %r425 + 0 ], [ %rd122 + 0 ], 0x10, %r426;
	// end inline asm
	add.s32 	%r457, %r452, %r30;
	add.s32 	%r427, %r457, 2048;
	// begin inline asm
	cp.async.cg.shared.global [ %r427 + 0 ], [ %rd123 + 0 ], 0x10, %r417;
	// end inline asm
	add.s32 	%r428, %r457, 6144;
	// begin inline asm
	cp.async.cg.shared.global [ %r428 + 0 ], [ %rd124 + 0 ], 0x10, %r417;
	// end inline asm
	add.s32 	%r429, %r457, 10240;
	// begin inline asm
	cp.async.cg.shared.global [ %r429 + 0 ], [ %rd125 + 0 ], 0x10, %r420;
	// end inline asm
	add.s32 	%r430, %r457, 14336;
	// begin inline asm
	cp.async.cg.shared.global [ %r430 + 0 ], [ %rd126 + 0 ], 0x10, %r420;
	// end inline asm
	add.s32 	%r431, %r457, 18432;
	// begin inline asm
	cp.async.cg.shared.global [ %r431 + 0 ], [ %rd127 + 0 ], 0x10, %r423;
	// end inline asm
	add.s32 	%r432, %r457, 22528;
	// begin inline asm
	cp.async.cg.shared.global [ %r432 + 0 ], [ %rd128 + 0 ], 0x10, %r423;
	// end inline asm
	add.s32 	%r433, %r457, 26624;
	// begin inline asm
	cp.async.cg.shared.global [ %r433 + 0 ], [ %rd129 + 0 ], 0x10, %r426;
	// end inline asm
	add.s32 	%r434, %r457, 30720;
	// begin inline asm
	cp.async.cg.shared.global [ %r434 + 0 ], [ %rd130 + 0 ], 0x10, %r426;
	// end inline asm
	cp.async.commit_group;
	.loc	1 53 22                         // dump_ptx.py:53:22
	add.s32 	%r935, %r935, 1;
	add.s64 	%rd197, %rd197, %rd23;
	add.s64 	%rd196, %rd196, 128;
	add.s32 	%r676, %r676, -64;
	setp.ne.b32 	%p21, %r25, %r935;
	@%p21 bra 	$L__BB0_3;
// %bb.4:                               // %._crit_edge.loopexit
	.loc	1 60 23                         // dump_ptx.py:60:23
	cvt.rn.f16x2.f32 	%r1064, %r934, %r930;
	cvt.rn.f16x2.f32 	%r1063, %r926, %r922;
	cvt.rn.f16x2.f32 	%r1062, %r918, %r914;
	cvt.rn.f16x2.f32 	%r1061, %r910, %r906;
	cvt.rn.f16x2.f32 	%r1060, %r933, %r929;
	cvt.rn.f16x2.f32 	%r1059, %r925, %r921;
	cvt.rn.f16x2.f32 	%r1058, %r917, %r913;
	cvt.rn.f16x2.f32 	%r1057, %r909, %r905;
	cvt.rn.f16x2.f32 	%r1056, %r932, %r928;
	cvt.rn.f16x2.f32 	%r1055, %r924, %r920;
	cvt.rn.f16x2.f32 	%r1054, %r916, %r912;
	cvt.rn.f16x2.f32 	%r1053, %r908, %r904;
	cvt.rn.f16x2.f32 	%r1052, %r931, %r927;
	cvt.rn.f16x2.f32 	%r1051, %r923, %r919;
	cvt.rn.f16x2.f32 	%r1050, %r915, %r911;
	cvt.rn.f16x2.f32 	%r1049, %r907, %r903;
	cvt.rn.f16x2.f32 	%r1048, %r902, %r898;
	cvt.rn.f16x2.f32 	%r1047, %r894, %r890;
	cvt.rn.f16x2.f32 	%r1046, %r886, %r882;
	cvt.rn.f16x2.f32 	%r1045, %r878, %r874;
	cvt.rn.f16x2.f32 	%r1044, %r901, %r897;
	cvt.rn.f16x2.f32 	%r1043, %r893, %r889;
	cvt.rn.f16x2.f32 	%r1042, %r885, %r881;
	cvt.rn.f16x2.f32 	%r1041, %r877, %r873;
	cvt.rn.f16x2.f32 	%r1040, %r900, %r896;
	cvt.rn.f16x2.f32 	%r1039, %r892, %r888;
	cvt.rn.f16x2.f32 	%r1038, %r884, %r880;
	cvt.rn.f16x2.f32 	%r1037, %r876, %r872;
	cvt.rn.f16x2.f32 	%r1036, %r899, %r895;
	cvt.rn.f16x2.f32 	%r1035, %r891, %r887;
	cvt.rn.f16x2.f32 	%r1034, %r883, %r879;
	cvt.rn.f16x2.f32 	%r1033, %r875, %r871;
	cvt.rn.f16x2.f32 	%r1032, %r870, %r866;
	cvt.rn.f16x2.f32 	%r1031, %r862, %r858;
	cvt.rn.f16x2.f32 	%r1030, %r854, %r850;
	cvt.rn.f16x2.f32 	%r1029, %r846, %r842;
	cvt.rn.f16x2.f32 	%r1028, %r869, %r865;
	cvt.rn.f16x2.f32 	%r1027, %r861, %r857;
	cvt.rn.f16x2.f32 	%r1026, %r853, %r849;
	cvt.rn.f16x2.f32 	%r1025, %r845, %r841;
	cvt.rn.f16x2.f32 	%r1024, %r868, %r864;
	cvt.rn.f16x2.f32 	%r1023, %r860, %r856;
	cvt.rn.f16x2.f32 	%r1022, %r852, %r848;
	cvt.rn.f16x2.f32 	%r1021, %r844, %r840;
	cvt.rn.f16x2.f32 	%r1020, %r867, %r863;
	cvt.rn.f16x2.f32 	%r1019, %r859, %r855;
	cvt.rn.f16x2.f32 	%r1018, %r851, %r847;
	cvt.rn.f16x2.f32 	%r1017, %r843, %r839;
	cvt.rn.f16x2.f32 	%r1016, %r838, %r834;
	cvt.rn.f16x2.f32 	%r1015, %r830, %r826;
	cvt.rn.f16x2.f32 	%r1014, %r822, %r818;
	cvt.rn.f16x2.f32 	%r1013, %r814, %r810;
	cvt.rn.f16x2.f32 	%r1012, %r837, %r833;
	cvt.rn.f16x2.f32 	%r1011, %r829, %r825;
	cvt.rn.f16x2.f32 	%r1010, %r821, %r817;
	cvt.rn.f16x2.f32 	%r1009, %r813, %r809;
	cvt.rn.f16x2.f32 	%r1008, %r836, %r832;
	cvt.rn.f16x2.f32 	%r1007, %r828, %r824;
	cvt.rn.f16x2.f32 	%r1006, %r820, %r816;
	cvt.rn.f16x2.f32 	%r1005, %r812, %r808;
	cvt.rn.f16x2.f32 	%r1004, %r835, %r831;
	cvt.rn.f16x2.f32 	%r1003, %r827, %r823;
	cvt.rn.f16x2.f32 	%r1002, %r819, %r815;
	cvt.rn.f16x2.f32 	%r1001, %r811, %r807;
	cvt.rn.f16x2.f32 	%r1000, %r806, %r802;
	cvt.rn.f16x2.f32 	%r999, %r798, %r794;
	cvt.rn.f16x2.f32 	%r998, %r790, %r786;
	cvt.rn.f16x2.f32 	%r997, %r782, %r778;
	cvt.rn.f16x2.f32 	%r996, %r805, %r801;
	cvt.rn.f16x2.f32 	%r995, %r797, %r793;
	cvt.rn.f16x2.f32 	%r994, %r789, %r785;
	cvt.rn.f16x2.f32 	%r993, %r781, %r777;
	cvt.rn.f16x2.f32 	%r992, %r804, %r800;
	cvt.rn.f16x2.f32 	%r991, %r796, %r792;
	cvt.rn.f16x2.f32 	%r990, %r788, %r784;
	cvt.rn.f16x2.f32 	%r989, %r780, %r776;
	cvt.rn.f16x2.f32 	%r988, %r803, %r799;
	cvt.rn.f16x2.f32 	%r987, %r795, %r791;
	cvt.rn.f16x2.f32 	%r986, %r787, %r783;
	cvt.rn.f16x2.f32 	%r985, %r779, %r775;
	cvt.rn.f16x2.f32 	%r984, %r774, %r770;
	cvt.rn.f16x2.f32 	%r983, %r766, %r762;
	cvt.rn.f16x2.f32 	%r982, %r758, %r754;
	cvt.rn.f16x2.f32 	%r981, %r750, %r746;
	cvt.rn.f16x2.f32 	%r980, %r773, %r769;
	cvt.rn.f16x2.f32 	%r979, %r765, %r761;
	cvt.rn.f16x2.f32 	%r978, %r757, %r753;
	cvt.rn.f16x2.f32 	%r977, %r749, %r745;
	cvt.rn.f16x2.f32 	%r976, %r772, %r768;
	cvt.rn.f16x2.f32 	%r975, %r764, %r760;
	cvt.rn.f16x2.f32 	%r974, %r756, %r752;
	cvt.rn.f16x2.f32 	%r973, %r748, %r744;
	cvt.rn.f16x2.f32 	%r972, %r771, %r767;
	cvt.rn.f16x2.f32 	%r971, %r763, %r759;
	cvt.rn.f16x2.f32 	%r970, %r755, %r751;
	cvt.rn.f16x2.f32 	%r969, %r747, %r743;
	cvt.rn.f16x2.f32 	%r968, %r742, %r738;
	cvt.rn.f16x2.f32 	%r967, %r734, %r730;
	cvt.rn.f16x2.f32 	%r966, %r726, %r722;
	cvt.rn.f16x2.f32 	%r965, %r718, %r714;
	cvt.rn.f16x2.f32 	%r964, %r741, %r737;
	cvt.rn.f16x2.f32 	%r963, %r733, %r729;
	cvt.rn.f16x2.f32 	%r962, %r725, %r721;
	cvt.rn.f16x2.f32 	%r961, %r717, %r713;
	cvt.rn.f16x2.f32 	%r960, %r740, %r736;
	cvt.rn.f16x2.f32 	%r959, %r732, %r728;
	cvt.rn.f16x2.f32 	%r958, %r724, %r720;
	cvt.rn.f16x2.f32 	%r957, %r716, %r712;
	cvt.rn.f16x2.f32 	%r956, %r739, %r735;
	cvt.rn.f16x2.f32 	%r955, %r731, %r727;
	cvt.rn.f16x2.f32 	%r954, %r723, %r719;
	cvt.rn.f16x2.f32 	%r953, %r715, %r711;
	cvt.rn.f16x2.f32 	%r952, %r710, %r706;
	cvt.rn.f16x2.f32 	%r951, %r702, %r698;
	cvt.rn.f16x2.f32 	%r950, %r694, %r690;
	cvt.rn.f16x2.f32 	%r949, %r686, %r682;
	cvt.rn.f16x2.f32 	%r948, %r709, %r705;
	cvt.rn.f16x2.f32 	%r947, %r701, %r697;
	cvt.rn.f16x2.f32 	%r946, %r693, %r689;
	cvt.rn.f16x2.f32 	%r945, %r685, %r681;
	cvt.rn.f16x2.f32 	%r944, %r708, %r704;
	cvt.rn.f16x2.f32 	%r943, %r700, %r696;
	cvt.rn.f16x2.f32 	%r942, %r692, %r688;
	cvt.rn.f16x2.f32 	%r941, %r684, %r680;
	cvt.rn.f16x2.f32 	%r940, %r707, %r703;
	cvt.rn.f16x2.f32 	%r939, %r699, %r695;
	cvt.rn.f16x2.f32 	%r938, %r691, %r687;
	cvt.rn.f16x2.f32 	%r937, %r683, %r679;
	bra.uni 	$L__BB0_5;
$L__BB0_1:                              // %.._crit_edge_crit_edge
	.loc	1 66 21                         // dump_ptx.py:66:21
	shl.b32 	%r936, %r23, 4;
	mov.b32 	%r937, 0;
	mov.b32 	%r938, %r937;
	mov.b32 	%r939, %r937;
	mov.b32 	%r940, %r937;
	mov.b32 	%r941, %r937;
	mov.b32 	%r942, %r937;
	mov.b32 	%r943, %r937;
	mov.b32 	%r944, %r937;
	mov.b32 	%r945, %r937;
	mov.b32 	%r946, %r937;
	mov.b32 	%r947, %r937;
	mov.b32 	%r948, %r937;
	mov.b32 	%r949, %r937;
	mov.b32 	%r950, %r937;
	mov.b32 	%r951, %r937;
	mov.b32 	%r952, %r937;
	mov.b32 	%r953, %r937;
	mov.b32 	%r954, %r937;
	mov.b32 	%r955, %r937;
	mov.b32 	%r956, %r937;
	mov.b32 	%r957, %r937;
	mov.b32 	%r958, %r937;
	mov.b32 	%r959, %r937;
	mov.b32 	%r960, %r937;
	mov.b32 	%r961, %r937;
	mov.b32 	%r962, %r937;
	mov.b32 	%r963, %r937;
	mov.b32 	%r964, %r937;
	mov.b32 	%r965, %r937;
	mov.b32 	%r966, %r937;
	mov.b32 	%r967, %r937;
	mov.b32 	%r968, %r937;
	mov.b32 	%r969, %r937;
	mov.b32 	%r970, %r937;
	mov.b32 	%r971, %r937;
	mov.b32 	%r972, %r937;
	mov.b32 	%r973, %r937;
	mov.b32 	%r974, %r937;
	mov.b32 	%r975, %r937;
	mov.b32 	%r976, %r937;
	mov.b32 	%r977, %r937;
	mov.b32 	%r978, %r937;
	mov.b32 	%r979, %r937;
	mov.b32 	%r980, %r937;
	mov.b32 	%r981, %r937;
	mov.b32 	%r982, %r937;
	mov.b32 	%r983, %r937;
	mov.b32 	%r984, %r937;
	mov.b32 	%r985, %r937;
	mov.b32 	%r986, %r937;
	mov.b32 	%r987, %r937;
	mov.b32 	%r988, %r937;
	mov.b32 	%r989, %r937;
	mov.b32 	%r990, %r937;
	mov.b32 	%r991, %r937;
	mov.b32 	%r992, %r937;
	mov.b32 	%r993, %r937;
	mov.b32 	%r994, %r937;
	mov.b32 	%r995, %r937;
	mov.b32 	%r996, %r937;
	mov.b32 	%r997, %r937;
	mov.b32 	%r998, %r937;
	mov.b32 	%r999, %r937;
	mov.b32 	%r1000, %r937;
	mov.b32 	%r1001, %r937;
	mov.b32 	%r1002, %r937;
	mov.b32 	%r1003, %r937;
	mov.b32 	%r1004, %r937;
	mov.b32 	%r1005, %r937;
	mov.b32 	%r1006, %r937;
	mov.b32 	%r1007, %r937;
	mov.b32 	%r1008, %r937;
	mov.b32 	%r1009, %r937;
	mov.b32 	%r1010, %r937;
	mov.b32 	%r1011, %r937;
	mov.b32 	%r1012, %r937;
	mov.b32 	%r1013, %r937;
	mov.b32 	%r1014, %r937;
	mov.b32 	%r1015, %r937;
	mov.b32 	%r1016, %r937;
	mov.b32 	%r1017, %r937;
	mov.b32 	%r1018, %r937;
	mov.b32 	%r1019, %r937;
	mov.b32 	%r1020, %r937;
	mov.b32 	%r1021, %r937;
	mov.b32 	%r1022, %r937;
	mov.b32 	%r1023, %r937;
	mov.b32 	%r1024, %r937;
	mov.b32 	%r1025, %r937;
	mov.b32 	%r1026, %r937;
	mov.b32 	%r1027, %r937;
	mov.b32 	%r1028, %r937;
	mov.b32 	%r1029, %r937;
	mov.b32 	%r1030, %r937;
	mov.b32 	%r1031, %r937;
	mov.b32 	%r1032, %r937;
	mov.b32 	%r1033, %r937;
	mov.b32 	%r1034, %r937;
	mov.b32 	%r1035, %r937;
	mov.b32 	%r1036, %r937;
	mov.b32 	%r1037, %r937;
	mov.b32 	%r1038, %r937;
	mov.b32 	%r1039, %r937;
	mov.b32 	%r1040, %r937;
	mov.b32 	%r1041, %r937;
	mov.b32 	%r1042, %r937;
	mov.b32 	%r1043, %r937;
	mov.b32 	%r1044, %r937;
	mov.b32 	%r1045, %r937;
	mov.b32 	%r1046, %r937;
	mov.b32 	%r1047, %r937;
	mov.b32 	%r1048, %r937;
	mov.b32 	%r1049, %r937;
	mov.b32 	%r1050, %r937;
	mov.b32 	%r1051, %r937;
	mov.b32 	%r1052, %r937;
	mov.b32 	%r1053, %r937;
	mov.b32 	%r1054, %r937;
	mov.b32 	%r1055, %r937;
	mov.b32 	%r1056, %r937;
	mov.b32 	%r1057, %r937;
	mov.b32 	%r1058, %r937;
	mov.b32 	%r1059, %r937;
	mov.b32 	%r1060, %r937;
	mov.b32 	%r1061, %r937;
	mov.b32 	%r1062, %r937;
	mov.b32 	%r1063, %r937;
	mov.b32 	%r1064, %r937;
$L__BB0_5:                              // %._crit_edge
	.loc	1 46 51                         // dump_ptx.py:46:51
	or.b32 	%r586, %r1, %r4;
	.loc	1 46 38                         // dump_ptx.py:46:38
	or.b32 	%r587, %r586, 124;
	or.b32 	%r588, %r586, 120;
	or.b32 	%r589, %r586, 116;
	or.b32 	%r590, %r586, 112;
	or.b32 	%r591, %r586, 108;
	or.b32 	%r592, %r586, 104;
	or.b32 	%r593, %r586, 100;
	or.b32 	%r594, %r586, 96;
	or.b32 	%r595, %r586, 92;
	or.b32 	%r596, %r586, 88;
	or.b32 	%r597, %r586, 84;
	or.b32 	%r598, %r586, 80;
	or.b32 	%r599, %r586, 76;
	or.b32 	%r600, %r586, 72;
	or.b32 	%r601, %r586, 68;
	or.b32 	%r602, %r586, 64;
	or.b32 	%r603, %r1, %r19;
	or.b32 	%r604, %r1, %r18;
	or.b32 	%r605, %r1, %r17;
	or.b32 	%r606, %r1, %r16;
	or.b32 	%r607, %r1, %r15;
	or.b32 	%r608, %r1, %r14;
	or.b32 	%r609, %r1, %r13;
	or.b32 	%r610, %r1, %r12;
	or.b32 	%r611, %r1, %r11;
	or.b32 	%r612, %r1, %r10;
	or.b32 	%r613, %r1, %r9;
	or.b32 	%r614, %r1, %r8;
	or.b32 	%r615, %r1, %r7;
	or.b32 	%r616, %r1, %r6;
	or.b32 	%r617, %r1, %r5;
	.loc	1 53 22                         // dump_ptx.py:53:22
	cp.async.wait_group 	0;
	bar.sync 	0;
	.loc	1 64 33                         // dump_ptx.py:64:33
	mul.lo.s32 	%r618, %r586, %r42;
	mul.lo.s32 	%r619, %r617, %r42;
	mul.lo.s32 	%r620, %r616, %r42;
	mul.lo.s32 	%r621, %r615, %r42;
	mul.lo.s32 	%r622, %r614, %r42;
	mul.lo.s32 	%r623, %r613, %r42;
	mul.lo.s32 	%r624, %r612, %r42;
	mul.lo.s32 	%r625, %r611, %r42;
	mul.lo.s32 	%r626, %r610, %r42;
	mul.lo.s32 	%r627, %r609, %r42;
	mul.lo.s32 	%r628, %r608, %r42;
	mul.lo.s32 	%r629, %r607, %r42;
	mul.lo.s32 	%r630, %r606, %r42;
	mul.lo.s32 	%r631, %r605, %r42;
	mul.lo.s32 	%r632, %r604, %r42;
	mul.lo.s32 	%r633, %r603, %r42;
	shl.b32 	%r634, %r42, 6;
	add.s32 	%r635, %r618, %r634;
	mul.lo.s32 	%r636, %r601, %r42;
	mul.lo.s32 	%r637, %r600, %r42;
	mul.lo.s32 	%r638, %r599, %r42;
	shl.b32 	%r639, %r42, 4;
	add.s32 	%r640, %r635, %r639;
	mul.lo.s32 	%r641, %r597, %r42;
	mul.lo.s32 	%r642, %r596, %r42;
	mul.lo.s32 	%r643, %r595, %r42;
	add.s32 	%r644, %r640, %r639;
	mul.lo.s32 	%r645, %r593, %r42;
	mul.lo.s32 	%r646, %r592, %r42;
	mul.lo.s32 	%r647, %r591, %r42;
	add.s32 	%r648, %r644, %r639;
	mul.lo.s32 	%r649, %r589, %r42;
	mul.lo.s32 	%r650, %r588, %r42;
	mul.lo.s32 	%r651, %r587, %r42;
	.loc	1 64 21                         // dump_ptx.py:64:21
	mad.wide.s32 	%rd163, %r618, 2, %rd48;
	mad.wide.s32 	%rd164, %r619, 2, %rd48;
	mad.wide.s32 	%rd165, %r620, 2, %rd48;
	mad.wide.s32 	%rd166, %r621, 2, %rd48;
	mad.wide.s32 	%rd167, %r622, 2, %rd48;
	mad.wide.s32 	%rd168, %r623, 2, %rd48;
	mad.wide.s32 	%rd169, %r624, 2, %rd48;
	mad.wide.s32 	%rd170, %r625, 2, %rd48;
	mad.wide.s32 	%rd171, %r626, 2, %rd48;
	mad.wide.s32 	%rd172, %r627, 2, %rd48;
	mad.wide.s32 	%rd173, %r628, 2, %rd48;
	mad.wide.s32 	%rd174, %r629, 2, %rd48;
	mad.wide.s32 	%rd175, %r630, 2, %rd48;
	mad.wide.s32 	%rd176, %r631, 2, %rd48;
	mad.wide.s32 	%rd177, %r632, 2, %rd48;
	mad.wide.s32 	%rd178, %r633, 2, %rd48;
	mad.wide.s32 	%rd179, %r635, 2, %rd48;
	mad.wide.s32 	%rd180, %r636, 2, %rd48;
	mad.wide.s32 	%rd181, %r637, 2, %rd48;
	mad.wide.s32 	%rd182, %r638, 2, %rd48;
	mad.wide.s32 	%rd183, %r640, 2, %rd48;
	mad.wide.s32 	%rd184, %r641, 2, %rd48;
	mad.wide.s32 	%rd185, %r642, 2, %rd48;
	mad.wide.s32 	%rd186, %r643, 2, %rd48;
	mad.wide.s32 	%rd187, %r644, 2, %rd48;
	mad.wide.s32 	%rd188, %r645, 2, %rd48;
	mad.wide.s32 	%rd189, %r646, 2, %rd48;
	mad.wide.s32 	%rd190, %r647, 2, %rd48;
	mad.wide.s32 	%rd191, %r648, 2, %rd48;
	mad.wide.s32 	%rd192, %r649, 2, %rd48;
	mad.wide.s32 	%rd193, %r650, 2, %rd48;
	mad.wide.s32 	%rd194, %r651, 2, %rd48;
	.loc	1 64 52                         // dump_ptx.py:64:52
	mul.wide.s32 	%rd195, %r21, 2;
	add.s64 	%rd131, %rd163, %rd195;
	add.s64 	%rd132, %rd164, %rd195;
	add.s64 	%rd133, %rd165, %rd195;
	add.s64 	%rd134, %rd166, %rd195;
	add.s64 	%rd135, %rd167, %rd195;
	add.s64 	%rd136, %rd168, %rd195;
	add.s64 	%rd137, %rd169, %rd195;
	add.s64 	%rd138, %rd170, %rd195;
	add.s64 	%rd139, %rd171, %rd195;
	add.s64 	%rd140, %rd172, %rd195;
	add.s64 	%rd141, %rd173, %rd195;
	add.s64 	%rd142, %rd174, %rd195;
	add.s64 	%rd143, %rd175, %rd195;
	add.s64 	%rd144, %rd176, %rd195;
	add.s64 	%rd145, %rd177, %rd195;
	add.s64 	%rd146, %rd178, %rd195;
	add.s64 	%rd147, %rd179, %rd195;
	add.s64 	%rd148, %rd180, %rd195;
	add.s64 	%rd149, %rd181, %rd195;
	add.s64 	%rd150, %rd182, %rd195;
	add.s64 	%rd151, %rd183, %rd195;
	add.s64 	%rd152, %rd184, %rd195;
	add.s64 	%rd153, %rd185, %rd195;
	add.s64 	%rd154, %rd186, %rd195;
	add.s64 	%rd155, %rd187, %rd195;
	add.s64 	%rd156, %rd188, %rd195;
	add.s64 	%rd157, %rd189, %rd195;
	add.s64 	%rd158, %rd190, %rd195;
	add.s64 	%rd159, %rd191, %rd195;
	add.s64 	%rd160, %rd192, %rd195;
	add.s64 	%rd161, %rd193, %rd195;
	add.s64 	%rd162, %rd194, %rd195;
	.loc	1 65 33                         // dump_ptx.py:65:33
	setp.lt.s32 	%p30, %r586, %r38;
	setp.lt.s32 	%p31, %r614, %r38;
	setp.lt.s32 	%p32, %r610, %r38;
	setp.lt.s32 	%p33, %r606, %r38;
	setp.lt.s32 	%p34, %r602, %r38;
	setp.lt.s32 	%p35, %r598, %r38;
	setp.lt.s32 	%p36, %r594, %r38;
	setp.lt.s32 	%p37, %r590, %r38;
	.loc	1 65 58                         // dump_ptx.py:65:58
	setp.lt.s32 	%p38, %r21, %r39;
	.loc	1 65 39                         // dump_ptx.py:65:39
	and.pred 	%p22, %p30, %p38;
	and.pred 	%p23, %p31, %p38;
	and.pred 	%p24, %p32, %p38;
	and.pred 	%p25, %p33, %p38;
	and.pred 	%p26, %p34, %p38;
	and.pred 	%p27, %p35, %p38;
	and.pred 	%p28, %p36, %p38;
	and.pred 	%p29, %p37, %p38;
	.loc	1 66 21                         // dump_ptx.py:66:21
	and.b32 	%r652, %r2, 3;
	shl.b32 	%r653, %r652, 10;
	shl.b32 	%r654, %r652, 5;
	and.b32 	%r655, %r2, 24;
	shl.b32 	%r656, %r655, 4;
	bfe.s32 	%r657, %r2, 2, 1;
	and.b32 	%r658, %r657, 528;
	or.b32 	%r659, %r658, %r653;
	or.b32 	%r660, %r654, %r656;
	xor.b32 	%r661, %r660, %r3;
	or.b32 	%r662, %r659, %r661;
	add.s32 	%r663, %r194, %r662;
	st.shared.v4.b32 	[%r663], {%r937, %r938, %r939, %r940};
	xor.b32 	%r664, %r662, 16;
	add.s32 	%r665, %r194, %r664;
	st.shared.v4.b32 	[%r665], {%r941, %r942, %r943, %r944};
	bar.sync 	0;
	shl.b32 	%r666, %r655, 7;
	shl.b32 	%r667, %r655, 2;
	bfe.s32 	%r668, %r2, 5, 1;
	and.b32 	%r669, %r668, 528;
	and.b32 	%r670, %r26, 128;
	or.b32 	%r671, %r666, %r936;
	or.b32 	%r672, %r669, %r667;
	xor.b32 	%r673, %r672, %r671;
	add.s32 	%r674, %r194, %r670;
	add.s32 	%r675, %r674, %r673;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r458, %r459, %r460, %r461}, [%r675];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r462, %r463, %r464, %r465}, [%r675+256];
	bar.sync 	0;
	st.shared.v4.b32 	[%r663], {%r945, %r946, %r947, %r948};
	st.shared.v4.b32 	[%r665], {%r949, %r950, %r951, %r952};
	bar.sync 	0;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r466, %r467, %r468, %r469}, [%r675];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r470, %r471, %r472, %r473}, [%r675+256];
	bar.sync 	0;
	st.shared.v4.b32 	[%r663], {%r953, %r954, %r955, %r956};
	st.shared.v4.b32 	[%r665], {%r957, %r958, %r959, %r960};
	bar.sync 	0;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r474, %r475, %r476, %r477}, [%r675];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r478, %r479, %r480, %r481}, [%r675+256];
	bar.sync 	0;
	st.shared.v4.b32 	[%r663], {%r961, %r962, %r963, %r964};
	st.shared.v4.b32 	[%r665], {%r965, %r966, %r967, %r968};
	bar.sync 	0;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r482, %r483, %r484, %r485}, [%r675];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r486, %r487, %r488, %r489}, [%r675+256];
	bar.sync 	0;
	st.shared.v4.b32 	[%r663], {%r969, %r970, %r971, %r972};
	st.shared.v4.b32 	[%r665], {%r973, %r974, %r975, %r976};
	bar.sync 	0;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r490, %r491, %r492, %r493}, [%r675];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r494, %r495, %r496, %r497}, [%r675+256];
	bar.sync 	0;
	st.shared.v4.b32 	[%r663], {%r977, %r978, %r979, %r980};
	st.shared.v4.b32 	[%r665], {%r981, %r982, %r983, %r984};
	bar.sync 	0;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r498, %r499, %r500, %r501}, [%r675];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r502, %r503, %r504, %r505}, [%r675+256];
	bar.sync 	0;
	st.shared.v4.b32 	[%r663], {%r985, %r986, %r987, %r988};
	st.shared.v4.b32 	[%r665], {%r989, %r990, %r991, %r992};
	bar.sync 	0;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r506, %r507, %r508, %r509}, [%r675];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r510, %r511, %r512, %r513}, [%r675+256];
	bar.sync 	0;
	st.shared.v4.b32 	[%r663], {%r993, %r994, %r995, %r996};
	st.shared.v4.b32 	[%r665], {%r997, %r998, %r999, %r1000};
	bar.sync 	0;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r514, %r515, %r516, %r517}, [%r675];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r518, %r519, %r520, %r521}, [%r675+256];
	bar.sync 	0;
	st.shared.v4.b32 	[%r663], {%r1001, %r1002, %r1003, %r1004};
	st.shared.v4.b32 	[%r665], {%r1005, %r1006, %r1007, %r1008};
	bar.sync 	0;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r522, %r523, %r524, %r525}, [%r675];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r526, %r527, %r528, %r529}, [%r675+256];
	bar.sync 	0;
	st.shared.v4.b32 	[%r663], {%r1009, %r1010, %r1011, %r1012};
	st.shared.v4.b32 	[%r665], {%r1013, %r1014, %r1015, %r1016};
	bar.sync 	0;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r530, %r531, %r532, %r533}, [%r675];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r534, %r535, %r536, %r537}, [%r675+256];
	bar.sync 	0;
	st.shared.v4.b32 	[%r663], {%r1017, %r1018, %r1019, %r1020};
	st.shared.v4.b32 	[%r665], {%r1021, %r1022, %r1023, %r1024};
	bar.sync 	0;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r538, %r539, %r540, %r541}, [%r675];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r542, %r543, %r544, %r545}, [%r675+256];
	bar.sync 	0;
	st.shared.v4.b32 	[%r663], {%r1025, %r1026, %r1027, %r1028};
	st.shared.v4.b32 	[%r665], {%r1029, %r1030, %r1031, %r1032};
	bar.sync 	0;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r546, %r547, %r548, %r549}, [%r675];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r550, %r551, %r552, %r553}, [%r675+256];
	bar.sync 	0;
	st.shared.v4.b32 	[%r663], {%r1033, %r1034, %r1035, %r1036};
	st.shared.v4.b32 	[%r665], {%r1037, %r1038, %r1039, %r1040};
	bar.sync 	0;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r554, %r555, %r556, %r557}, [%r675];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r558, %r559, %r560, %r561}, [%r675+256];
	bar.sync 	0;
	st.shared.v4.b32 	[%r663], {%r1041, %r1042, %r1043, %r1044};
	st.shared.v4.b32 	[%r665], {%r1045, %r1046, %r1047, %r1048};
	bar.sync 	0;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r562, %r563, %r564, %r565}, [%r675];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r566, %r567, %r568, %r569}, [%r675+256];
	bar.sync 	0;
	st.shared.v4.b32 	[%r663], {%r1049, %r1050, %r1051, %r1052};
	st.shared.v4.b32 	[%r665], {%r1053, %r1054, %r1055, %r1056};
	bar.sync 	0;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r570, %r571, %r572, %r573}, [%r675];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r574, %r575, %r576, %r577}, [%r675+256];
	bar.sync 	0;
	st.shared.v4.b32 	[%r663], {%r1057, %r1058, %r1059, %r1060};
	st.shared.v4.b32 	[%r665], {%r1061, %r1062, %r1063, %r1064};
	bar.sync 	0;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r578, %r579, %r580, %r581}, [%r675];
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 {%r582, %r583, %r584, %r585}, [%r675+256];
	// begin inline asm
	@%p22 st.global.v4.b32 [ %rd131 + 0 ], { %r458, %r459, %r460, %r461 };
	// end inline asm
	// begin inline asm
	@%p22 st.global.v4.b32 [ %rd132 + 0 ], { %r462, %r463, %r464, %r465 };
	// end inline asm
	// begin inline asm
	@%p22 st.global.v4.b32 [ %rd133 + 0 ], { %r466, %r467, %r468, %r469 };
	// end inline asm
	// begin inline asm
	@%p22 st.global.v4.b32 [ %rd134 + 0 ], { %r470, %r471, %r472, %r473 };
	// end inline asm
	// begin inline asm
	@%p23 st.global.v4.b32 [ %rd135 + 0 ], { %r474, %r475, %r476, %r477 };
	// end inline asm
	// begin inline asm
	@%p23 st.global.v4.b32 [ %rd136 + 0 ], { %r478, %r479, %r480, %r481 };
	// end inline asm
	// begin inline asm
	@%p23 st.global.v4.b32 [ %rd137 + 0 ], { %r482, %r483, %r484, %r485 };
	// end inline asm
	// begin inline asm
	@%p23 st.global.v4.b32 [ %rd138 + 0 ], { %r486, %r487, %r488, %r489 };
	// end inline asm
	// begin inline asm
	@%p24 st.global.v4.b32 [ %rd139 + 0 ], { %r490, %r491, %r492, %r493 };
	// end inline asm
	// begin inline asm
	@%p24 st.global.v4.b32 [ %rd140 + 0 ], { %r494, %r495, %r496, %r497 };
	// end inline asm
	// begin inline asm
	@%p24 st.global.v4.b32 [ %rd141 + 0 ], { %r498, %r499, %r500, %r501 };
	// end inline asm
	// begin inline asm
	@%p24 st.global.v4.b32 [ %rd142 + 0 ], { %r502, %r503, %r504, %r505 };
	// end inline asm
	// begin inline asm
	@%p25 st.global.v4.b32 [ %rd143 + 0 ], { %r506, %r507, %r508, %r509 };
	// end inline asm
	// begin inline asm
	@%p25 st.global.v4.b32 [ %rd144 + 0 ], { %r510, %r511, %r512, %r513 };
	// end inline asm
	// begin inline asm
	@%p25 st.global.v4.b32 [ %rd145 + 0 ], { %r514, %r515, %r516, %r517 };
	// end inline asm
	// begin inline asm
	@%p25 st.global.v4.b32 [ %rd146 + 0 ], { %r518, %r519, %r520, %r521 };
	// end inline asm
	// begin inline asm
	@%p26 st.global.v4.b32 [ %rd147 + 0 ], { %r522, %r523, %r524, %r525 };
	// end inline asm
	// begin inline asm
	@%p26 st.global.v4.b32 [ %rd148 + 0 ], { %r526, %r527, %r528, %r529 };
	// end inline asm
	// begin inline asm
	@%p26 st.global.v4.b32 [ %rd149 + 0 ], { %r530, %r531, %r532, %r533 };
	// end inline asm
	// begin inline asm
	@%p26 st.global.v4.b32 [ %rd150 + 0 ], { %r534, %r535, %r536, %r537 };
	// end inline asm
	// begin inline asm
	@%p27 st.global.v4.b32 [ %rd151 + 0 ], { %r538, %r539, %r540, %r541 };
	// end inline asm
	// begin inline asm
	@%p27 st.global.v4.b32 [ %rd152 + 0 ], { %r542, %r543, %r544, %r545 };
	// end inline asm
	// begin inline asm
	@%p27 st.global.v4.b32 [ %rd153 + 0 ], { %r546, %r547, %r548, %r549 };
	// end inline asm
	// begin inline asm
	@%p27 st.global.v4.b32 [ %rd154 + 0 ], { %r550, %r551, %r552, %r553 };
	// end inline asm
	// begin inline asm
	@%p28 st.global.v4.b32 [ %rd155 + 0 ], { %r554, %r555, %r556, %r557 };
	// end inline asm
	// begin inline asm
	@%p28 st.global.v4.b32 [ %rd156 + 0 ], { %r558, %r559, %r560, %r561 };
	// end inline asm
	// begin inline asm
	@%p28 st.global.v4.b32 [ %rd157 + 0 ], { %r562, %r563, %r564, %r565 };
	// end inline asm
	// begin inline asm
	@%p28 st.global.v4.b32 [ %rd158 + 0 ], { %r566, %r567, %r568, %r569 };
	// end inline asm
	// begin inline asm
	@%p29 st.global.v4.b32 [ %rd159 + 0 ], { %r570, %r571, %r572, %r573 };
	// end inline asm
	// begin inline asm
	@%p29 st.global.v4.b32 [ %rd160 + 0 ], { %r574, %r575, %r576, %r577 };
	// end inline asm
	// begin inline asm
	@%p29 st.global.v4.b32 [ %rd161 + 0 ], { %r578, %r579, %r580, %r581 };
	// end inline asm
	// begin inline asm
	@%p29 st.global.v4.b32 [ %rd162 + 0 ], { %r582, %r583, %r584, %r585 };
	// end inline asm
	.loc	1 66 4                          // dump_ptx.py:66:4
	ret;
$L__tmp6:
$L__func_end0:
                                        // -- End function
}
	.file	1 "/home/zhaoling/WORKSPACE/Triton_insrt/dump_ptx.py"
	.file	2 "/home/zhaoling/miniconda3/envs/pyzl/lib/python3.13/site-packages/triton/language/standard.py"
	.section	.debug_abbrev
	{
.b8 1                                   // Abbreviation Code
.b8 17                                  // DW_TAG_compile_unit
.b8 1                                   // DW_CHILDREN_yes
.b8 37                                  // DW_AT_producer
.b8 8                                   // DW_FORM_string
.b8 19                                  // DW_AT_language
.b8 5                                   // DW_FORM_data2
.b8 3                                   // DW_AT_name
.b8 8                                   // DW_FORM_string
.b8 16                                  // DW_AT_stmt_list
.b8 6                                   // DW_FORM_data4
.b8 27                                  // DW_AT_comp_dir
.b8 8                                   // DW_FORM_string
.b8 0                                   // EOM(1)
.b8 0                                   // EOM(2)
.b8 2                                   // Abbreviation Code
.b8 46                                  // DW_TAG_subprogram
.b8 0                                   // DW_CHILDREN_no
.b8 3                                   // DW_AT_name
.b8 8                                   // DW_FORM_string
.b8 32                                  // DW_AT_inline
.b8 11                                  // DW_FORM_data1
.b8 0                                   // EOM(1)
.b8 0                                   // EOM(2)
.b8 3                                   // Abbreviation Code
.b8 46                                  // DW_TAG_subprogram
.b8 1                                   // DW_CHILDREN_yes
.b8 17                                  // DW_AT_low_pc
.b8 1                                   // DW_FORM_addr
.b8 18                                  // DW_AT_high_pc
.b8 1                                   // DW_FORM_addr
.b8 49                                  // DW_AT_abstract_origin
.b8 19                                  // DW_FORM_ref4
.b8 0                                   // EOM(1)
.b8 0                                   // EOM(2)
.b8 4                                   // Abbreviation Code
.b8 29                                  // DW_TAG_inlined_subroutine
.b8 0                                   // DW_CHILDREN_no
.b8 49                                  // DW_AT_abstract_origin
.b8 19                                  // DW_FORM_ref4
.b8 17                                  // DW_AT_low_pc
.b8 1                                   // DW_FORM_addr
.b8 18                                  // DW_AT_high_pc
.b8 1                                   // DW_FORM_addr
.b8 88                                  // DW_AT_call_file
.b8 11                                  // DW_FORM_data1
.b8 89                                  // DW_AT_call_line
.b8 11                                  // DW_FORM_data1
.b8 87                                  // DW_AT_call_column
.b8 11                                  // DW_FORM_data1
.b8 0                                   // EOM(1)
.b8 0                                   // EOM(2)
.b8 0                                   // EOM(3)
	}
	.section	.debug_info
	{
.b32 182                                // Length of Unit
.b8 2                                   // DWARF version number
.b8 0
.b32 .debug_abbrev                      // Offset Into Abbrev. Section
.b8 8                                   // Address Size (in bytes)
.b8 1                                   // Abbrev [1] 0xb:0xaf DW_TAG_compile_unit
.b8 116                                 // DW_AT_producer
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2                                   // DW_AT_language
.b8 0
.b8 100                                 // DW_AT_name
.b8 117
.b8 109
.b8 112
.b8 95
.b8 112
.b8 116
.b8 120
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line                        // DW_AT_stmt_list
.b8 47                                  // DW_AT_comp_dir
.b8 104
.b8 111
.b8 109
.b8 101
.b8 47
.b8 122
.b8 104
.b8 97
.b8 111
.b8 108
.b8 105
.b8 110
.b8 103
.b8 47
.b8 87
.b8 79
.b8 82
.b8 75
.b8 83
.b8 80
.b8 65
.b8 67
.b8 69
.b8 47
.b8 84
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 95
.b8 105
.b8 110
.b8 115
.b8 114
.b8 116
.b8 0
.b8 2                                   // Abbrev [2] 0x4b:0x10 DW_TAG_subprogram
.b8 109                                 // DW_AT_name
.b8 97
.b8 116
.b8 109
.b8 117
.b8 108
.b8 95
.b8 107
.b8 101
.b8 114
.b8 110
.b8 101
.b8 108
.b8 0
.b8 1                                   // DW_AT_inline
.b8 3                                   // Abbrev [3] 0x5b:0x5e DW_TAG_subprogram
.b64 $L__func_begin0                    // DW_AT_low_pc
.b64 $L__func_end0                      // DW_AT_high_pc
.b32 75                                 // DW_AT_abstract_origin
.b8 4                                   // Abbrev [4] 0x70:0x18 DW_TAG_inlined_subroutine
.b32 75                                 // DW_AT_abstract_origin
.b64 $L__tmp1                           // DW_AT_low_pc
.b64 $L__tmp2                           // DW_AT_high_pc
.b8 1                                   // DW_AT_call_file
.b8 37                                  // DW_AT_call_line
.b8 27                                  // DW_AT_call_column
.b8 4                                   // Abbrev [4] 0x88:0x18 DW_TAG_inlined_subroutine
.b32 75                                 // DW_AT_abstract_origin
.b64 $L__tmp2                           // DW_AT_low_pc
.b64 $L__tmp3                           // DW_AT_high_pc
.b8 1                                   // DW_AT_call_file
.b8 38                                  // DW_AT_call_line
.b8 27                                  // DW_AT_call_column
.b8 4                                   // Abbrev [4] 0xa0:0x18 DW_TAG_inlined_subroutine
.b32 75                                 // DW_AT_abstract_origin
.b64 $L__tmp4                           // DW_AT_low_pc
.b64 $L__tmp5                           // DW_AT_high_pc
.b8 1                                   // DW_AT_call_file
.b8 53                                  // DW_AT_call_line
.b8 33                                  // DW_AT_call_column
.b8 0                                   // End Of Children Mark
.b8 0                                   // End Of Children Mark
	}
	.section	.debug_macinfo	{	}
